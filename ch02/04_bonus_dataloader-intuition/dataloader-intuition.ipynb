{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d95f841a-63c9-41d4-aea1-496b3d2024dd",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbd7c0d-70f8-4386-a114-907e96c950b0",
   "metadata": {},
   "source": [
    " ## Data sampling with a sliding window with number data\n",
    " ## 使用滑动窗口对数字数据进行采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ed23175-41be-4a7e-8c45-1f100b35a1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.5.0\n"
     ]
    }
   ],
   "source": [
    "# 导入版本检查模块\n",
    "from importlib.metadata import version\n",
    "# 导入PyTorch库\n",
    "import torch\n",
    "\n",
    "# 打印PyTorch版本号\n",
    "print(\"torch version:\", version(\"torch\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ac652d-7b38-4843-9fbd-494cdc8ec12c",
   "metadata": {},
   "source": [
    " To understand the dataloader, which using a sliding window approach, more intuitive, we can consider a dataset that consists of digits only:\n",
    " 为了更直观地理解使用滑动窗口方法的数据加载器，我们可以考虑一个仅由数字组成的数据集：\n",
    "\n",
    "```\n",
    "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 ... 1000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e3f5d3c-95fe-42b2-8051-205f7803675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打开文件用于写入，使用utf-8编码\n",
    "with open(\"number-data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    # 循环生成0-1000的数字\n",
    "    for number in range(1001):\n",
    "        # 将每个数字写入文件，数字之间用空格分隔\n",
    "        f.write(f\"{number} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7becae19-a5a0-4236-87d5-f5eb9b6eb045",
   "metadata": {},
   "source": [
    " Next, we make a small modification to the `token_ids`: instead of using a tokenizer, we parse the integers directly from the text file:\n",
    " 接下来，我们对`token_ids`做一个小修改：不使用分词器，而是直接从文本文件中解析整数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74b41073-4c9f-46e2-a1bd-d38e4122b375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从torch.utils.data导入Dataset和DataLoader类\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# 定义GPTDatasetV1类，继承自Dataset\n",
    "class GPTDatasetV1(Dataset):\n",
    "    # 初始化函数,接收文本、分词器、最大长度和步长作为参数\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        # 初始化输入ID列表\n",
    "        self.input_ids = []\n",
    "        # 初始化目标ID列表\n",
    "        self.target_ids = []\n",
    "\n",
    "        # 修改:不使用分词器,直接将文本分割成整数列表\n",
    "        # token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        token_ids = [int(i) for i in txt.strip().split()]\n",
    "\n",
    "        # 使用滑动窗口将文本分成重叠的序列,每个序列长度为max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            # 获取输入序列片段\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            # 获取目标序列片段(比输入序列向后移动一位)\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            # 将输入序列转换为tensor并添加到input_ids列表\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            # 将目标序列转换为tensor并添加到target_ids列表\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    # 返回数据集长度\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    # 根据索引返回数据样本\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eb30ebe-97b3-43c5-9ff1-a97d621b3c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据加载器的函数,接收以下参数:\n",
    "# txt: 输入文本\n",
    "# batch_size: 批次大小,默认为4\n",
    "# max_length: 序列最大长度,默认为256\n",
    "# stride: 滑动窗口步长,默认为128\n",
    "# shuffle: 是否打乱数据,默认为True\n",
    "# drop_last: 是否丢弃最后不完整的批次,默认为True\n",
    "# num_workers: 数据加载的工作进程数,默认为0\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "\n",
    "    # 初始化分词器\n",
    "    # 注释掉原来的tiktoken分词器\n",
    "    # tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    # 将分词器设为None,因为我们直接从文本解析整数\n",
    "    tokenizer = None\n",
    "\n",
    "    # 使用输入参数创建GPTDatasetV1数据集实例\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # 创建并返回DataLoader实例,设置:\n",
    "    # - dataset: 上面创建的数据集\n",
    "    # - batch_size: 批次大小\n",
    "    # - shuffle: 是否打乱数据\n",
    "    # - drop_last: 是否丢弃最后不完整的批次\n",
    "    # - num_workers: 数据加载的工作进程数\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    # 返回创建的数据加载器\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dd68ef-59f7-45ff-ba44-e311c899ddcd",
   "metadata": {},
   "source": [
    " Let's test the dataloader with a batch size of 1 for an LLM with a context size of 4:\n",
    " 让我们用批次大小为1、上下文长度为4的设置来测试数据加载器:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df31d96c-6bfd-4564-a956-6192242d7579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打开并读取number-data.txt文件\n",
    "# 使用utf-8编码打开文件\n",
    "# 将文件内容读取到raw_text变量中\n",
    "with open(\"number-data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9226d00c-ad9a-4949-a6e4-9afccfc7214f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0, 1, 2, 3]]), tensor([[1, 2, 3, 4]])]\n"
     ]
    }
   ],
   "source": [
    "# 创建数据加载器,设置:\n",
    "# - batch_size=1: 每批次1个样本\n",
    "# - max_length=4: 序列最大长度为4\n",
    "# - stride=1: 滑动窗口步长为1\n",
    "# - shuffle=False: 不打乱数据顺序\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "# 创建数据迭代器\n",
    "data_iter = iter(dataloader)\n",
    "# 获取第一批数据\n",
    "first_batch = next(data_iter)\n",
    "# 打印第一批数据\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10deb4bc-4de1-4d20-921e-4b1c7a0e1a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[1, 2, 3, 4]]), tensor([[2, 3, 4, 5]])]\n"
     ]
    }
   ],
   "source": [
    "# 获取第二批数据\n",
    "second_batch = next(data_iter)\n",
    "# 打印第二批数据\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85a6c312-0144-4128-8d2c-06a4dc223ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[2, 3, 4, 5]]), tensor([[3, 4, 5, 6]])]\n"
     ]
    }
   ],
   "source": [
    "# 获取第三批数据\n",
    "third_batch = next(data_iter)\n",
    "# 打印第三批数据\n",
    "print(third_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14b7ec67-083a-4b28-bcb9-f4c8e97e250e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[996, 997, 998, 999]]), tensor([[ 997,  998,  999, 1000]])]\n"
     ]
    }
   ],
   "source": [
    "# 遍历数据加载器中的所有批次\n",
    "for batch in dataloader:\n",
    "    pass\n",
    "\n",
    "# 获取最后一个批次\n",
    "last_batch = batch\n",
    "# 打印最后一个批次\n",
    "print(last_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ae6d45-f26e-4b83-9c7b-cff55ffa7d16",
   "metadata": {},
   "source": [
    " Now, let's look at the batched inputs:\n",
    " 现在,让我们看看批量输入:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1916e7a6-f03d-4f09-91a6-d0bdbac5a58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[992, 993, 994, 995],\n",
      "        [996, 997, 998, 999]])\n",
      "\n",
      "Targets:\n",
      " tensor([[ 993,  994,  995,  996],\n",
      "        [ 997,  998,  999, 1000]])\n"
     ]
    }
   ],
   "source": [
    "# 创建数据加载器:\n",
    "# - batch_size=2: 每批次2个样本\n",
    "# - max_length=4: 序列最大长度为4 \n",
    "# - stride=4: 滑动窗口步长为4\n",
    "# - shuffle=False: 不打乱数据顺序\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=2, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "# 遍历数据加载器中的所有批次\n",
    "for inputs, targets in dataloader:\n",
    "    pass\n",
    "\n",
    "# 打印最后一批次的输入和目标\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd66560-25d5-4800-acc1-432735dfc7d6",
   "metadata": {},
   "source": [
    " Finally, a data loader with shuffling:\n",
    " 最后,让我们看看带有打乱功能的数据加载器:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39dd4952-5333-45f0-9032-f93007d742b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[880, 881, 882, 883],\n",
      "        [112, 113, 114, 115]])\n",
      "\n",
      "Targets:\n",
      " tensor([[881, 882, 883, 884],\n",
      "        [113, 114, 115, 116]])\n"
     ]
    }
   ],
   "source": [
    "# 设置随机种子为123,保证结果可复现\n",
    "torch.manual_seed(123)\n",
    "# 创建数据加载器:\n",
    "# - batch_size=2: 每批次2个样本\n",
    "# - max_length=4: 序列最大长度为4\n",
    "# - stride=4: 滑动窗口步长为4 \n",
    "# - shuffle=True: 打乱数据顺序\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=2, max_length=4, stride=4, shuffle=True)\n",
    "\n",
    "# 遍历数据加载器中的所有批次\n",
    "for inputs, targets in dataloader:\n",
    "    pass\n",
    "\n",
    "# 打印最后一批次的输入和目标\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
