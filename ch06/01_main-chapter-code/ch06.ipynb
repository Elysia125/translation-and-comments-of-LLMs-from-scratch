{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c024bfa4-1a7a-4751-b5a1-827225a3478b",
   "metadata": {
    "id": "c024bfa4-1a7a-4751-b5a1-827225a3478b"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfabadb8-5935-45ff-b39c-db7a29012129",
   "metadata": {
    "id": "bfabadb8-5935-45ff-b39c-db7a29012129"
   },
   "source": [
    "# Chapter 6: Finetuning for Text Classification\n",
    "# 第六章：文本分类的微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7e01c2-1c84-4f2a-bb51-2e0b74abda90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b7e01c2-1c84-4f2a-bb51-2e0b74abda90",
    "outputId": "9495f150-9d79-4910-d6e7-6c0d9aae4a41"
   },
   "outputs": [],
   "source": [
    "# 导入版本检查模块\n",
    "from importlib.metadata import version\n",
    "\n",
    "# 定义需要检查版本的包列表\n",
    "pkgs = [\"matplotlib\",\n",
    "        \"numpy\", \n",
    "        \"tiktoken\",\n",
    "        \"torch\",\n",
    "        \"tensorflow\", # 用于OpenAI的预训练权重\n",
    "        \"pandas\"      # 用于数据集加载\n",
    "       ]\n",
    "\n",
    "# 遍历包列表并打印每个包的版本\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a445828a-ff10-4efa-9f60-a2e2aed4c87d",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/chapter-overview.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a84cf35-b37f-4c15-8972-dfafc9fadc1c",
   "metadata": {
    "id": "3a84cf35-b37f-4c15-8972-dfafc9fadc1c"
   },
   "source": [
    "## 6.1 Different categories of finetuning\n",
    "## 6.1 不同类型的微调"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede3d731-5123-4f02-accd-c670ce50a5a3",
   "metadata": {
    "id": "ede3d731-5123-4f02-accd-c670ce50a5a3"
   },
   "source": [
    "- No code in this section\n",
    "- 这节没有代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac45579d-d485-47dc-829e-43be7f4db57b",
   "metadata": {},
   "source": [
    "- The most common ways to finetune language models are instruction-finetuning and classification finetuning\n",
    "- 微调语言模型最常见的方式是指令微调和分类微调\n",
    "- Instruction-finetuning, depicted below, is the topic of the next chapter\n",
    "- 下面将介绍的指令微调是下一章的主题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c29ef42-46d9-43d4-8bb4-94974e1665e4",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/instructions.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f60321-95b8-46a9-97bf-1d07fda2c3dd",
   "metadata": {},
   "source": [
    "- Classification finetuning, the topic of this chapter, is a procedure you may already be familiar with if you have a background in machine learning -- it's similar to training a convolutional network to classify handwritten digits, for example\n",
    "- 分类微调是本章的主题，如果你有机器学习背景的话应该已经很熟悉了 -- 它类似于训练卷积网络来分类手写数字\n",
    "- In classification finetuning, we have a specific number of class labels (for example, \"spam\" and \"not spam\") that the model can output\n",
    "- 在分类微调中，我们有特定数量的类别标签（例如\"垃圾邮件\"和\"非垃圾邮件\"）作为模型的输出\n",
    "- A classification finetuned model can only predict classes it has seen during training (for example, \"spam\" or \"not spam\"), whereas an instruction-finetuned model can usually perform many tasks\n",
    "- 分类微调模型只能预测它在训练期间见过的类别（例如\"垃圾邮件\"或\"非垃圾邮件\"），而指令微调模型通常可以执行多种任务\n",
    "- We can think of a classification-finetuned model as a very specialized model; in practice, it is much easier to create a specialized model than a generalist model that performs well on many different tasks\n",
    "- 我们可以将分类微调模型视为一个非常专门化的模型；在实践中，创建一个专门化的模型比创建一个在多个不同任务上表现良好的通用模型要容易得多"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b37a0c4-0bb1-4061-b1fe-eaa4416d52c3",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/spam-non-spam.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7017a2-32aa-4002-a2f3-12aac293ccdf",
   "metadata": {
    "id": "8c7017a2-32aa-4002-a2f3-12aac293ccdf"
   },
   "source": [
    "## 6.2 Preparing the dataset\n",
    "## 6.2 准备数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f628975-d2e8-4f7f-ab38-92bb868b7067",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/overview-1.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbd459f-63fa-4d8c-8499-e23103156c7d",
   "metadata": {
    "id": "9fbd459f-63fa-4d8c-8499-e23103156c7d"
   },
   "source": [
    "- This section prepares the dataset we use for classification finetuning\n",
    "- 本节准备用于分类微调的数据集\n",
    "- We use a dataset consisting of spam and non-spam text messages to finetune the LLM to classify them  \n",
    "- 我们使用一个由垃圾短信和非垃圾短信组成的数据集来微调LLM进行分类\n",
    "- First, we download and unzip the dataset\n",
    "- 首先，我们下载并解压数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def7c09b-af9c-4216-90ce-5e67aed1065c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "def7c09b-af9c-4216-90ce-5e67aed1065c",
    "outputId": "424e4423-f623-443c-ab9e-656f9e867559"
   },
   "outputs": [],
   "source": [
    "# 导入所需的库\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 定义数据集的URL和相关文件路径\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"  # zip文件保存路径\n",
    "extracted_path = \"sms_spam_collection\"  # 解压目录\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"  # 最终数据文件路径\n",
    "\n",
    "# 定义下载和解压数据的函数\n",
    "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
    "    # 检查文件是否已存在\n",
    "    if data_file_path.exists():\n",
    "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
    "        return\n",
    "\n",
    "    # 下载文件\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        with open(zip_path, \"wb\") as out_file:\n",
    "            out_file.write(response.read())\n",
    "\n",
    "    # 解压文件\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "\n",
    "    # 添加.tsv文件扩展名并重命名\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path)\n",
    "    print(f\"File downloaded and saved as {data_file_path}\")\n",
    "\n",
    "# 执行下载和解压函数\n",
    "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac2d19-06d0-4005-916b-0bd4b1ee50d1",
   "metadata": {
    "id": "6aac2d19-06d0-4005-916b-0bd4b1ee50d1"
   },
   "source": [
    "- The dataset is saved as a tab-separated text file, which we can load into a pandas DataFrame\n",
    "- 数据集以制表符分隔的文本文件形式保存，我们可以将其加载到pandas DataFrame中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0ed4da-ac31-4e4d-8bdd-2153be4656a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "da0ed4da-ac31-4e4d-8bdd-2153be4656a4",
    "outputId": "a16c5cde-d341-4887-a93f-baa9bec542ab"
   },
   "outputs": [],
   "source": [
    "# 导入pandas库\n",
    "import pandas as pd\n",
    "\n",
    "# 使用pandas读取数据文件\n",
    "# data_file_path: 数据文件路径\n",
    "# sep=\"\\t\": 使用制表符作为分隔符\n",
    "# header=None: 文件没有标题行\n",
    "# names=[\"Label\", \"Text\"]: 为列指定名称\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "\n",
    "# 显示DataFrame内容\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b6e631-4f0b-4aab-82b9-8898e6663109",
   "metadata": {
    "id": "e7b6e631-4f0b-4aab-82b9-8898e6663109"
   },
   "source": [
    "- When we check the class distribution, we see that the data contains \"ham\" (i.e., \"not spam\") much more frequently than \"spam\"\n",
    "- 当我们检查类别分布时，我们发现数据中\"ham\"(即\"非垃圾邮件\")的频率远高于\"spam\"(垃圾邮件)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495a5280-9d7c-41d4-9719-64ab99056d4c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "495a5280-9d7c-41d4-9719-64ab99056d4c",
    "outputId": "761e0482-43ba-4f46-f4b7-6774dae51b38"
   },
   "outputs": [],
   "source": [
    "# 统计并打印每个标签的出现次数\n",
    "print(df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f773f054-0bdc-4aad-bbf6-397621bf63db",
   "metadata": {
    "id": "f773f054-0bdc-4aad-bbf6-397621bf63db"
   },
   "source": [
    "- For simplicity, and because we prefer a small dataset for educational purposes anyway (it will make it possible to finetune the LLM faster), we subsample (undersample) the dataset so that it contains 747 instances from each class\n",
    "- 为了简单起见，并且因为我们更倾向于使用小型数据集用于教育目的(这样可以更快地对LLM进行微调)，我们对数据集进行下采样，使每个类别包含747个样本\n",
    "- (Next to undersampling, there are several other ways to deal with class balances, but they are out of the scope of a book on LLMs; you can find examples and more information in the [`imbalanced-learn` user guide](https://imbalanced-learn.org/stable/user_guide.html))\n",
    "- (除了下采样之外，还有其他几种处理类别平衡的方法，但这些不在LLM书籍的讨论范围内；你可以在[`imbalanced-learn`用户指南](https://imbalanced-learn.org/stable/user_guide.html)中找到更多示例和信息)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be4a0a2-9704-4a96-b38f-240339818688",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7be4a0a2-9704-4a96-b38f-240339818688",
    "outputId": "396dc415-cb71-4a88-e85d-d88201c6d73f"
   },
   "outputs": [],
   "source": [
    "def create_balanced_dataset(df):\n",
    "    \"\"\"\n",
    "    创建一个平衡的数据集，通过对多数类进行下采样来平衡类别分布\n",
    "    \n",
    "    参数:\n",
    "        df: pandas DataFrame，包含'Label'和'Text'列的原始数据集\n",
    "        \n",
    "    返回:\n",
    "        balanced_df: pandas DataFrame，包含相同数量的spam和ham样本的平衡数据集\n",
    "    \"\"\"\n",
    "    \n",
    "    # 统计垃圾邮件(spam)的数量\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "    \n",
    "    # 从非垃圾邮件(ham)中随机采样，使其数量与垃圾邮件相同\n",
    "    # random_state=123确保结果可重现\n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "    \n",
    "    # 将采样后的ham子集与所有spam样本合并\n",
    "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "# 创建平衡数据集\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "# 打印每个类别的样本数量\n",
    "print(balanced_df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fd2f5a-06d8-4d30-a2e3-230b86c559d6",
   "metadata": {
    "id": "d3fd2f5a-06d8-4d30-a2e3-230b86c559d6"
   },
   "source": [
    "- Next, we change the string class labels \"ham\" and \"spam\" into integer class labels 0 and 1:\n",
    "- 接下来，我们将字符串类别标签\"ham\"和\"spam\"转换为整数类别标签0和1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1b10c3d-5d57-42d0-8de8-cf80a06f5ffd",
   "metadata": {
    "id": "c1b10c3d-5d57-42d0-8de8-cf80a06f5ffd"
   },
   "outputs": [],
   "source": [
    "# 使用map函数将字符串标签转换为数值标签\n",
    "# \"ham\"映射为0, \"spam\"映射为1\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5715e685-35b4-4b45-a86c-8a8694de9d6f",
   "metadata": {
    "id": "5715e685-35b4-4b45-a86c-8a8694de9d6f"
   },
   "source": [
    "- Let's now define a function that randomly divides the dataset into training, validation, and test subsets\n",
    "- 现在让我们定义一个函数，将数据集随机分割为训练集、验证集和测试集子集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "uQl0Psdmx15D",
   "metadata": {
    "id": "uQl0Psdmx15D"
   },
   "outputs": [],
   "source": [
    "def random_split(df, train_frac, validation_frac):\n",
    "    \"\"\"\n",
    "    将数据集随机分割为训练集、验证集和测试集\n",
    "    \n",
    "    参数:\n",
    "        df: pandas DataFrame，要分割的数据集\n",
    "        train_frac: float，训练集所占比例\n",
    "        validation_frac: float，验证集所占比例\n",
    "        \n",
    "    返回:\n",
    "        train_df: pandas DataFrame，训练集\n",
    "        validation_df: pandas DataFrame，验证集 \n",
    "        test_df: pandas DataFrame，测试集\n",
    "    \"\"\"\n",
    "    # 打乱整个DataFrame\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "    # 计算分割索引\n",
    "    train_end = int(len(df) * train_frac)\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "\n",
    "    # 分割DataFrame\n",
    "    train_df = df[:train_end]  # 训练集\n",
    "    validation_df = df[train_end:validation_end]  # 验证集\n",
    "    test_df = df[validation_end:]  # 测试集(剩余部分)\n",
    "\n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "# 使用70%数据作为训练集,10%作为验证集,20%作为测试集进行分割\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "\n",
    "# 将分割后的数据集保存到CSV文件\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d7a0c5-1d5f-458a-b685-3f49520b0094",
   "metadata": {},
   "source": [
    "## 6.3 Creating data loaders\n",
    "## 6.3 创建数据加载器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7126108a-75e7-4862-b0fb-cbf59a18bb6c",
   "metadata": {
    "id": "7126108a-75e7-4862-b0fb-cbf59a18bb6c"
   },
   "source": [
    "- Note that the text messages have different lengths; if we want to combine multiple training examples in a batch, we have to either\n",
    "- 请注意文本消息的长度不同；如果我们想在一个批次中组合多个训练样本，我们必须选择以下其中一种方式：\n",
    "  1. truncate all messages to the length of the shortest message in the dataset or batch\n",
    "  1. 将所有消息截断到数据集或批次中最短消息的长度\n",
    "  2. pad all messages to the length of the longest message in the dataset or batch\n",
    "  2. 将所有消息填充到数据集或批次中最长消息的长度\n",
    "\n",
    "- We choose option 2 and pad all messages to the longest message in the dataset\n",
    "- 我们选择选项2，将所有消息填充到数据集中最长消息的长度\n",
    "\n",
    "- For that, we use `<|endoftext|>` as a padding token, as discussed in chapter 2\n",
    "- 为此，我们使用`<|endoftext|>`作为填充标记，正如第2章中讨论的那样"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0829f33f-1428-4f22-9886-7fee633b3666",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/pad-input-sequences.webp?123\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c3c463-8763-4cc0-9320-41c7eaad8ab7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74c3c463-8763-4cc0-9320-41c7eaad8ab7",
    "outputId": "b5b48439-32c8-4b37-cca2-c9dc8fa86563"
   },
   "outputs": [],
   "source": [
    "# 导入tiktoken库用于分词\n",
    "import tiktoken\n",
    "\n",
    "# 获取GPT-2的分词器\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# 打印<|endoftext|>标记的编码ID\n",
    "# allowed_special参数允许对特殊标记进行编码\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f582ff-68bf-450e-bd87-5fb61afe431c",
   "metadata": {
    "id": "04f582ff-68bf-450e-bd87-5fb61afe431c"
   },
   "source": [
    "- The `SpamDataset` class below identifies the longest sequence in the training dataset and adds the padding token to the others to match that sequence length\n",
    "- 下面的`SpamDataset`类识别训练数据集中最长的序列，并为其他序列添加填充标记以匹配该序列长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7791b52-af18-4ac4-afa9-b921068e383e",
   "metadata": {
    "id": "d7791b52-af18-4ac4-afa9-b921068e383e"
   },
   "outputs": [],
   "source": [
    "# 导入必要的PyTorch库\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        # 读取CSV文件数据\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        # 对文本进行预分词处理\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
    "        ]\n",
    "\n",
    "        if max_length is None:\n",
    "            # 如果未指定最大长度,则使用数据集中最长序列的长度\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            # 使用指定的最大长度\n",
    "            self.max_length = max_length\n",
    "            # 如果序列长度超过max_length则进行截断\n",
    "            self.encoded_texts = [\n",
    "                encoded_text[:self.max_length]\n",
    "                for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "\n",
    "        # 将所有序列填充到最长序列的长度\n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 获取指定索引的编码文本和标签\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        # 将数据转换为张量格式\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回数据集的大小\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_length(self):\n",
    "        # 计算数据集中最长序列的长度\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uzj85f8ou82h",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uzj85f8ou82h",
    "outputId": "d08f1cf0-c24d-445f-a3f8-793532c3716f"
   },
   "outputs": [],
   "source": [
    "# 创建训练数据集\n",
    "# 使用SpamDataset类加载训练数据\n",
    "# max_length设为None表示使用数据集中最长序列的长度\n",
    "train_dataset = SpamDataset(\n",
    "    csv_file=\"train.csv\",\n",
    "    max_length=None,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# 打印训练数据集中最长序列的长度\n",
    "print(train_dataset.max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bdd932-97eb-4b88-9cf9-d766ea4c3a60",
   "metadata": {},
   "source": [
    "- We also pad the validation and test set to the longest training sequence\n",
    "- 我们还将验证集和测试集填充到与训练序列相同的最大长度\n",
    "- Note that validation and test set samples that are longer than the longest training example are being truncated via `encoded_text[:self.max_length]` in the `SpamDataset` code \n",
    "- 请注意，在SpamDataset代码中，长于最长训练样本的验证集和测试集样本会通过`encoded_text[:self.max_length]`被截断\n",
    "- This behavior is entirely optional, and it would also work well if we set `max_length=None` in both the validation and test set cases\n",
    "- 这种行为完全是可选的，如果我们在验证集和测试集中都设置`max_length=None`也同样可以正常工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb0c502d-a75e-4248-8ea0-196e2b00c61e",
   "metadata": {
    "id": "bb0c502d-a75e-4248-8ea0-196e2b00c61e"
   },
   "outputs": [],
   "source": [
    "# 创建验证数据集\n",
    "# 使用与训练集相同的最大长度\n",
    "val_dataset = SpamDataset(\n",
    "    csv_file=\"validation.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# 创建测试数据集 \n",
    "# 同样使用与训练集相同的最大长度\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=\"test.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20170d89-85a0-4844-9887-832f5d23432a",
   "metadata": {},
   "source": [
    "- Next, we use the dataset to instantiate the data loaders, which is similar to creating the data loaders in previous chapters\n",
    "- 接下来，我们使用数据集来实例化数据加载器，这与在前几章中创建数据加载器的方式类似"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bcc349-205f-48f8-9655-95ff21f5e72f",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/batch.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8681adc0-6f02-4e75-b01a-a6ab75d05542",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8681adc0-6f02-4e75-b01a-a6ab75d05542",
    "outputId": "3266c410-4fdb-4a8c-a142-7f707e2525ab"
   },
   "outputs": [],
   "source": [
    "# 导入DataLoader类用于加载数据\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 设置数据加载的参数\n",
    "num_workers = 0  # 数据加载的工作进程数\n",
    "batch_size = 8   # 每个批次的样本数\n",
    "\n",
    "# 设置随机种子以确保可重复性\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# 创建训练数据加载器\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,     # 训练数据集\n",
    "    batch_size=batch_size,     # 批次大小\n",
    "    shuffle=True,              # 随机打乱数据\n",
    "    num_workers=num_workers,   # 工作进程数\n",
    "    drop_last=True,           # 丢弃最后不完整的批次\n",
    ")\n",
    "\n",
    "# 创建验证数据加载器\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,       # 验证数据集\n",
    "    batch_size=batch_size,     # 批次大小\n",
    "    num_workers=num_workers,   # 工作进程数\n",
    "    drop_last=False,          # 保留最后不完整的批次\n",
    ")\n",
    "\n",
    "# 创建测试数据加载器\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,      # 测试数据集\n",
    "    batch_size=batch_size,     # 批次大小\n",
    "    num_workers=num_workers,   # 工作进程数\n",
    "    drop_last=False,          # 保留最后不完整的批次\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7335db-e0bb-4e27-80c5-eea11e593a57",
   "metadata": {},
   "source": [
    "- As a verification step, we iterate through the data loaders and ensure that the batches contain 8 training examples each, where each training example consists of 120 tokens\n",
    "- 作为验证步骤，我们遍历数据加载器并确保每个批次包含8个训练样本，其中每个训练样本由120个标记组成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dee6882-4c3a-4964-af15-fa31f86ad047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印训练数据加载器信息\n",
    "print(\"Train loader:\")\n",
    "\n",
    "# 遍历训练数据加载器中的一个批次\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "# 打印输入批次的维度\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "# 打印标签批次的维度\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdd7947-7039-49bf-8a5e-c0a2f4281ca1",
   "metadata": {},
   "source": [
    "- Lastly, let's print the total number of batches in each dataset\n",
    "- 最后，让我们打印每个数据集中的批次总数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IZfw-TYD2zTj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IZfw-TYD2zTj",
    "outputId": "6934bbf2-9797-4fbe-d26b-1a246e18c2fb"
   },
   "outputs": [],
   "source": [
    "# 打印训练数据加载器中的批次总数\n",
    "print(f\"{len(train_loader)} training batches\")\n",
    "# 打印验证数据加载器中的批次总数 \n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "# 打印测试数据加载器中的批次总数\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c4f61a-5f5d-4b3b-97cf-151b617d1d6c",
   "metadata": {
    "id": "d1c4f61a-5f5d-4b3b-97cf-151b617d1d6c"
   },
   "source": [
    "## 6.4 Initializing a model with pretrained weights\n",
    "## 6.4 使用预训练权重初始化模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e1af8b-8bd1-4b44-8b8b-dc031496e208",
   "metadata": {},
   "source": [
    "- In this section, we initialize the pretrained model we worked with in the previous chapter\n",
    "- 在本节中，我们将初始化在上一章中使用的预训练模型\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/overview-2.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2992d779-f9fb-4812-a117-553eb790a5a9",
   "metadata": {
    "id": "2992d779-f9fb-4812-a117-553eb790a5a9"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 选择要使用的GPT-2模型大小\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "# 设置输入提示文本\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "# 基础配置参数\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # 词汇表大小\n",
    "    \"context_length\": 1024,  # 上下文长度\n",
    "    \"drop_rate\": 0.0,        # Dropout比率\n",
    "    \"qkv_bias\": True         # 是否使用Query-Key-Value偏置\n",
    "}\n",
    "\n",
    "# 不同规模GPT-2模型的配置参数\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},    # 小型模型(1.24亿参数)\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},  # 中型模型(3.55亿参数)\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},   # 大型模型(7.74亿参数)\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},     # 超大型模型(15.58亿参数)\n",
    "}\n",
    "\n",
    "# 更新基础配置,添加所选模型的特定参数\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "# 确保训练数据集的最大长度不超过模型的上下文长度限制\n",
    "assert train_dataset.max_length <= BASE_CONFIG[\"context_length\"], (\n",
    "    f\"Dataset length {train_dataset.max_length} exceeds model's context \"\n",
    "    f\"length {BASE_CONFIG['context_length']}. Reinitialize data sets with \"\n",
    "    f\"`max_length={BASE_CONFIG['context_length']}`\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022a649a-44f5-466c-8a8e-326c063384f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "022a649a-44f5-466c-8a8e-326c063384f5",
    "outputId": "7091e401-8442-4f47-a1d9-ecb42a1ef930"
   },
   "outputs": [],
   "source": [
    "# 导入所需的模块\n",
    "from gpt_download import download_and_load_gpt2\n",
    "from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "\n",
    "# 从CHOOSE_MODEL中提取模型大小信息\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "# 下载并加载预训练的GPT-2模型\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "# 使用BASE_CONFIG初始化GPT模型\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "# 将预训练权重加载到模型中\n",
    "load_weights_into_gpt(model, params)\n",
    "# 将模型设置为评估模式\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e056c-abe0-415f-b34d-df686204259e",
   "metadata": {},
   "source": [
    "- To ensure that the model was loaded correctly, let's double-check that it generates coherent text\n",
    "- 为了确保模型加载正确,让我们再次检查它是否能生成连贯的文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ac25ff-74b1-4149-8dc5-4c429d464330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的函数\n",
    "from previous_chapters import (\n",
    "    generate_text_simple,    # 用于生成文本的简单函数\n",
    "    text_to_token_ids,       # 将文本转换为token ID的函数\n",
    "    token_ids_to_text        # 将token ID转换回文本的函数\n",
    ")\n",
    "\n",
    "\n",
    "# 设置输入提示文本\n",
    "text_1 = \"Every effort moves you\"\n",
    "\n",
    "# 使用模型生成文本\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,                            # 预训练的GPT-2模型\n",
    "    idx=text_to_token_ids(text_1, tokenizer), # 将输入文本转换为token ID\n",
    "    max_new_tokens=15,                      # 生成15个新token\n",
    "    context_size=BASE_CONFIG[\"context_length\"] # 使用模型的上下文长度\n",
    ")\n",
    "\n",
    "# 将生成的token ID转换回可读文本并打印\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69162550-6a02-4ece-8db1-06c71d61946f",
   "metadata": {},
   "source": [
    "- Before we finetune the model as a classifier, let's see if the model can perhaps already classify spam messages via prompting\n",
    "- 在我们将模型微调为分类器之前，让我们看看模型是否已经可以通过提示来分类垃圾邮件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94224aa9-c95a-4f8a-a420-76d01e3a800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置测试文本,询问模型是否为垃圾邮件\n",
    "text_2 = (\n",
    "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"  # 提示模型回答是或否\n",
    "    \" 'You are a winner you have been specially\"                # 测试文本内容\n",
    "    \" selected to receive $1000 cash or a $2000 award.'\"       # 典型的垃圾邮件内容\n",
    ")\n",
    "\n",
    "# 使用模型生成回答\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,                                               # 使用预训练的GPT-2模型\n",
    "    idx=text_to_token_ids(text_2, tokenizer),                 # 将提示文本转换为token ID\n",
    "    max_new_tokens=23,                                        # 生成23个新token\n",
    "    context_size=BASE_CONFIG[\"context_length\"]                # 使用模型的上下文长度\n",
    ")\n",
    "\n",
    "# 将生成的token ID转换回可读文本并打印\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce39ed0-2c77-410d-8392-dd15d4b22016",
   "metadata": {},
   "source": [
    "- As we can see, the model is not very good at following instructions\n",
    "- 正如我们所见，该模型并不擅长遵循指令\n",
    "- This is expected, since it has only been pretrained and not instruction-finetuned (instruction finetuning will be covered in the next chapter)\n",
    "- 这是意料之中的，因为它只经过了预训练，而没有经过指令微调(指令微调将在下一章介绍)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9ae440-32f9-412f-96cf-fd52cc3e2522",
   "metadata": {
    "id": "4c9ae440-32f9-412f-96cf-fd52cc3e2522"
   },
   "source": [
    "## 6.5 Adding a classification head\n",
    "## 6.5 添加分类头"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e9d66f-76b2-40fc-9ec5-3f972a8db9c0",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/lm-head.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217bac05-78df-4412-bd80-612f8061c01d",
   "metadata": {},
   "source": [
    "- In this section, we are modifying the pretrained LLM to make it ready for classification finetuning\n",
    "- 在本节中，我们将修改预训练的LLM，使其准备好进行分类微调\n",
    "- Let's take a look at the model architecture first\n",
    "- 让我们先看看模型架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23aff91-6bd0-48da-88f6-353657e6c981",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d8f7a01-b7c0-48d4-b1e7-8c12cc7ad932",
    "outputId": "b6a5b9b5-a92f-498f-d7cb-b58dd99e4497"
   },
   "outputs": [],
   "source": [
    "# 打印模型信息\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f640a76-dd00-4769-9bc8-1aed0cec330d",
   "metadata": {},
   "source": [
    "- Above, we can see the architecture we implemented in chapter 4 neatly laid out\n",
    "- 在上面,我们可以清晰地看到在第4章中实现的架构\n",
    "- The goal is to replace and finetune the output layer\n",
    "- 目标是替换和微调输出层\n",
    "- To achieve this, we first freeze the model, meaning that we make all layers non-trainable\n",
    "- 为了实现这一点,我们首先冻结模型,这意味着将所有层设置为不可训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fkMWFl-0etea",
   "metadata": {
    "id": "fkMWFl-0etea"
   },
   "outputs": [],
   "source": [
    "# 冻结所有模型参数,使其不可训练\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72155f83-87d9-476a-a978-a15aa2d44147",
   "metadata": {},
   "source": [
    "- Then, we replace the output layer (`model.out_head`), which originally maps the layer inputs to 50,257 dimensions (the size of the vocabulary)\n",
    "- 然后，我们替换输出层(`model.out_head`)，它原本将层输入映射到50,257维(词汇表的大小)\n",
    "- Since we finetune the model for binary classification (predicting 2 classes, \"spam\" and \"not spam\"), we can replace the output layer as shown below, which will be trainable by default\n",
    "- 由于我们对模型进行二分类微调(预测\"垃圾邮件\"和\"非垃圾邮件\"两个类别)，我们可以像下面所示替换输出层，该层默认是可训练的\n",
    "- Note that we use `BASE_CONFIG[\"emb_dim\"]` (which is equal to 768 in the `\"gpt2-small (124M)\"` model) to keep the code below more general\n",
    "- 注意，我们使用`BASE_CONFIG[\"emb_dim\"]`(在`\"gpt2-small (124M)\"`模型中等于768)来保持下面的代码更具通用性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e759fa0-0f69-41be-b576-17e5f20e04cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子以确保可重复性\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# 定义输出类别数量(垃圾邮件/非垃圾邮件)\n",
    "num_classes = 2\n",
    "# 替换输出层,将嵌入维度映射到类别数量\n",
    "model.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30be5475-ae77-4f97-8f3e-dec462b1339f",
   "metadata": {},
   "source": [
    "- Technically, it's sufficient to only train the output layer\n",
    "- 从技术上讲,仅训练输出层就足够了\n",
    "- However, as I found in [Finetuning Large Language Models](https://magazine.sebastianraschka.com/p/finetuning-large-language-models), experiments show that finetuning additional layers can noticeably improve the performance  \n",
    "- 然而,正如我在[微调大型语言模型](https://magazine.sebastianraschka.com/p/finetuning-large-language-models)中发现的,实验表明微调额外的层可以显著提高性能\n",
    "- So, we are also making the last transformer block and the final `LayerNorm` module connecting the last transformer block to the output layer trainable\n",
    "- 因此,我们还将最后一个transformer块和连接最后一个transformer块到输出层的最终`LayerNorm`模块设置为可训练的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be7c1eb-c46c-4065-8525-eea1b8c66d10",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/trainable.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2aedc120-5ee3-48f6-92f2-ad9304ebcdc7",
   "metadata": {
    "id": "2aedc120-5ee3-48f6-92f2-ad9304ebcdc7"
   },
   "outputs": [],
   "source": [
    "# 将最后一个transformer块设置为可训练\n",
    "for param in model.trf_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 将最终的LayerNorm层设置为可训练 \n",
    "for param in model.final_norm.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f012b899-8284-4d3a-97c0-8a48eb33ba2e",
   "metadata": {},
   "source": [
    "- We can still use this model similar to before in previous chapters\n",
    "- 我们仍然可以像前几章一样使用这个模型\n",
    "- For example, let's feed it some text input  \n",
    "- 例如,让我们输入一些文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f645c06a-7df6-451c-ad3f-eafb18224ebc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f645c06a-7df6-451c-ad3f-eafb18224ebc",
    "outputId": "27e041b1-d731-48a1-cf60-f22d4565304e"
   },
   "outputs": [],
   "source": [
    "# 对输入文本进行编码\n",
    "inputs = tokenizer.encode(\"Do you have time\")\n",
    "# 将编码后的输入转换为tensor并增加一个batch维度\n",
    "inputs = torch.tensor(inputs).unsqueeze(0)\n",
    "# 打印输入内容\n",
    "print(\"Inputs:\", inputs)\n",
    "# 打印输入维度 - 形状为(batch_size, num_tokens)\n",
    "print(\"Inputs dimensions:\", inputs.shape) # shape: (batch_size, num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbf8481-772d-467b-851c-a62b86d0cb1b",
   "metadata": {},
   "source": [
    "- What's different compared to previous chapters is that it now has two output dimensions instead of 50,257\n",
    "- 与前几章不同的是,现在它有两个输出维度而不是50,257个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dc84f1-85cc-4609-9cee-94ff539f00f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48dc84f1-85cc-4609-9cee-94ff539f00f4",
    "outputId": "9cae7448-253d-4776-973e-0af190b06354"
   },
   "outputs": [],
   "source": [
    "# 使用torch.no_grad()避免计算梯度\n",
    "with torch.no_grad():\n",
    "    # 将输入传入模型得到输出\n",
    "    outputs = model(inputs)\n",
    "\n",
    "# 打印输出内容\n",
    "print(\"Outputs:\\n\", outputs)\n",
    "# 打印输出维度 - 形状为(batch_size, num_tokens, num_classes)\n",
    "print(\"Outputs dimensions:\", outputs.shape) # shape: (batch_size, num_tokens, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75430a01-ef9c-426a-aca0-664689c4f461",
   "metadata": {},
   "source": [
    "- As discussed in previous chapters, for each input token, there's one output vector\n",
    "- 如前几章所述,对于每个输入token,都会有一个对应的输出向量\n",
    "- Since we fed the model a text sample with 4 input tokens, the output consists of 4 2-dimensional output vectors above  \n",
    "- 由于我们输入了包含4个token的文本样本,所以输出包含了上面的4个二维输出向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df9144f-6817-4be4-8d4b-5d4dadfe4a9b",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/input-and-output.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb8616-c791-4f5c-bac0-5302f663e46a",
   "metadata": {},
   "source": [
    "- In chapter 3, we discussed the attention mechanism, which connects each input token to each other input token\n",
    "- 在第3章中,我们讨论了注意力机制,它将每个输入token与其他所有输入token相连接\n",
    "\n",
    "- In chapter 3, we then also introduced the causal attention mask that is used in GPT-like models; this causal mask lets a current token only attend to the current and previous token positions\n",
    "- 在第3章中,我们还介绍了GPT类模型中使用的因果注意力掩码;这种因果掩码只允许当前token关注当前和之前的token位置\n",
    "\n",
    "- Based on this causal attention mechanism, the 4th (last) token contains the most information among all tokens because it's the only token that includes information about all other tokens\n",
    "- 基于这种因果注意力机制,第4个(最后一个)token包含了所有token中最多的信息,因为它是唯一包含了所有其他token信息的token\n",
    "\n",
    "- Hence, we are particularly interested in this last token, which we will finetune for the spam classification task\n",
    "- 因此,我们特别关注这个最后的token,我们将用它来微调垃圾邮件分类任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49383a8c-41d5-4dab-98f1-238bca0c2ed7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49383a8c-41d5-4dab-98f1-238bca0c2ed7",
    "outputId": "e79eb155-fa1f-46ed-ff8c-d828c3a3fabd"
   },
   "outputs": [],
   "source": [
    "# 打印最后一个输出token\n",
    "print(\"Last output token:\", outputs[:, -1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df08ae0-e664-4670-b7c5-8a2280d9b41b",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/attention-mask.webp\" width=200px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aa4aef-e1e9-491b-9adf-5aa973e59b8c",
   "metadata": {},
   "source": [
    "## 6.6 Calculating the classification loss and accuracy\n",
    "## 6.6 计算分类损失和准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669e1fd1-ace8-44b4-b438-185ed0ba8b33",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/overview-3.webp?1\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7df4ee-0a34-4a4d-896d-affbbf81e0b3",
   "metadata": {},
   "source": [
    "- Before explaining the loss calculation, let's have a brief look at how the model outputs are turned into class labels\n",
    "- 在解释损失计算之前,让我们先简单看看模型输出是如何转换为类别标签的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557996dd-4c6b-49c4-ab83-f60ef7e1d69e",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/class-argmax.webp\" width=600px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77faab1-3461-4118-866a-6171f2b89aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印最后一个输出token的值\n",
    "print(\"Last output token:\", outputs[:, -1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edd71fa-628a-4d00-b81d-6d8bcb2c341d",
   "metadata": {},
   "source": [
    "- Similar to chapter 5, we convert the outputs (logits) into probability scores via the `softmax` function and then obtain the index position of the largest probability value via the `argmax` function\n",
    "- 与第5章类似,我们通过`softmax`函数将输出(logits)转换为概率分数,然后通过`argmax`函数获取最大概率值的索引位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81efa92-9be1-4b9e-8790-ce1fc7b17f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过softmax函数将输出转换为概率分布\n",
    "probas = torch.softmax(outputs[:, -1, :], dim=-1)\n",
    "# 获取概率最大的类别索引作为预测标签\n",
    "label = torch.argmax(probas)\n",
    "# 打印预测的类别标签\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414a6f02-307e-4147-a416-14d115bf8179",
   "metadata": {},
   "source": [
    "- Note that the softmax function is optional here, as explained in chapter 5, because the largest outputs correspond to the largest probability scores\n",
    "- 注意这里的softmax函数是可选的,正如第5章所解释的,因为最大的输出对应最大的概率分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f9ad66-4969-4501-8239-3ccdb37e71a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取最后一个token的输出logits\n",
    "logits = outputs[:, -1, :]\n",
    "# 获取概率最大的类别索引作为预测标签\n",
    "label = torch.argmax(logits)\n",
    "# 打印预测的类别标签\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb20d3a-cbba-4ab1-8584-d94e16589505",
   "metadata": {},
   "source": [
    "- We can apply this concept to calculate the so-called classification accuracy, which computes the percentage of correct predictions in a given dataset\n",
    "- 我们可以应用这个概念来计算所谓的分类准确率,它计算给定数据集中正确预测的百分比\n",
    "- To calculate the classification accuracy, we can apply the preceding `argmax`-based prediction code to all examples in a dataset and calculate the fraction of correct predictions as follows:\n",
    "- 为了计算分类准确率,我们可以将前面基于`argmax`的预测代码应用到数据集中的所有样本,并按如下方式计算正确预测的比例:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ecf9572-aed0-4a21-9c3b-7f9f2aec5f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    # 将模型设置为评估模式\n",
    "    model.eval()\n",
    "    # 初始化正确预测数和样本总数\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "\n",
    "    # 如果未指定批次数,使用整个数据加载器的长度\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # 否则取指定批次数和数据加载器长度的较小值\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            # 将输入和目标批次移到指定设备\n",
    "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "            # 在不计算梯度的情况下进行前向传播\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_batch)[:, -1, :]  # 获取最后一个输出token的logits\n",
    "            # 获取预测标签\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # 累加样本数和正确预测数\n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
    "        else:\n",
    "            break\n",
    "    # 返回准确率\n",
    "    return correct_predictions / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7165fe46-a284-410b-957f-7524877d1a1a",
   "metadata": {},
   "source": [
    "- Let's apply the function to calculate the classification accuracies for the different datasets:\n",
    "- 让我们应用这个函数来计算不同数据集的分类准确率:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390e5255-8427-488c-adef-e1c10ab4fb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查是否有可用的CUDA设备,如果有则使用GPU,否则使用CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 注意:\n",
    "# 取消注释以下代码可以在Apple Silicon芯片上运行(如果适用)\n",
    "# 根据在M3 MacBook Air上的测试,速度大约是Apple CPU的2倍\n",
    "# 在PyTorch 2.4版本中,CPU和MPS的结果是相同的\n",
    "# 但在早期版本的PyTorch中,使用MPS可能会得到不同的结果\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device(\"cuda\")\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "#else:\n",
    "#    device = torch.device(\"cpu\")\n",
    "#print(f\"Running on {device} device.\")\n",
    "\n",
    "# 将模型移动到指定设备(对于nn.Module类,不需要赋值 model = model.to(device))\n",
    "model.to(device)\n",
    "\n",
    "# 设置随机种子以确保训练数据加载器中的随机打乱可重现\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# 计算训练、验证和测试集的准确率\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "# 打印各数据集的准确率\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30345e2a-afed-4d22-9486-f4010f90a871",
   "metadata": {},
   "source": [
    "- As we can see, the prediction accuracies are not very good, since we haven't finetuned the model, yet\n",
    "- 正如我们所看到的,由于我们还没有对模型进行微调,预测准确率并不是很理想"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4a9d15-8fc7-48a2-8734-d92a2f265328",
   "metadata": {},
   "source": [
    "- Before we can start finetuning (/training), we first have to define the loss function we want to optimize during training\n",
    "- 在开始微调(训练)之前,我们首先需要定义在训练过程中要优化的损失函数\n",
    "\n",
    "- The goal is to maximize the spam classification accuracy of the model; however, classification accuracy is not a differentiable function\n",
    "- 目标是最大化模型的垃圾邮件分类准确率;然而,分类准确率不是一个可微函数\n",
    "\n",
    "- Hence, instead, we minimize the cross-entropy loss as a proxy for maximizing the classification accuracy (you can learn more about this topic in lecture 8 of my freely available [Introduction to Deep Learning](https://sebastianraschka.com/blog/2021/dl-course.html#l08-multinomial-logistic-regression--softmax-regression) class)\n",
    "- 因此,我们转而通过最小化交叉熵损失来间接实现最大化分类准确率的目标(你可以在我免费提供的[深度学习入门](https://sebastianraschka.com/blog/2021/dl-course.html#l08-multinomial-logistic-regression--softmax-regression)课程的第8讲中了解更多相关内容)\n",
    "\n",
    "- The `calc_loss_batch` function is the same here as in chapter 5, except that we are only interested in optimizing the last token `model(input_batch)[:, -1, :]` instead of all tokens `model(input_batch)`\n",
    "- `calc_loss_batch`函数与第5章中的相同,只是我们只关注优化最后一个token `model(input_batch)[:, -1, :]`,而不是所有token `model(input_batch)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f1e9547-806c-41a9-8aba-3b2822baabe4",
   "metadata": {
    "id": "2f1e9547-806c-41a9-8aba-3b2822baabe4"
   },
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    # 将输入和目标批次移动到指定设备\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    \n",
    "    # 获取模型输出的最后一个token的logits\n",
    "    # shape: [batch_size, vocab_size]\n",
    "    logits = model(input_batch)[:, -1, :]  \n",
    "    \n",
    "    # 计算交叉熵损失\n",
    "    # logits shape: [batch_size, vocab_size]\n",
    "    # target_batch shape: [batch_size]\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a013aab9-f854-4866-ad55-5b8350adb50a",
   "metadata": {},
   "source": [
    "The `calc_loss_loader` is exactly the same as in chapter 5\n",
    "`calc_loss_loader` 函数与第5章中的完全相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7b83e10-5720-45e7-ac5e-369417ca846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as in chapter 5\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    # 初始化总损失为0\n",
    "    total_loss = 0.\n",
    "    \n",
    "    # 如果数据加载器为空,返回nan\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        # 如果未指定批次数,使用数据加载器的全部批次\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # 如果指定的批次数超过数据加载器的批次数\n",
    "        # 则将批次数减少到与数据加载器的批次数相匹配\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "        \n",
    "    # 遍历数据加载器中的批次\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            # 计算当前批次的损失\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    # 返回平均损失\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56826ecd-6e74-40e6-b772-d3541e585067",
   "metadata": {},
   "source": [
    "- Using the `calc_closs_loader`, we compute the initial training, validation, and test set losses before we start training\n",
    "- 使用`calc_closs_loader`函数,我们在开始训练之前计算初始的训练集、验证集和测试集的损失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f00e53-5beb-4e64-b147-f26fd481c6ff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6f00e53-5beb-4e64-b147-f26fd481c6ff",
    "outputId": "49df8648-9e38-4314-854d-9faacd1b2e89"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad(): # 禁用梯度跟踪以提高效率,因为此时还未开始训练\n",
    "    # 计算训练集损失值\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    # 计算验证集损失值 \n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "    # 计算测试集损失值\n",
    "    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
    "\n",
    "# 打印各数据集的损失值\n",
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\") \n",
    "print(f\"Test loss: {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04b980b-e583-4f62-84a0-4edafaf99d5d",
   "metadata": {},
   "source": [
    "- In the next section, we train the model to improve the loss values and consequently the classification accuracy\n",
    "- 在下一节中,我们将训练模型以改善损失值,从而提高分类准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456ae0fd-6261-42b4-ab6a-d24289953083",
   "metadata": {
    "id": "456ae0fd-6261-42b4-ab6a-d24289953083"
   },
   "source": [
    "## 6.7 Finetuning the model on supervised data\n",
    "## 6.7 在有监督数据上微调模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9b099b-0829-4f72-8a2b-4363e3497026",
   "metadata": {},
   "source": [
    "- In this section, we define and use the training function to improve the classification accuracy of the model\n",
    "- 在本节中,我们定义并使用训练函数来提高模型的分类准确率\n",
    "- The `train_classifier_simple` function below is practically the same as the `train_model_simple` function we used for pretraining the model in chapter 5  \n",
    "- 下面的`train_classifier_simple`函数实际上与我们在第5章用于预训练模型的`train_model_simple`函数相同\n",
    "- The only two differences are that we now\n",
    "- 现在只有两个区别:\n",
    "  1. track the number of training examples seen (`examples_seen`) instead of the number of tokens seen\n",
    "  1. 跟踪已处理的训练样本数量(`examples_seen`)而不是已处理的标记数量\n",
    "  2. calculate the accuracy after each epoch instead of printing a sample text after each epoch\n",
    "  2. 在每个epoch结束后计算准确率,而不是打印样本文本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979b6222-1dc2-4530-9d01-b6b04fe3de12",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/training-loop.webp?1\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "Csbr60to50FL",
   "metadata": {
    "id": "Csbr60to50FL"
   },
   "outputs": [],
   "source": [
    "# 总体上与第5章的`train_model_simple`相同\n",
    "def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                            eval_freq, eval_iter):\n",
    "    # 初始化列表用于跟踪损失值和已处理的样本数\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    examples_seen, global_step = 0, -1  # 初始化已处理样本数和全局步数\n",
    "\n",
    "    # 主训练循环\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # 将模型设置为训练模式\n",
    "\n",
    "        # 遍历训练数据加载器中的每个批次\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  # 重置上一批次的梯度\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)  # 计算当前批次的损失值\n",
    "            loss.backward()  # 计算损失梯度\n",
    "            optimizer.step()  # 使用梯度更新模型权重\n",
    "            examples_seen += input_batch.shape[0]  # 更新已处理的样本数\n",
    "            global_step += 1  # 更新全局步数\n",
    "\n",
    "            # 定期评估模型性能\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)  # 评估训练集和验证集的损失\n",
    "                train_losses.append(train_loss)  # 记录训练损失\n",
    "                val_losses.append(val_loss)  # 记录验证损失\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"  # 打印当前训练状态\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # 每个epoch结束后计算准确率\n",
    "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)  # 计算训练集准确率\n",
    "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)  # 计算验证集准确率\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")  # 打印训练准确率\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")  # 打印验证准确率\n",
    "        train_accs.append(train_accuracy)  # 记录训练准确率\n",
    "        val_accs.append(val_accuracy)  # 记录验证准确率\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen  # 返回训练过程中记录的所有指标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9624cb30-3e3a-45be-b006-c00475b58ae8",
   "metadata": {},
   "source": [
    "- The `evaluate_model` function used in the `train_classifier_simple` is the same as the one we used in chapter 5\n",
    "- 在`train_classifier_simple`中使用的`evaluate_model`函数与我们在第5章中使用的相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bcc7bc04-6aa6-4516-a147-460e2f466eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 与第5章相同的函数\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    \"\"\"评估模型在训练集和验证集上的性能\n",
    "    \n",
    "    Args:\n",
    "        model: 要评估的模型\n",
    "        train_loader: 训练数据加载器\n",
    "        val_loader: 验证数据加载器 \n",
    "        device: 运行设备(CPU/GPU)\n",
    "        eval_iter: 评估时使用的批次数量\n",
    "        \n",
    "    Returns:\n",
    "        train_loss: 训练集上的损失值\n",
    "        val_loss: 验证集上的损失值\n",
    "    \"\"\"\n",
    "    model.eval()  # 将模型设置为评估模式\n",
    "    with torch.no_grad():  # 不计算梯度\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)  # 计算训练损失\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)  # 计算验证损失\n",
    "    model.train()  # 将模型恢复为训练模式\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e807bfe9-364d-46b2-9e25-3b000c3ef6f9",
   "metadata": {},
   "source": [
    "- The training takes about 5 minutes on a M3 MacBook Air laptop computer and less than half a minute on a V100 or A100 GPU\n",
    "- 在 M3 MacBook Air 笔记本电脑上训练大约需要5分钟，而在 V100 或 A100 GPU 上训练不到半分钟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X7kU3aAj7vTJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X7kU3aAj7vTJ",
    "outputId": "504a033e-2bf8-41b5-a037-468309845513"
   },
   "outputs": [],
   "source": [
    "# 导入时间模块用于计时\n",
    "import time\n",
    "\n",
    "# 记录开始时间\n",
    "start_time = time.time()\n",
    "\n",
    "# 设置随机种子以确保结果可重现\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# 创建优化器,使用AdamW算法,设置学习率和权重衰减\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "# 设置训练轮数为5\n",
    "num_epochs = 5\n",
    "# 调用训练函数开始训练,返回训练过程中的各项指标\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
    ")\n",
    "\n",
    "# 记录结束时间\n",
    "end_time = time.time()\n",
    "# 计算总训练时间(分钟)\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "# 打印训练完成时间\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1261bf90-3ce7-4591-895a-044a05538f30",
   "metadata": {},
   "source": [
    "- Similar to chapter 5, we use matplotlib to plot the loss function for the training and validation set\n",
    "- 与第5章类似,我们使用matplotlib绘制训练集和验证集的损失函数图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cURgnDqdCeka",
   "metadata": {
    "id": "cURgnDqdCeka"
   },
   "outputs": [],
   "source": [
    "# 导入matplotlib库用于绘图\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
    "    # 创建一个5x3大小的图形和坐标轴对象\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # 绘制训练和验证损失随epochs变化的曲线\n",
    "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")  # 绘制训练曲线\n",
    "    ax1.plot(epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\")  # 绘制验证曲线\n",
    "    ax1.set_xlabel(\"Epochs\")  # 设置x轴标签\n",
    "    ax1.set_ylabel(label.capitalize())  # 设置y轴标签\n",
    "    ax1.legend()  # 添加图例\n",
    "\n",
    "    # 创建第二个x轴用于显示已处理的样本数\n",
    "    ax2 = ax1.twiny()  # 创建共享y轴的第二个x轴\n",
    "    ax2.plot(examples_seen, train_values, alpha=0)  # 绘制不可见的曲线以对齐刻度\n",
    "    ax2.set_xlabel(\"Examples seen\")  # 设置第二个x轴的标签\n",
    "\n",
    "    # 调整布局以适应所有元素\n",
    "    fig.tight_layout()\n",
    "    # 保存图形为PDF文件\n",
    "    plt.savefig(f\"{label}-plot.pdf\")\n",
    "    # 显示图形\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OIqRt466DiGk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "OIqRt466DiGk",
    "outputId": "b16987cf-0001-4652-ddaf-02f7cffc34db"
   },
   "outputs": [],
   "source": [
    "# 创建一个从0到num_epochs的等间隔张量,长度与train_losses相同\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "# 创建一个从0到examples_seen的等间隔张量,长度与train_losses相同\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "# 绘制训练和验证损失曲线\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd28174-1836-44ba-b6c0-7e0be774fadc",
   "metadata": {},
   "source": [
    "- Above, based on the downward slope, we see that the model learns well\n",
    "- 从上面的下降趋势可以看出，模型学习效果良好\n",
    "- Furthermore, the fact that the training and validation loss are very close indicates that the model does not tend to overfit the training data  \n",
    "- 此外，训练损失和验证损失非常接近这一事实表明，模型不倾向于过拟合训练数据\n",
    "- Similarly, we can plot the accuracy below\n",
    "- 同样，我们可以在下面绘制准确率图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yz8BIsaF0TUo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "yz8BIsaF0TUo",
    "outputId": "3a7ed967-1f2a-4c6d-f4a3-0cc8cc9d6c5f"
   },
   "outputs": [],
   "source": [
    "# 创建一个从0到num_epochs的等间隔张量,长度与train_accs相同\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\n",
    "# 创建一个从0到examples_seen的等间隔张量,长度与train_accs相同 \n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n",
    "\n",
    "# 绘制训练和验证准确率曲线\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_accs, val_accs, label=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aba699-21bc-42de-a69c-99f370bb0363",
   "metadata": {},
   "source": [
    "- Based on the accuracy plot above, we can see that the model achieves a relatively high training and validation accuracy after epochs 4 and 5\n",
    "- However, we have to keep in mind that we specified `eval_iter=5` in the training function earlier, which means that we only estimated the training and validation set performances\n",
    "- We can compute the training, validation, and test set performances over the complete dataset as follows below\n",
    "- 根据上面的准确率图，我们可以看到模型在第4和第5个epoch后获得了相对较高的训练和验证准确率\n",
    "- 然而，我们必须记住，我们在训练函数中指定了`eval_iter=5`，这意味着我们只估计了训练和验证集的性能\n",
    "- 我们可以按照以下方式计算完整数据集上的训练、验证和测试集性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UHWaJFrjY0zW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UHWaJFrjY0zW",
    "outputId": "e111e6e6-b147-4159-eb9d-19d4e809ed34"
   },
   "outputs": [],
   "source": [
    "# 计算训练集、验证集和测试集的准确率\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "# 打印各个数据集的准确率\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6882649f-dc7b-401f-84d2-024ff79c74a1",
   "metadata": {},
   "source": [
    "- We can see that the training and validation set performances are practically identical\n",
    "- However, based on the slightly lower test set performance, we can see that the model overfits the training data to a very small degree, as well as the validation data that has been used for tweaking some of the hyperparameters, such as the learning rate\n",
    "- This is normal, however, and this gap could potentially be further reduced by increasing the model's dropout rate (`drop_rate`) or the `weight_decay` in the optimizer setting\n",
    "- 我们可以看到训练集和验证集的表现几乎相同\n",
    "- 然而，基于略低的测试集性能，我们可以看出模型在很小程度上过拟合训练数据，以及用于调整一些超参数（如学习率）的验证数据\n",
    "- 然而，这是正常的，这种差距可能通过增加模型的辍学率（`drop_rate`）或优化器设置中的`weight_decay`来进一步减少"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74d9ad7-3ec1-450e-8c9f-4fc46d3d5bb0",
   "metadata": {},
   "source": [
    "## 6.8 Using the LLM as a spam classifier\n",
    "## 6.8 使用LLM作为垃圾邮件分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ebcfa2-479e-408b-9cf0-7421f6144855",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/overview-4.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5408e6-83e4-4e5a-8503-c2fba6073f31",
   "metadata": {},
   "source": [
    "- Finally, let's use the finetuned GPT model in action\n",
    "- The `classify_review` function below implements the data preprocessing steps similar to the `SpamDataset` we implemented earlier\n",
    "- Then, the function returns the predicted integer class label from the model and returns the corresponding class name\n",
    " - 最后，让我们使用微调后的GPT模型\n",
    " - 下面的`classify_review`函数实现了类似于我们之前实现的`SpamDataset`的数据预处理步骤\n",
    " - 然后，该函数返回模型预测的整数类标签，并返回相应的类名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aHdn6xvL-IW5",
   "metadata": {
    "id": "aHdn6xvL-IW5"
   },
   "outputs": [],
   "source": [
    "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
    "    model.eval()\n",
    "\n",
    "    # 准备输入给模型\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    supported_context_length = model.pos_emb.weight.shape[0]\n",
    "    # 注意：在书中，这原本被错误地写成了pos_emb.weight.shape[1]\n",
    "    # 这不会破坏代码，但会导致不必要的截断（从1024到768）\n",
    "\n",
    "    # 如果序列过长，则截断\n",
    "    input_ids = input_ids[:min(max_length, supported_context_length)]\n",
    "\n",
    "    # 将序列填充到最长序列\n",
    "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
    "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0) # 添加批次维度\n",
    "\n",
    "    # 模型推理\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)[:, -1, :]  # 最后一个输出标记的对数\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    # 返回分类结果\n",
    "    return \"spam\" if predicted_label == 1 else \"not spam\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29682d8-a899-4d9b-b973-f8d5ec68172c",
   "metadata": {},
   "source": [
    "- Let's try it out on a few examples below\n",
    "- 让我们结合下面的例子来尝试它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apU_pf51AWSV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "apU_pf51AWSV",
    "outputId": "d0fde0a5-e7a3-4dbe-d9c5-0567dbab7e62"
   },
   "outputs": [],
   "source": [
    "text_1 = (\n",
    "    \"You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.\"\n",
    ")\n",
    "\n",
    "# 对文本进行分类\n",
    "print(classify_review(\n",
    "    text_1, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1g5VTOo_Ajs5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1g5VTOo_Ajs5",
    "outputId": "659b08eb-b6a9-4a8a-9af7-d94c757e93c2"
   },
   "outputs": [],
   "source": [
    "# 定义待分类的文本\n",
    "text_2 = (\n",
    "    \"Hey, just wanted to check if we're still on\"\n",
    "    \" for dinner tonight? Let me know!\"\n",
    ")\n",
    "\n",
    "# 调用 classify_review 函数对文本进行分类，并打印结果\n",
    "print(classify_review(\n",
    "    text_2, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf736e39-0d47-40c1-8d18-1f716cf7a81e",
   "metadata": {},
   "source": [
    " - Finally, let's save the model in case we want to reuse the model later without having to train it again\n",
    " - 最后，让我们保存模型，以便将来可以在不重新训练的情况下重用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "mYnX-gI1CfQY",
   "metadata": {
    "id": "mYnX-gI1CfQY"
   },
   "outputs": [],
   "source": [
    "# 保存模型的状态字典到文件 \"review_classifier.pth\"\n",
    "torch.save(model.state_dict(), \"review_classifier.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba78cf7c-6b80-4f71-a50e-3ccc73839af6",
   "metadata": {},
   "source": [
    "- Then, in a new session, we could load the model as follows\n",
    " - 然后，在一个新的会话中，我们可以按如下方式加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e68a5-d492-493b-87ef-45c475f353f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型的状态字典\n",
    "model_state_dict = torch.load(\"review_classifier.pth\", map_location=device, weights_only=True)\n",
    "# 将状态字典加载到模型中\n",
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b70ac71-234f-4eeb-b33d-c62726d50cd4",
   "metadata": {
    "id": "5b70ac71-234f-4eeb-b33d-c62726d50cd4"
   },
   "source": [
    "## Summary and takeaways\n",
    "## 总结和收获"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafdc910-d616-47ab-aa85-f90c6e7ed80e",
   "metadata": {},
   "source": [
    "- See the [./gpt_class_finetune.py](./gpt_class_finetune.py) script, a self-contained script for classification finetuning\n",
    "- 请查看 [./gpt_class_finetune.py](./gpt_class_finetune.py) 脚本，这是一个用于分类微调的独立脚本\n",
    "- You can find the exercise solutions in [./exercise-solutions.ipynb](./exercise-solutions.ipynb)\n",
    "- 您可以在 [./exercise-solutions.ipynb](./exercise-solutions.ipynb) 中找到练习解决方案\n",
    "- In addition, interested readers can find an introduction to parameter-efficient training with low-rank adaptation (LoRA) in [appendix E](../../appendix-E)\n",
    "- 此外，感兴趣的读者可以在 [appendix E](../../appendix-E) 中找到关于低秩适应（LoRA）参数高效训练的介绍"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
