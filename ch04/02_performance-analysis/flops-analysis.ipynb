{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtQYMbLvgzO-"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbrESHKtgzPA"
   },
   "source": [
    "# FLOPS Analysis\n",
    "# FLOPS 分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xS2WjniMgzPB"
   },
   "source": [
    " - FLOPs (Floating Point Operations Per Second) measure the computational complexity of neural network models by counting the number of floating-point operations executed\n",
    "- FLOPs (每秒浮点运算次数) 通过计算执行的浮点运算次数来衡量神经网络模型的计算复杂度\n",
    " - High FLOPs indicate more intensive computation and energy consumption  \n",
    "- 高 FLOPs 表示更密集的计算和更多的能源消耗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L01-NzkggzPB"
   },
   "outputs": [],
   "source": [
    "# pip install -r requirements-extra.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ObzfVatqgzPC",
    "outputId": "3ead6a41-ac38-4db1-9fc3-012fb3ad18cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thop version: 0.1.1-2209072238\n",
      "torch version: 2.4.1+cu121\n"
     ]
    }
   ],
   "source": [
    "# 从importlib.metadata导入version函数\n",
    "from importlib.metadata import version\n",
    "\n",
    "# 定义需要检查版本的包列表\n",
    "pkgs = [\n",
    "    \"thop\",    # 用于计算模型FLOPs的包\n",
    "    \"torch\",   # PyTorch深度学习框架\n",
    "]\n",
    "\n",
    "# 遍历包列表并打印每个包的版本\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74UpjSLjgzPC"
   },
   "source": [
    "&nbsp;\n",
    "# Simple benchmark with fixed batch size\n",
    "# 使用固定批量大小的简单基准测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90pnCK39gzPD"
   },
   "source": [
    "- forward pass only\n",
    "- 仅前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GerIdRMXd6g9",
    "outputId": "177c6d00-a817-40fe-badd-95cfa8ac9b51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-small (124M)  : 5.1e+11 FLOPS\n",
      "gpt-medium (355M) : 1.4e+12 FLOPS\n",
      "gpt-large (774M)  : 3.2e+12 FLOPS\n",
      "gpt-xl (1558M)    : 6.4e+12 FLOPS\n"
     ]
    }
   ],
   "source": [
    "# 导入PyTorch库\n",
    "import torch\n",
    "# 导入profile函数用于分析FLOPs\n",
    "from thop import profile\n",
    "# 从之前的章节导入GPT模型\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "\n",
    "# 定义基础配置字典\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # 词汇表大小\n",
    "    \"context_length\": 1024,  # 上下文长度\n",
    "    \"drop_rate\": 0.0,        # Dropout比率\n",
    "    \"qkv_bias\": True         # 是否使用QKV偏置\n",
    "}\n",
    "\n",
    "# 定义不同规模GPT模型的配置\n",
    "model_configs = {\n",
    "    \"gpt-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},    # 小型GPT配置\n",
    "    \"gpt-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},  # 中型GPT配置\n",
    "    \"gpt-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},   # 大型GPT配置\n",
    "    \"gpt-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},     # 超大型GPT配置\n",
    "}\n",
    "\n",
    "# 设置计算设备(GPU如果可用,否则使用CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 设置批量大小\n",
    "batch_size = 2\n",
    "# 生成随机输入张量并移至指定设备\n",
    "input_tensor = torch.randint(0, 50257, (batch_size, 1024)).to(device)\n",
    "\n",
    "# 遍历不同规模的模型配置\n",
    "for size in model_configs:\n",
    "    # 更新基础配置\n",
    "    BASE_CONFIG.update(model_configs[size])\n",
    "\n",
    "    # 初始化模型,转换为bfloat16格式\n",
    "    model = GPTModel(BASE_CONFIG).bfloat16()\n",
    "    # 将模型移至指定设备\n",
    "    model.to(device)\n",
    "\n",
    "    # MACS = multiply-accumulate operations\n",
    "    # MACS通常被计算为两个FLOPS(一个乘法和一个累加)\n",
    "    macs, params = profile(model, inputs=(input_tensor,), verbose=False)\n",
    "    # 计算总FLOPS\n",
    "    flops = 2*macs\n",
    "    # 打印模型规模和对应的FLOPS\n",
    "    print(f\"{size:18}: {flops:.1e} FLOPS\")\n",
    "\n",
    "    # 删除模型释放内存\n",
    "    del model\n",
    "    # 清空CUDA缓存\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_S6V05QmgzPD"
   },
   "source": [
    "&nbsp;\n",
    "# Simple benchmark with automatic batch size finding\n",
    "# 使用自动批量大小查找的简单基准测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amw4E983gzPD"
   },
   "source": [
    " - forward pass only\n",
    "- 仅前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h08VOiqpgzPE",
    "outputId": "a6a90ef8-28fb-4b55-9268-6915b0c84c51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing gpt-small (124M)\n",
      "  Batch size 256: 6.5e+13 FLOPS\n",
      "  Batch size 384: 9.7e+13 FLOPS\n",
      "  Batch size 388: 9.8e+13 FLOPS\n",
      "  Batch size 389: 9.8e+13 FLOPS\n",
      "\n",
      "Processing gpt-medium (355M)\n",
      "  Batch size 256: 1.9e+14 FLOPS\n",
      "  Batch size 260: 1.9e+14 FLOPS\n",
      "  Batch size 262: 1.9e+14 FLOPS\n",
      "  Batch size 263: 1.9e+14 FLOPS\n",
      "\n",
      "Processing gpt-large (774M)\n",
      "  Batch size 256: 4.0e+14 FLOPS\n",
      "\n",
      "Processing gpt-xl (1558M)\n",
      "  Batch size 128: 4.1e+14 FLOPS\n",
      "  Batch size 136: 4.3e+14 FLOPS\n",
      "  Batch size 140: 4.5e+14 FLOPS\n",
      "  Batch size 142: 4.5e+14 FLOPS\n",
      "  Batch size 143: 4.6e+14 FLOPS\n"
     ]
    }
   ],
   "source": [
    "# 遍历不同规模的模型配置\n",
    "for size in model_configs:\n",
    "    # 打印当前处理的模型大小\n",
    "    print(f\"\\nProcessing {size}\")\n",
    "    # 复制基础配置\n",
    "    config = BASE_CONFIG.copy()\n",
    "    # 更新当前模型的配置\n",
    "    config.update(model_configs[size])\n",
    "\n",
    "    # 设置最小批量大小为1\n",
    "    min_batch_size = 1\n",
    "    # 初始化最大批量大小为None\n",
    "    max_batch_size = None\n",
    "    # 设置可能的最大批量大小为4096\n",
    "    max_possible_batch_size = 4096\n",
    "\n",
    "    # 当最小批量大小小于等于可能的最大批量大小时继续循环\n",
    "    while min_batch_size <= max_possible_batch_size:\n",
    "        # 计算当前尝试的批量大小(二分查找)\n",
    "        batch_size = (min_batch_size + max_possible_batch_size) // 2\n",
    "        try:\n",
    "            # 生成随机输入张量\n",
    "            input_tensor = torch.randint(\n",
    "                0, config[\"vocab_size\"],\n",
    "                (batch_size, config[\"context_length\"]),\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "            # 初始化模型并转换为bfloat16格式,移至指定设备\n",
    "            model = GPTModel(config).bfloat16().to(device)\n",
    "\n",
    "            # MACS = multiply-accumulate operations\n",
    "            # MACS通常被计算为两个FLOPS(一个乘法和一个累加)\n",
    "            macs, params = profile(model, inputs=(input_tensor,), verbose=False)\n",
    "            # 计算总FLOPS\n",
    "            flops = 2 * macs\n",
    "            # 打印当前批量大小和对应的FLOPS\n",
    "            print(f\"  Batch size {batch_size}: {flops:.1e} FLOPS\")\n",
    "\n",
    "            # 如果成功,尝试更大的批量大小\n",
    "            min_batch_size = batch_size + 1\n",
    "            max_batch_size = batch_size\n",
    "\n",
    "            # 清理内存\n",
    "            del model, input_tensor\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            # 如果发生内存不足错误\n",
    "            if \"out of memory\" in str(e):\n",
    "                # 尝试更小的批量大小\n",
    "                max_possible_batch_size = batch_size - 1\n",
    "\n",
    "                # 清理内存\n",
    "                try:\n",
    "                    del model, input_tensor\n",
    "                    torch.cuda.empty_cache()\n",
    "                except NameError:\n",
    "                    pass\n",
    "            else:\n",
    "                # 如果是其他错误则抛出\n",
    "                raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4lD7tfcgzPE"
   },
   "source": [
    "&nbsp;\n",
    "# Benchmark with automatic batch size finding and Model FLOP Utilization (MFU)\n",
    "# 使用自动批量大小查找和模型FLOP利用率(MFU)的基准测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70Y2mblVgzPE"
   },
   "source": [
    "- Model FLOPs Utilization (MFU) explanation from the [PaLM paper](https://arxiv.org/abs/2204.02311)\n",
    "- 模型 FLOPs 利用率(MFU)解释,来自 [PaLM 论文](https://arxiv.org/abs/2204.02311)\n",
    "\n",
    "> We propose a new metric for efficiency that is implementation-independent and permits a cleaner comparison of system efficiency, called model FLOPs utilization (MFU). This is the ratio of the observed throughput (tokens-per-second) relative to the theoretical maximum throughput of a system operating at peak FLOPs. Crucially, the \"theoretical maximum\" throughput only accounts for the required operations to compute the forward+backward passes, and not rematerialization.\n",
    "> 我们提出了一个新的效率指标,它与实现无关并允许对系统效率进行更清晰的比较,称为模型 FLOPs 利用率(MFU)。这是观察到的吞吐量(每秒令牌数)相对于系统在峰值 FLOPs 下运行的理论最大吞吐量的比率。至关重要的是,\"理论最大\"吞吐量仅考虑计算前向+后向传递所需的操作,而不考虑重物化。\n",
    "\n",
    "$$\\text{MFU} = \\frac{\\text{Observed Tokens per Second}}{\\text{Theoretical Max Tokens per Second}}$$\n",
    "$$\\text{MFU} = \\frac{\\text{观察到的每秒令牌数}}{\\text{理论最大每秒令牌数}}$$\n",
    "\n",
    "where\n",
    "其中\n",
    "\n",
    "$$\\text{Theoretical Max Tokens per Second} = \\frac{\\text{Max FLOPs per Second}}{\\text{Total FLOPs per Token}}$$\n",
    "$$\\text{理论最大每秒令牌数} = \\frac{\\text{每秒最大 FLOPs}}{\\text{每个令牌的总 FLOPs}}$$\n",
    "\n",
    "and\n",
    "和\n",
    "\n",
    "$$\\text{Tokens per Second} = \\frac{\\text{Batch Size} \\times \\text{Sequence Length}}{\\text{Total Time}}$$\n",
    "$$\\text{每秒令牌数} = \\frac{\\text{批量大小} \\times \\text{序列长度}}{\\text{总时间}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKttjC8xgzPF"
   },
   "source": [
    " - forward and backward pass\n",
    " - 前向和后向传递"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6aO4rjtNgzPF"
   },
   "outputs": [],
   "source": [
    "# 由 GPU 制造商提供的理论最大每秒浮点运算次数\n",
    "\n",
    "# 定义不同 GPU 型号和精度下的每秒浮点运算次数\n",
    "flops_per_second = {\n",
    "    # H100 GPU 的规格\n",
    "    \"H100\": {\n",
    "        torch.float32: 51.22e12,  # H100 在 FP32 精度下可达 51.22 TFLOPs\n",
    "        torch.float16: 204.9e12,  # H100 在 FP16 精度下可达 204.9 TFLOPs\n",
    "        torch.bfloat16: 204.9e12  # H100 在 BF16 精度下可达 204.9 TFLOPs\n",
    "    },\n",
    "    # L4 GPU 的规格\n",
    "    \"L4\": {\n",
    "        torch.float32: 30.29e12,  # L4 在 FP32 精度下可达 30.29 TFLOPs\n",
    "        torch.float16: 30.29e12,  # L4 在 FP16 精度下可达 30.29 TFLOPs\n",
    "        torch.bfloat16: 30.29e12  # L4 在 BF16 精度下可达 30.29 TFLOPs\n",
    "    },\n",
    "    # T4 GPU 的规格\n",
    "    \"T4\": {\n",
    "        torch.float32: 8.1e12,    # T4 在 FP32 精度下可达 8.1 TFLOPs\n",
    "        torch.float16: 65.13e12,  # T4 在 FP16 精度下可达 65.13 TFLOPs\n",
    "        torch.bfloat16: 65.13e12  # T4 在 BF16 精度下可达 65.13 TFLOPs\n",
    "    },\n",
    "    # A10G GPU 的规格\n",
    "    \"A10G\": {\n",
    "        torch.float32: 31.52e12,  # A10G 在 FP32 精度下可达 31.52 TFLOPs\n",
    "        torch.float16: 31.52e12,  # A10G 在 FP16 精度下可达 31.52 TFLOPs\n",
    "        torch.bfloat16: 31.52e12  # A10G 在 BF16 精度下可达 31.52 TFLOPs\n",
    "    },\n",
    "    # A100 GPU 的规格\n",
    "    \"A100\": {\n",
    "        torch.float32: 19.49e12,  # A100 在 FP32 精度下可达 19.49 TFLOPs\n",
    "        torch.float16: 77.97e12,  # A100 在 FP16 精度下可达 77.97 TFLOPs\n",
    "        torch.bfloat16: 77.97e12  # A100 在 BF16 精度下可达 77.97 TFLOPs\n",
    "    },\n",
    "    # RTX 3080 GPU 的规格\n",
    "    \"RTX_3080\": {\n",
    "        torch.float32: 29.77e12,  # RTX 3080 在 FP32 精度下可达 29.77 TFLOPs\n",
    "        torch.float16: 29.77e12,  # RTX 3080 在 FP16 精度下可达 29.77 TFLOPs\n",
    "        torch.bfloat16: 29.77e12  # RTX 3080 在 BF16 精度下可达 29.77 TFLOPs\n",
    "    },\n",
    "    # RTX 3090 GPU 的规格\n",
    "    \"RTX_3090\": {\n",
    "        torch.float32: 35.58e12,  # RTX 3090 在 FP32 精度下可达 35.58 TFLOPs\n",
    "        torch.float16: 35.58e12,  # RTX 3090 在 FP16 精度下可达 35.58 TFLOPs\n",
    "        torch.bfloat16: 35.58e12  # RTX 3090 在 BF16 精度下可达 35.58 TFLOPs\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "HW5qWfE7gzPF",
    "outputId": "bb1663bc-ee66-44f1-f54d-0bb66ee0d0c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Model: A100\n",
      "\n",
      "Processing gpt-small (124M)\n",
      "  Batch size 16: Tokens/sec: 34248.82, MFU: 0.3256\n",
      "  Batch size 24: Tokens/sec: 62568.34, MFU: 0.5948\n",
      "\n",
      "Processing gpt-medium (355M)\n",
      "  Batch size 4: Tokens/sec: 20159.93, MFU: 0.5483\n",
      "  Batch size 6: Tokens/sec: 21717.66, MFU: 0.5907\n",
      "  Batch size 7: Tokens/sec: 22536.25, MFU: 0.6130\n",
      "\n",
      "Processing gpt-large (774M)\n",
      "  Batch size 8: Tokens/sec: 12465.21, MFU: 0.7406\n",
      "\n",
      "Processing gpt-xl (1558M)\n",
      "  Batch size 4: Tokens/sec: 6779.92, MFU: 0.8113\n"
     ]
    }
   ],
   "source": [
    "# 导入时间模块用于计时\n",
    "import time\n",
    "\n",
    "# 定义函数用于获取 GPU 型号\n",
    "def get_gpu_model(flops_per_second_dict):\n",
    "    # 获取当前设备的 GPU 名称\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    # 遍历已知的 GPU 型号\n",
    "    for model in flops_per_second_dict.keys():\n",
    "        # 如果在设备名称中找到匹配的型号\n",
    "        if model in device_name:\n",
    "            return model\n",
    "    # 如果没有找到匹配的型号，返回\"Unknown\"\n",
    "    return \"Unknown\"  # Default if no matching model is found\n",
    "\n",
    "\n",
    "# 获取当前 GPU 型号\n",
    "gpu_model = get_gpu_model(flops_per_second)\n",
    "# 打印 GPU 型号\n",
    "print(\"GPU Model:\", gpu_model)\n",
    "\n",
    "# 如果 GPU 型号已知，则继续执行\n",
    "if gpu_model != \"Unknown\":\n",
    "\n",
    "    # 遍历不同的模型配置大小\n",
    "    for size in model_configs:\n",
    "        print(f\"\\nProcessing {size}\")\n",
    "        # 复制基础配置\n",
    "        config = BASE_CONFIG.copy()\n",
    "        # 更新特定大小的配置\n",
    "        config.update(model_configs[size])\n",
    "\n",
    "        # 初始化批次大小的搜索范围\n",
    "        min_batch_size = 1\n",
    "        max_batch_size = None\n",
    "        max_possible_batch_size = 4096\n",
    "\n",
    "        # 二分搜索最大可用批次大小\n",
    "        while min_batch_size <= max_possible_batch_size:\n",
    "            # 计算中间批次大小\n",
    "            batch_size = (min_batch_size + max_possible_batch_size) // 2\n",
    "            try:\n",
    "                # 生成随机输入张量\n",
    "                input_tensor = torch.randint(\n",
    "                    0, config[\"vocab_size\"],\n",
    "                    (batch_size, config[\"context_length\"]),\n",
    "                    device=device\n",
    "                )\n",
    "\n",
    "                # 创建模型并转换为 bfloat16 精度\n",
    "                model = GPTModel(config).bfloat16().to(device)\n",
    "                # 设置为训练模式\n",
    "                model.train()\n",
    "\n",
    "                # 开始计时\n",
    "                torch.cuda.synchronize()\n",
    "                start_time = time.time()\n",
    "\n",
    "                # 前向传播和反向传播\n",
    "                output = model(input_tensor)\n",
    "                loss = output.sum()  # 计算虚拟损失\n",
    "                loss.backward()\n",
    "\n",
    "                # 结束计时\n",
    "                torch.cuda.synchronize()\n",
    "                end_time = time.time()\n",
    "\n",
    "                # 计算总用时\n",
    "                total_time_seconds = end_time - start_time\n",
    "\n",
    "                # 计算前向传播的 FLOPs\n",
    "                macs, params = profile(model, inputs=(input_tensor,), verbose=False)\n",
    "                flops_forward = 2 * macs  # 假设一个 MAC 等于两个 FLOPs\n",
    "\n",
    "                # 估算反向传播的 FLOPs（通常是前向传播的 2 倍）\n",
    "                flops_backward = 2 * flops_forward\n",
    "\n",
    "                # 计算前向+反向传播的总 FLOPs\n",
    "                total_flops = flops_forward + flops_backward\n",
    "\n",
    "                # 获取模型数据类型和对应的理论最大 FLOPs\n",
    "                data_type = next(model.parameters()).dtype\n",
    "                max_flops_per_second = flops_per_second[gpu_model].get(data_type, 0)\n",
    "\n",
    "                # 计算每秒处理的 token 数\n",
    "                tokens_processed = batch_size * config[\"context_length\"]\n",
    "                tokens_per_second = tokens_processed / total_time_seconds\n",
    "\n",
    "                # 计算每个 token 的 FLOPs\n",
    "                flops_per_token = total_flops / tokens_processed\n",
    "\n",
    "                # 计算理论最大每秒 token 数\n",
    "                if flops_per_token > 0:\n",
    "                    theoretical_max_tokens_per_second = max_flops_per_second / flops_per_token\n",
    "                else:\n",
    "                    theoretical_max_tokens_per_second = 0  # 避免除以零\n",
    "\n",
    "                # 计算 MFU（模型 FLOPs 利用率）\n",
    "                if theoretical_max_tokens_per_second > 0:\n",
    "                    mfu = tokens_per_second / theoretical_max_tokens_per_second\n",
    "                else:\n",
    "                    mfu = 0  # 避免除以零\n",
    "\n",
    "                # 打印当前批次大小的性能指标\n",
    "                print(f\"  Batch size {batch_size}: Tokens/sec: {tokens_per_second:.2f}, MFU: {mfu:.4f}\")\n",
    "\n",
    "                # 如果成功，尝试更大的批次大小\n",
    "                min_batch_size = batch_size + 1\n",
    "                max_batch_size = batch_size\n",
    "\n",
    "                # 清理内存\n",
    "                del model, input_tensor, output, loss\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                # 处理内存不足错误\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    # 尝试更小的批次大小\n",
    "                    max_possible_batch_size = batch_size - 1\n",
    "\n",
    "                    # 清理内存\n",
    "                    try:\n",
    "                        del model, input_tensor\n",
    "                        torch.cuda.empty_cache()\n",
    "                    except NameError:\n",
    "                        pass\n",
    "                else:\n",
    "                    # 如果是其他错误则抛出\n",
    "                    raise e\n",
    "\n",
    "else:\n",
    "    # 如果 GPU 型号未知，打印提示信息\n",
    "    print(\"Unknown GPU model. Please update the flops_per_second dictionary with your GPU information.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LovmswRigzPG"
   },
   "source": [
    "- a value of 1.0 is best (equal to 100%)\n",
    "- 1.0是最佳值(等于100%)\n",
    "- Note that the batch sizes are smaller than previously because we also carry out the backward pass here, which is more memory-intensive  \n",
    "- 注意这里的批量大小比之前小,因为我们还要执行反向传播,这需要更多内存"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
