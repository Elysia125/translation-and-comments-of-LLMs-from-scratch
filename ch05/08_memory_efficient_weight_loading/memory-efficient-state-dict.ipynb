{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1E_HhLEeYqFG"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuWudYFWYiH7"
   },
   "source": [
    "# Memory-efficient Model Weight Loading\n",
    "# 内存高效的模型权重加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qt0Qyg6ewUt6"
   },
   "source": [
    "- This notebook provides tips for loading larger pretrained or finetuned models when GPU (or CPU) memory is limited\n",
    "- 本笔记本提供了在 GPU（或 CPU）内存有限时加载较大的预训练或微调模型的技巧\n",
    "\n",
    "- Specifically, it focuses on cases where you saved the model using `torch.save(model.state_dict(), \"model.pth\")` (for example, in chapters 5-7) and want to load it in a new session later for continued pretraining or additional finetuning\n",
    "- 具体来说，它主要关注使用 `torch.save(model.state_dict(), \"model.pth\")` 保存模型的情况（例如在第 5-7 章中），以便稍后在新会话中加载它以继续预训练或额外的微调\n",
    "\n",
    "- While the example uses an LLM, the methods explained in this notebook are general and apply to loading any PyTorch model, not just LLMs\n",
    "- 虽然示例使用了 LLM，但本笔记本中解释的方法是通用的，适用于加载任何 PyTorch 模型，而不仅仅是 LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/memory-efficient-loading/memory-efficient-loading.webp\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SxQzFoS-IXdY",
    "outputId": "b28ebfbd-9036-4696-d95a-7f96fdf29919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory_profiler version: 0.61.0\n",
      "torch version: 2.4.1+cu121\n"
     ]
    }
   ],
   "source": [
    "# 导入版本检查所需的模块\n",
    "from importlib.metadata import version\n",
    "\n",
    "# 定义需要检查版本的包列表\n",
    "pkgs = [\n",
    "    \"torch\",  # PyTorch 深度学习框架\n",
    "]\n",
    "\n",
    "# 遍历包列表并打印每个包的版本\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y47iQaQKyHap"
   },
   "source": [
    "&nbsp;\n",
    "## 1. Benchmark utilities\n",
    "## 1. 基准测试工具"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQeOEoo6yT0X"
   },
   "source": [
    "- First, let's define some utility code to track VRAM (GPU memory)\n",
    "- 首先，让我们定义一些用于跟踪 VRAM（GPU 内存）的实用代码\n",
    "- Later, we will also introduce a tool to track the main system RAM (CPU memory) \n",
    "- 稍后，我们还将引入一个用于跟踪主系统 RAM（CPU 内存）的工具\n",
    "- The purpose of these functions will become clear when we apply them later\n",
    "- 当我们稍后应用这些函数时，它们的用途就会变得清晰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pEiqjYrVivgt"
   },
   "outputs": [],
   "source": [
    "# 导入垃圾回收模块\n",
    "import gc\n",
    "# 导入时间模块\n",
    "import time\n",
    "# 导入PyTorch\n",
    "import torch\n",
    "\n",
    "\n",
    "def start_memory_tracking():\n",
    "    \"\"\"初始化GPU内存追踪\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        # 重置GPU峰值内存统计信息\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    else:\n",
    "        # 如果没有可用的CUDA GPU,打印提示信息\n",
    "        print(\"This notebook is intended for CUDA GPUs but CUDA is not available.\")\n",
    "\n",
    "def print_memory_usage():\n",
    "    # 计算最大GPU内存使用量(GB)\n",
    "    max_gpu_memory = torch.cuda.max_memory_allocated() / (1024 ** 3)  # 将字节转换为GB\n",
    "    # 打印最大GPU内存使用量\n",
    "    print(f\"Maximum GPU memory allocated: {max_gpu_memory:.1f} GB\")\n",
    "\n",
    "def cleanup():\n",
    "    # 执行垃圾回收\n",
    "    gc.collect()\n",
    "    # 清空GPU缓存\n",
    "    torch.cuda.empty_cache()\n",
    "    # 等待3秒钟让内存清理完成\n",
    "    time.sleep(3)  # 一些缓冲时间以允许内存清理\n",
    "    # 重置GPU峰值内存统计信息\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    # 计算并打印最大GPU内存使用量(GB)\n",
    "    max_memory_allocated = torch.cuda.max_memory_allocated(device) / (1024 ** 3)\n",
    "    print(f\"Maximum GPU memory allocated: {max_memory_allocated:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5oJwoc-kkXs"
   },
   "source": [
    "&nbsp;\n",
    "## 2. Model setup\n",
    "## 2. 模型设置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfJE0vnMyr88"
   },
   "source": [
    "- This code section sets up the model itself\n",
    "- 这段代码设置模型本身\n",
    "- Here, we use the \"large\" GPT-2 model to make things more interesting (you may use the \"gpt2-small (124M)\" to lower the memory requirements and execution time of this notebook)\n",
    "- 在这里，我们使用\"large\"版本的GPT-2模型来使事情更有趣（你可以使用\"gpt2-small (124M)\"来降低内存需求和运行时间）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tMuhCYaVI0w7"
   },
   "outputs": [],
   "source": [
    "# 从前面章节导入GPTModel类\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "\n",
    "# 定义基础配置字典\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # 词汇表大小\n",
    "    \"context_length\": 1024,  # 上下文长度\n",
    "    \"drop_rate\": 0.0,        # Dropout比率\n",
    "    \"qkv_bias\": True         # 是否使用Query-Key-Value偏置\n",
    "}\n",
    "\n",
    "# 定义不同规模GPT-2模型的配置参数\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},    # 小型号GPT-2,1.24亿参数\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},  # 中型号GPT-2,3.55亿参数\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},   # 大型号GPT-2,7.74亿参数\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},     # 超大型号GPT-2,15.58亿参数\n",
    "}\n",
    "\n",
    "# 选择要使用的模型规模\n",
    "CHOOSE_MODEL = \"gpt2-xl (1558M)\"\n",
    "\n",
    "# 将选定模型的配置更新到基础配置中\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWYoo1z5y8aX"
   },
   "source": [
    "- Now, let's see the GPU memory functions in action:\n",
    "- 现在，让我们看看 GPU 内存函数的实际效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GK3NEA3eJv3f",
    "outputId": "60573d6e-c603-45e7-8283-b1e92e2a0013"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum GPU memory allocated: 6.4 GB\n"
     ]
    }
   ],
   "source": [
    "# 开始追踪内存使用情况\n",
    "start_memory_tracking()\n",
    "\n",
    "# 创建GPT模型实例\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "# 设置设备为CUDA\n",
    "device = torch.device(\"cuda\") \n",
    "# 将模型移动到GPU\n",
    "model.to(device)\n",
    "\n",
    "# 打印当前内存使用情况\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIhwBEBxzBsF"
   },
   "source": [
    " - Additionally, let's make sure that the model runs okay by passing in some example tensor\n",
    " - 此外,让我们通过传入一些示例张量来确保模型运行正常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "i_j6nZruUd7g"
   },
   "outputs": [],
   "source": [
    "# 测试模型是否正常工作(这里不需要追踪内存)\n",
    "test_input = torch.tensor([[1, 2, 3]]).to(device)  # 创建测试输入张量并移至GPU\n",
    "model.eval()  # 将模型设置为评估模式\n",
    "\n",
    "with torch.no_grad():  # 禁用梯度计算\n",
    "    model(test_input)  # 运行模型前向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgNb8c32zh4g"
   },
   "source": [
    "- Next, imagine we were pretraining the model and saving it for later use\n",
    "- 接下来，假设我们要预训练模型并保存以供后续使用\n",
    "- We skip the actual pretraining here for simplicity and just save the initialized model (but the same concept applies) \n",
    "- 为了简单起见，我们在这里跳过实际的预训练过程，只保存初始化后的模型（但概念是一样的）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wUIXjcsimXU7"
   },
   "outputs": [],
   "source": [
    "# 这里应该是训练代码...\n",
    "\n",
    "# 将模型设置为训练模式\n",
    "model.train()\n",
    "\n",
    "# 保存模型的状态字典到文件\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9tBS4HUzz1g"
   },
   "source": [
    "- Lastly, we delete the model and example tensor in the Python session to reset the GPU memory\n",
    "- 最后，我们在Python会话中删除模型和示例张量以重置GPU内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SqmTzztqKnTs",
    "outputId": "1198afb9-2d97-4b6a-9bdb-41551f25749d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum GPU memory allocated: 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# 删除模型和测试输入张量以释放内存\n",
    "del model, test_input\n",
    "# 清理并重置内存\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EnO8beUJ6Sb"
   },
   "source": [
    "&nbsp;\n",
    "## 3. Weight loading\n",
    "## 3. 权重加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtAXKjsG0AVL"
   },
   "source": [
    "- Now begins the interesting part where we load the pretrained model weights\n",
    "- 现在开始有趣的部分，我们要加载预训练的模型权重\n",
    "- Let's see how much GPU memory is required to load the previously saved model  \n",
    "- 让我们看看加载之前保存的模型需要多少GPU内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wCrQNbSJJO9w",
    "outputId": "9b203868-a8ef-4011-fc2b-611cc0d10994"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum GPU memory allocated: 12.8 GB\n"
     ]
    }
   ],
   "source": [
    "# 然后加载预训练权重\n",
    "\n",
    "# 开始追踪内存使用情况\n",
    "start_memory_tracking()\n",
    "\n",
    "# 创建一个新的GPT模型实例\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "# 将模型移动到指定设备(GPU/CPU)\n",
    "model.to(device)\n",
    "\n",
    "# 从文件加载预训练权重并更新模型参数\n",
    "model.load_state_dict(\n",
    "    torch.load(\"model.pth\", map_location=device, weights_only=True)\n",
    ")\n",
    "# 再次确保模型在正确的设备上\n",
    "model.to(device)\n",
    "# 将模型设置为评估模式\n",
    "model.eval();\n",
    "\n",
    "# 打印当前内存使用情况\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AGvOrcN0KdJ"
   },
   "source": [
    "- Notice that the memory is 2x as large as in the previous session\n",
    "- 注意内存使用量是之前会话的2倍\n",
    "- This is because we have the same model in memory twice, for a short period of time:\n",
    "- 这是因为在短时间内我们在内存中有两份相同的模型:\n",
    "  - The first time via `model.to(device)`\n",
    "  - 第一次是通过 `model.to(device)`\n",
    "  - The second time via the code line `model.load_state_dict(torch.load(\"model.pth\", map_location=device, weights_only=True))`; eventually, the loaded model weights will be copied into the model, and the `state_dict` will be discarded, but for a brief amount of time, we have both the main model and the loaded `state_dict` in memory\n",
    "  - 第二次是通过代码行 `model.load_state_dict(torch.load(\"model.pth\", map_location=device, weights_only=True))`; 最终,加载的模型权重会被复制到模型中,而 `state_dict` 会被丢弃,但在短时间内,我们在内存中同时拥有主模型和加载的 `state_dict`\n",
    "- The remaining sections focus on addressing this\n",
    "- 接下来的章节将重点解决这个问题\n",
    "- But first, let's test the model and reset the GPU memory\n",
    "- 但首先,让我们测试模型并重置 GPU 内存\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DvlUn-nmmbuj",
    "outputId": "11d3ab68-f570-4c1e-c631-fe5547026799"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum GPU memory allocated: 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# 测试模型是否正常工作(这里不需要追踪内存)\n",
    "test_input = torch.tensor([[1, 2, 3]]).to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model(test_input)\n",
    "\n",
    "# 删除模型和测试输入以释放内存\n",
    "del model, test_input\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdPnW3iLLrjX"
   },
   "source": [
    "&nbsp;\n",
    "## 4. Loading weights sequentially\n",
    "## 4. 顺序加载权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYqtUON602TD"
   },
   "source": [
    "- One workaround for the problem of having the model weights in GPU memory twice, as highlighted in the previous section, is to load the model sequentially\n",
    "- 对于前面提到的在 GPU 内存中有两份模型权重的问题,一个解决方法是顺序加载模型\n",
    "- Below, we:\n",
    "- 下面我们将:\n",
    "  - first load the model into GPU memory\n",
    "  - 首先将模型加载到 GPU 内存中\n",
    "  - then load the model weights into CPU memory  \n",
    "  - 然后将模型权重加载到 CPU 内存中\n",
    "  - and finally copy each parameter one by one into GPU memory\n",
    "  - 最后将每个参数逐个复制到 GPU 内存中\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DOIGTNWTmx9G",
    "outputId": "145162e6-aaa6-4c2a-ed8f-f1cf068adb80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum GPU memory allocated: 6.4 GB\n",
      "Maximum GPU memory allocated: 6.7 GB\n"
     ]
    }
   ],
   "source": [
    "# 开始追踪内存使用情况\n",
    "start_memory_tracking()\n",
    "\n",
    "# 创建模型并移动到指定设备\n",
    "model = GPTModel(BASE_CONFIG).to(device)\n",
    "\n",
    "# 从文件加载模型权重到CPU内存\n",
    "state_dict = torch.load(\"model.pth\", map_location=\"cpu\", weights_only=True)\n",
    "\n",
    "# 打印当前内存使用情况\n",
    "print_memory_usage()\n",
    "\n",
    "# 顺序复制权重到模型参数\n",
    "with torch.no_grad():\n",
    "    # 遍历模型的所有命名参数\n",
    "    for name, param in model.named_parameters():\n",
    "        # 如果参数名存在于state_dict中\n",
    "        if name in state_dict:\n",
    "            # 将权重复制到设备上的模型参数\n",
    "            param.copy_(state_dict[name].to(device))\n",
    "        # 如果参数名不存在,打印警告\n",
    "        else:\n",
    "            print(f\"Warning: {name} not found in state_dict.\")\n",
    "\n",
    "# 打印最终内存使用情况\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pn9xD_xL1ZzM"
   },
   "source": [
    "- As we can see above, the memory usage is much lower than before\n",
    "- 如上所示,内存使用量比之前低得多\n",
    "- Notice that the memory increases from 6.4 to 6.7 GB because initially, we only have the model in memory, and then we have the model plus 1 parameter tensor in memory (we temporarily move the parameter tensor to the GPU so we can assign it using `\".to\"` the model)\n",
    "- 注意到内存从6.4 GB增加到6.7 GB,这是因为最初我们只在内存中有模型,然后我们在内存中同时有模型和1个参数张量(我们暂时将参数张量移动到GPU,以便使用`\".to\"`将其分配给模型)\n",
    "- Overall, this is a significant improvement\n",
    "- 总的来说,这是一个显著的改进\n",
    "- Again, let's briefly test the model and then reset the GPU memory for the next section\n",
    "- 再次简单测试模型,然后为下一部分重置GPU内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PRHnjA48nJgw",
    "outputId": "dcd6b1b2-538f-4862-96a6-a5fcbf3326a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum GPU memory allocated: 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# 测试模型是否正常工作(这里不需要追踪内存)\n",
    "test_input = torch.tensor([[1, 2, 3]]).to(device)\n",
    "model.eval()\n",
    "\n",
    "# 使用无梯度计算模式\n",
    "with torch.no_grad():\n",
    "    model(test_input)\n",
    "\n",
    "# 删除不再需要的变量以释放内存\n",
    "del model, test_input, state_dict, param\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5M92LK7usb-Z"
   },
   "source": [
    "&nbsp;\n",
    "## 5. Loading the model with low CPU memory\n",
    "## 5. 以低CPU内存加载模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R45qgeB613e2"
   },
   "source": [
    "- In the previous session, we reduced GPU memory use by loading the weights (`state_dict`) into CPU memory first before copying them one-by-one into the model\n",
    "- 在上一节中,我们通过先将权重(`state_dict`)加载到CPU内存中,然后再逐个复制到模型中来减少GPU内存使用\n",
    "- However, what do we do if we have limited CPU memory?\n",
    "- 但是,如果我们的CPU内存有限该怎么办?\n",
    "- This section uses PyTorch's so-called `\"meta\"` device approach to load a model on machines with large GPU memory but small CPU memory\n",
    "- 本节使用PyTorch所谓的`\"meta\"`设备方法在具有大GPU内存但小CPU内存的机器上加载模型\n",
    "- But first, let's define a convenience function to monitor CPU memory\n",
    "- 首先,让我们定义一个方便的函数来监控CPU内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BrcWy0q-3Bbe"
   },
   "outputs": [],
   "source": [
    "# 导入操作系统相关功能的模块\n",
    "import os\n",
    "# 导入系统和进程监控模块\n",
    "import psutil \n",
    "# 导入线程相关功能\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "def memory_usage_in_gb(func, *args, **kwargs):\n",
    "    # 获取当前进程对象\n",
    "    process = psutil.Process(os.getpid())\n",
    "\n",
    "    # 测量函数运行前的基准内存使用量\n",
    "    baseline_mem = process.memory_info().rss / 1024 ** 3  # 转换为GB单位\n",
    "\n",
    "    # 在单独的线程中开始监控内存\n",
    "    mem_usage = []\n",
    "    done = False\n",
    "\n",
    "    def monitor_memory():\n",
    "        # 持续监控直到done为True\n",
    "        while not done:\n",
    "            # 记录当前内存使用量(GB)\n",
    "            mem_usage.append(process.memory_info().rss / 1024 ** 3)  # 转换为GB\n",
    "            # 每0.1秒采样一次\n",
    "            time.sleep(0.1)\n",
    "\n",
    "    # 创建并启动监控线程\n",
    "    t = Thread(target=monitor_memory)\n",
    "    t.start()\n",
    "\n",
    "    # 运行目标函数\n",
    "    func(*args, **kwargs)\n",
    "\n",
    "    # 停止监控\n",
    "    done = True\n",
    "    t.join()\n",
    "\n",
    "    # 计算峰值内存使用量\n",
    "    peak_mem_usage_gb = max(mem_usage) - baseline_mem\n",
    "    return peak_mem_usage_gb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ayy30Ytd5hjF"
   },
   "source": [
    "- To start with, let's track the CPU memory of the sequential weight loading approach from the previous section\n",
    "- 首先,让我们跟踪上一节中顺序加载权重方法的 CPU 内存使用情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rCkV6IbQtpVn",
    "outputId": "26c0435a-1e3d-4e8f-fbe2-f9655bad61b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum GPU memory allocated: 6.4 GB\n",
      "Maximum GPU memory allocated: 6.7 GB\n",
      "-> Maximum CPU memory allocated: 6.3 GB\n"
     ]
    }
   ],
   "source": [
    "# 定义顺序加载权重的函数\n",
    "def load_sequentially():\n",
    "    # 开始跟踪内存使用情况\n",
    "    start_memory_tracking()\n",
    "\n",
    "    # 创建模型并移至指定设备\n",
    "    model = GPTModel(BASE_CONFIG).to(device)\n",
    "\n",
    "    # 从文件加载状态字典到CPU,仅加载权重\n",
    "    state_dict = torch.load(\"model.pth\", map_location=\"cpu\", weights_only=True)\n",
    "\n",
    "    # 打印当前内存使用情况\n",
    "    print_memory_usage()\n",
    "\n",
    "    # 顺序复制权重到模型参数\n",
    "    with torch.no_grad():\n",
    "        # 遍历模型的所有命名参数\n",
    "        for name, param in model.named_parameters():\n",
    "            # 如果参数名存在于状态字典中\n",
    "            if name in state_dict:\n",
    "                # 将权重复制到设备上的参数\n",
    "                param.copy_(state_dict[name].to(device))\n",
    "            else:\n",
    "                # 打印警告信息\n",
    "                print(f\"Warning: {name} not found in state_dict.\")\n",
    "\n",
    "    # 打印最终内存使用情况\n",
    "    print_memory_usage()\n",
    "\n",
    "\n",
    "# 测量函数的峰值内存使用量\n",
    "peak_memory_used = memory_usage_in_gb(load_sequentially)\n",
    "# 打印最大CPU内存分配量\n",
    "print(f\"-> Maximum CPU memory allocated: {peak_memory_used:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWrmnCML5oKy"
   },
   "source": [
    "- Now, suppose we have a machine with low CPU memory but large GPU memory\n",
    "- 现在,假设我们有一台 CPU 内存较低但 GPU 内存较大的机器\n",
    "\n",
    "- We can trade off CPU memory and GPU memory usage by introducing PyTorch's so-called \"meta\" device\n",
    "- 我们可以通过引入 PyTorch 的所谓\"meta\"设备来权衡 CPU 内存和 GPU 内存的使用\n",
    "\n",
    "- PyTorch's meta device is a special device type that allows you to create tensors without allocating actual memory for their data, effectively creating \"meta\" tensors\n",
    "- PyTorch 的 meta 设备是一种特殊的设备类型,它允许创建张量而无需为其数据分配实际内存,从而有效地创建\"meta\"张量\n",
    "\n",
    "- This is useful for tasks like model analysis or architecture definition, where you need tensor shapes and types without the overhead of memory allocation\n",
    "- 这对于模型分析或架构定义等任务很有用,在这些任务中,你需要张量形状和类型,但不需要内存分配的开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PBErC_5Yt8ly",
    "outputId": "8799db06-191c-47c4-92fa-fbb95d685aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum GPU memory allocated: 12.8 GB\n",
      "Maximum GPU memory allocated: 12.8 GB\n",
      "-> Maximum CPU memory allocated: 1.3 GB\n"
     ]
    }
   ],
   "source": [
    "def load_sequentially_with_meta():\n",
    "    # 开始内存追踪\n",
    "    start_memory_tracking()\n",
    "\n",
    "    # 使用meta设备创建模型,不分配实际内存\n",
    "    with torch.device(\"meta\"):\n",
    "        model = GPTModel(BASE_CONFIG)\n",
    "\n",
    "    # 将模型转换为空模型并移至指定设备\n",
    "    model = model.to_empty(device=device)\n",
    "\n",
    "    # 直接将状态字典加载到指定设备,仅加载权重\n",
    "    state_dict = torch.load(\"model.pth\", map_location=device, weights_only=True)\n",
    "\n",
    "    # 打印当前内存使用情况\n",
    "    print_memory_usage()\n",
    "\n",
    "    # 顺序复制权重到模型参数\n",
    "    with torch.no_grad():\n",
    "        # 遍历模型的所有命名参数\n",
    "        for name, param in model.named_parameters():\n",
    "            # 如果参数名存在于状态字典中\n",
    "            if name in state_dict:\n",
    "                # 将权重复制到参数\n",
    "                param.copy_(state_dict[name])\n",
    "            else:\n",
    "                # 打印警告信息\n",
    "                print(f\"Warning: {name} not found in state_dict.\")\n",
    "\n",
    "    # 打印最终内存使用情况\n",
    "    print_memory_usage()\n",
    "\n",
    "# 测量函数的峰值内存使用量\n",
    "peak_memory_used = memory_usage_in_gb(load_sequentially_with_meta)\n",
    "# 打印最大CPU内存分配量\n",
    "print(f\"-> Maximum CPU memory allocated: {peak_memory_used:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VpnCABp75-VQ"
   },
   "source": [
    "- As we can see above, by creating the model on the meta-device and loading the weights directly into GPU memory, we effectively reduced the CPU memory requirements\n",
    "- 如上所示,通过在meta设备上创建模型并直接将权重加载到GPU内存中,我们有效地降低了CPU内存需求\n",
    "- One might ask: \"Is the sequential weight loading still necessary then, and how does that compare to the original approach?\"\n",
    "- 有人可能会问:\"那么顺序加载权重是否仍然必要,与原始方法相比如何?\"\n",
    "- Let's check the simple PyTorch weight loading approach for comparison (from the first weight loading section in this notebook):\n",
    "- 让我们检查简单的PyTorch权重加载方法进行比较(来自本笔记本中第一个权重加载部分):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4f-bqBNRuR39",
    "outputId": "f7c0a901-b404-433a-9b93-2bbfa8183c56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum GPU memory allocated: 12.8 GB\n",
      "-> Maximum CPU memory allocated: 4.4 GB\n"
     ]
    }
   ],
   "source": [
    "# 定义基准函数\n",
    "def baseline():\n",
    "    # 开始内存追踪\n",
    "    start_memory_tracking()\n",
    "\n",
    "    # 使用基础配置创建模型\n",
    "    model = GPTModel(BASE_CONFIG)\n",
    "    # 将模型移至指定设备\n",
    "    model.to(device)\n",
    "\n",
    "    # 加载模型权重并移至指定设备\n",
    "    model.load_state_dict(torch.load(\"model.pth\", map_location=device, weights_only=True))\n",
    "    model.to(device)\n",
    "    # 将模型设置为评估模式\n",
    "    model.eval();\n",
    "\n",
    "    # 打印内存使用情况\n",
    "    print_memory_usage()\n",
    "\n",
    "# 测量函数的峰值内存使用量\n",
    "peak_memory_used = memory_usage_in_gb(baseline)\n",
    "# 打印最大CPU内存分配量\n",
    "print(f\"-> Maximum CPU memory allocated: {peak_memory_used:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKAjxbX86xnb"
   },
   "source": [
    "- As we can see above, the \"simple\" weight loading without the meta device uses more memory\n",
    "- 如上所示,不使用meta设备的\"简单\"权重加载会使用更多内存\n",
    "- In other words, if you have a machine with limited CPU memory, you can use the meta device approach to directly load the model weights into GPU memory to reduce peak CPU memory usage\n",
    "- 换句话说,如果你的机器CPU内存有限,可以使用meta设备方法直接将模型权重加载到GPU内存中以减少峰值CPU内存使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## 6. Using `mmap=True` (recommmended)\n",
    "## 6. 使用 `mmap=True` (推荐)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As an intermediate or advanced `torch.load` user, you may wonder how these approaches compare to the `mmap=True` setting in PyTorch\n",
    "- 作为一个中级或高级的`torch.load`用户,你可能想知道这些方法与PyTorch中的`mmap=True`设置相比如何\n",
    "- The `mmap=True` setting in PyTorch enables memory-mapped file I/O, which allows the tensor to access data directly from disk storage, thus reducing memory usage by not loading the entire file into RAM if RAM is limited\n",
    "- PyTorch中的`mmap=True`设置启用了内存映射文件I/O,这允许张量直接从磁盘存储访问数据,从而在RAM有限的情况下通过不将整个文件加载到RAM中来减少内存使用\n",
    "- Also, see the helpful comment by [mikaylagawarecki](https://github.com/rasbt/LLMs-from-scratch/issues/402)\n",
    "- 另请参阅[mikaylagawarecki](https://github.com/rasbt/LLMs-from-scratch/issues/402)的有用评论\n",
    "- At first glance, it may look less efficient than the sequential approaches above:\n",
    "- 乍看之下,它可能看起来不如上面的顺序方法高效:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GKwV0AMNemuR",
    "outputId": "e207f2bf-5c87-498e-80fe-e8c4016ac711"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum GPU memory allocated: 6.4 GB\n",
      "-> Maximum CPU memory allocated: 5.9 GB\n"
     ]
    }
   ],
   "source": [
    "# 定义最佳实践函数\n",
    "def best_practices():\n",
    "    # 使用meta设备创建模型\n",
    "    with torch.device(\"meta\"):\n",
    "        model = GPTModel(BASE_CONFIG)\n",
    "\n",
    "    # 加载模型权重,使用mmap=True进行内存映射加载\n",
    "    model.load_state_dict(\n",
    "        torch.load(\"model.pth\", map_location=device, weights_only=True, mmap=True),\n",
    "        assign=True\n",
    "    )\n",
    "\n",
    "    # 打印内存使用情况\n",
    "    print_memory_usage()\n",
    "\n",
    "# 测量函数的峰值内存使用量\n",
    "peak_memory_used = memory_usage_in_gb(best_practices)\n",
    "# 打印最大CPU内存分配量\n",
    "print(f\"-> Maximum CPU memory allocated: {peak_memory_used:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The reason why the CPU RAM usage is so high is that there's enough CPU RAM available on this machine\n",
    "- CPU RAM使用率如此之高的原因是这台机器有足够的CPU RAM可用\n",
    "- However, if you were to run this on a machine with limited CPU RAM, the `mmap` approach would use less memory  \n",
    "- 但是,如果你在CPU RAM有限的机器上运行,`mmap`方法会使用更少的内存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## 7. Other methods\n",
    "## 7. 其他方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This notebook is focused on simple, built-in methods for loading weights in PyTorch\n",
    "- 本笔记本专注于在PyTorch中加载权重的简单内置方法\n",
    "- The recommended approach for limited CPU memory cases is the `mmap=True` approach explained enough\n",
    "- 对于CPU内存有限的情况,推荐使用上面解释的`mmap=True`方法\n",
    "- Alternatively, one other option is a brute-force approach that saves and loads each weight tensor separately:\n",
    "- 另外,还有一种暴力方法是分别保存和加载每个权重张量:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2CgPEZUIb00w"
   },
   "outputs": [],
   "source": [
    "# 创建一个GPT模型实例\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "# 假设这是你已经训练好的模型\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "# 创建一个目录来存储单独的参数文件\n",
    "os.makedirs(\"model_parameters\", exist_ok=True)\n",
    "\n",
    "# 遍历状态字典中的每个参数\n",
    "for name, param in state_dict.items():\n",
    "    # 将每个参数张量单独保存到文件中\n",
    "    torch.save(param.cpu(), f\"model_parameters/{name}.pt\")\n",
    "\n",
    "# 删除模型释放内存\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gTsmtJK-b4yy",
    "outputId": "d361e2d3-e34c-48d7-9047-846c9bfd291e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum GPU memory allocated: 6.4 GB\n",
      "Maximum GPU memory allocated: 6.4 GB\n",
      "-> Maximum CPU memory allocated: 0.3 GB\n"
     ]
    }
   ],
   "source": [
    "# 定义一个函数用于单独加载权重\n",
    "def load_individual_weights():\n",
    "\n",
    "    # 开始跟踪内存使用情况\n",
    "    start_memory_tracking()\n",
    "\n",
    "    # 使用meta设备创建一个空模型\n",
    "    with torch.device(\"meta\"):\n",
    "        model = GPTModel(BASE_CONFIG)\n",
    "\n",
    "    # 将模型转换为空权重模型并移至指定设备\n",
    "    model = model.to_empty(device=device)\n",
    "\n",
    "    # 打印当前内存使用情况\n",
    "    print_memory_usage()\n",
    "    # 设置参数文件所在目录\n",
    "    param_dir = \"model_parameters\"\n",
    "\n",
    "    # 禁用梯度计算,逐个加载权重\n",
    "    with torch.no_grad():\n",
    "        for name, param in model.named_parameters():\n",
    "            # 构建权重文件路径\n",
    "            weight_path = os.path.join(param_dir, f\"{name}.pt\")\n",
    "            if os.path.exists(weight_path):\n",
    "                # 加载权重文件\n",
    "                param_data = torch.load(weight_path, map_location=\"cpu\", weights_only=True)\n",
    "                # 将权重复制到模型参数中\n",
    "                param.copy_(param_data)\n",
    "                # 删除临时加载的权重释放内存\n",
    "                del param_data  # Free memory\n",
    "            else:\n",
    "                # 如果权重文件不存在则打印警告\n",
    "                print(f\"Warning: {name} not found in {param_dir}.\")\n",
    "\n",
    "    # 打印最终内存使用情况\n",
    "    print_memory_usage()\n",
    "\n",
    "\n",
    "# 测量加载过程中的峰值内存使用\n",
    "peak_memory_used = memory_usage_in_gb(load_individual_weights)\n",
    "# 打印峰值内存使用量\n",
    "print(f\"-> Maximum CPU memory allocated: {peak_memory_used:.1f} GB\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
