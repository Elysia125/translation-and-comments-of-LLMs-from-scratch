{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1b280ab-b61f-4d1a-bf7e-44e5f9ed3a5c",
   "metadata": {
    "id": "e1b280ab-b61f-4d1a-bf7e-44e5f9ed3a5c"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efde77f2-6af3-4781-8597-89ecd3f41a52",
   "metadata": {
    "id": "efde77f2-6af3-4781-8597-89ecd3f41a52"
   },
   "source": [
    "# # Llama 3.2 From Scratch (A Standalone Notebook)\n",
    "# 从零开始实现 Llama 3.2 (独立笔记本)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cdef4d-de59-4a65-89f9-fa2a8ef3471d",
   "metadata": {
    "id": "55cdef4d-de59-4a65-89f9-fa2a8ef3471d"
   },
   "source": [
    "- This notebook is purposefully minimal and focuses on the code to implement the Llama 3.2 1B and 3B LLMs\n",
    "- 本笔记本刻意保持简洁,专注于实现 Llama 3.2 1B 和 3B 大语言模型的代码\n",
    "\n",
    "- For a step-by-step guide that explains the individual components and the relationship between GPT, Llama 2, and Llama 3, please see the following companion notebooks:\n",
    "- 如需了解各个组件以及 GPT、Llama 2 和 Llama 3 之间关系的分步指南,请参阅以下配套笔记本:\n",
    "\n",
    "  - [Converting a From-Scratch GPT Architecture to Llama 2](converting-gpt-to-llama2.ipynb)\n",
    "  - [Converting Llama 2 to Llama 3.2 From Scratch](converting-llama2-to-llama3.ipynb)\n",
    "  \n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/gpt-to-llama/llama32.webp\" width=\"700px\">\n",
    "  \n",
    "- About the code:\n",
    "- 关于代码:\n",
    "\n",
    "  - all code is my own code, mapping the Llama 3 architecture onto the model code implemented in my [Build A Large Language Model (From Scratch)](http://mng.bz/orYv) book; the code is released under a permissive open-source Apache 2.0 license (see [LICENSE.txt](https://github.com/rasbt/LLMs-from-scratch/blob/main/LICENSE.txt))\n",
    "  - 所有代码均为本人原创,将 Llama 3 架构映射到我的《从零开始构建大型语言模型》一书中实现的模型代码;代码以宽松的开源 Apache 2.0 许可证发布(参见 [LICENSE.txt](https://github.com/rasbt/LLMs-from-scratch/blob/main/LICENSE.txt))\n",
    "\n",
    "  - the tokenizer code is inspired by the original [Llama 3 tokenizer code](https://github.com/meta-llama/llama3/blob/main/llama/tokenizer.py), which Meta AI used to to extends the Tiktoken GPT-4 tokenizer\n",
    "  - 分词器代码的灵感来自原始的 [Llama 3 分词器代码](https://github.com/meta-llama/llama3/blob/main/llama/tokenizer.py),Meta AI 用它来扩展 Tiktoken GPT-4 分词器\n",
    "\n",
    "  - the RoPE rescaling section is inspired by the [_compute_llama3_parameters function](https://github.com/huggingface/transformers/blob/5c1027bf09717f664b579e01cbb8ec3ef5aeb140/src/transformers/modeling_rope_utils.py#L329-L347) in the `transformers` library\n",
    "  - RoPE 重缩放部分的灵感来自 `transformers` 库中的 [_compute_llama3_parameters 函数](https://github.com/huggingface/transformers/blob/5c1027bf09717f664b579e01cbb8ec3ef5aeb140/src/transformers/modeling_rope_utils.py#L329-L347)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c201adb-747e-437b-9a62-442802941e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd1b65a8-4301-444a-bd7c-a6f2bd1df9df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dd1b65a8-4301-444a-bd7c-a6f2bd1df9df",
    "outputId": "4f762354-e0a3-4cc2-e5d4-e61a227a202c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blobfile version: 3.0.0\n",
      "huggingface_hub version: 0.25.2\n",
      "tiktoken version: 0.8.0\n",
      "torch version: 2.5.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"blobfile\",         # to download pretrained weights\n",
    "    \"huggingface_hub\",  # to download pretrained weights\n",
    "    \"tiktoken\",         # to implement the tokenizer\n",
    "    \"torch\",            # to implement the model\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653410a6-dd2b-4eb2-a722-23d9782e726d",
   "metadata": {
    "id": "653410a6-dd2b-4eb2-a722-23d9782e726d"
   },
   "source": [
    "&nbsp;\n",
    "# 1. Architecture code\n",
    "# 1. 架构代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82076c21-9331-4dcd-b017-42b046cf1a60",
   "metadata": {
    "id": "82076c21-9331-4dcd-b017-42b046cf1a60"
   },
   "outputs": [],
   "source": [
    "# 导入PyTorch相关库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# 前馈神经网络模块\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        # 调用父类初始化\n",
    "        super().__init__()\n",
    "        # 第一个全连接层,将输入维度映射到隐藏维度\n",
    "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        # 第二个全连接层,将输入维度映射到隐藏维度\n",
    "        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        # 第三个全连接层,将隐藏维度映射回输入维度\n",
    "        self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 通过第一个全连接层\n",
    "        x_fc1 = self.fc1(x)\n",
    "        # 通过第二个全连接层\n",
    "        x_fc2 = self.fc2(x)\n",
    "        # 应用SiLU激活函数并进行门控操作\n",
    "        x = nn.functional.silu(x_fc1) * x_fc2\n",
    "        # 通过第三个全连接层返回结果\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b9a346f-5826-4083-9162-abd56afc03f0",
   "metadata": {
    "id": "4b9a346f-5826-4083-9162-abd56afc03f0"
   },
   "outputs": [],
   "source": [
    "# 预计算旋转位置编码(RoPE)参数\n",
    "def precompute_rope_params(head_dim, theta_base=10_000, context_length=4096, freq_config=None):\n",
    "    # 确保头部维度是偶数\n",
    "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "\n",
    "    # 计算逆频率\n",
    "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2)[: (head_dim // 2)].float() / head_dim))\n",
    "\n",
    "    # 如果提供了频率配置,进行频率调整\n",
    "    if freq_config is not None:\n",
    "        # 计算低频和高频波长\n",
    "        low_freq_wavelen = freq_config[\"original_context_length\"] / freq_config[\"low_freq_factor\"]\n",
    "        high_freq_wavelen = freq_config[\"original_context_length\"] / freq_config[\"high_freq_factor\"]\n",
    "\n",
    "        # 计算波长\n",
    "        wavelen = 2 * torch.pi / inv_freq\n",
    "\n",
    "        # 根据波长条件调整逆频率\n",
    "        inv_freq_llama = torch.where(\n",
    "            wavelen > low_freq_wavelen, inv_freq / freq_config[\"factor\"], inv_freq\n",
    "        )\n",
    "\n",
    "        # 计算平滑因子\n",
    "        smooth_factor = (freq_config[\"original_context_length\"] / wavelen - freq_config[\"low_freq_factor\"]) / (\n",
    "            freq_config[\"high_freq_factor\"] - freq_config[\"low_freq_factor\"]\n",
    "        )\n",
    "\n",
    "        # 计算平滑后的逆频率\n",
    "        smoothed_inv_freq = (\n",
    "            (1 - smooth_factor) * (inv_freq / freq_config[\"factor\"]) + smooth_factor * inv_freq\n",
    "        )\n",
    "\n",
    "        # 判断中频区域并应用平滑\n",
    "        is_medium_freq = (wavelen <= low_freq_wavelen) & (wavelen >= high_freq_wavelen)\n",
    "        inv_freq_llama = torch.where(is_medium_freq, smoothed_inv_freq, inv_freq_llama)\n",
    "        inv_freq = inv_freq_llama\n",
    "\n",
    "    # 生成位置索引\n",
    "    positions = torch.arange(context_length)\n",
    "\n",
    "    # 计算角度\n",
    "    angles = positions[:, None] * inv_freq[None, :]  # 形状: (context_length, head_dim // 2)\n",
    "\n",
    "    # 扩展角度以匹配head_dim\n",
    "    angles = torch.cat([angles, angles], dim=1)  # 形状: (context_length, head_dim)\n",
    "\n",
    "    # 预计算正弦和余弦值\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "\n",
    "    return cos, sin\n",
    "\n",
    "\n",
    "# 计算旋转位置编码\n",
    "def compute_rope(x, cos, sin):\n",
    "    # x: (batch_size, num_heads, seq_len, head_dim)\n",
    "    # 获取输入张量的维度\n",
    "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
    "    # 确保头部维度是偶数\n",
    "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
    "\n",
    "    # 将输入张量分成前半部分和后半部分\n",
    "    x1 = x[..., : head_dim // 2]  # 前半部分\n",
    "    x2 = x[..., head_dim // 2 :]  # 后半部分\n",
    "\n",
    "    # 调整正弦和余弦的形状\n",
    "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)  # 形状: (1, 1, seq_len, head_dim)\n",
    "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # 应用旋转变换\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    x_rotated = (x * cos) + (rotated * sin)\n",
    "\n",
    "    # 返回旋转后的结果,并确保数据类型与输入一致\n",
    "    return x_rotated.to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8169ab5-f976-4222-a2e1-eb1cabf267cb",
   "metadata": {
    "id": "e8169ab5-f976-4222-a2e1-eb1cabf267cb"
   },
   "outputs": [],
   "source": [
    "# 定义一个共享缓冲区类,用于存储和复用计算结果\n",
    "class SharedBuffers:\n",
    "    # 类变量,用于存储缓冲区\n",
    "    _buffers = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_buffers(context_length, head_dim, rope_base, freq_config, dtype=torch.float32):\n",
    "        # 根据输入参数生成唯一的键值\n",
    "        key = (context_length, head_dim, rope_base, tuple(freq_config.values()) if freq_config else freq_config, dtype)\n",
    "\n",
    "        # 如果缓冲区中没有对应的键值,则创建新的缓冲区\n",
    "        if key not in SharedBuffers._buffers:\n",
    "            # 创建因果掩码矩阵\n",
    "            mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "            # 预计算RoPE参数\n",
    "            cos, sin = precompute_rope_params(head_dim, rope_base, context_length, freq_config)\n",
    "            # 转换数据类型\n",
    "            if dtype is not None:\n",
    "                cos = cos.to(dtype)\n",
    "                sin = sin.to(dtype)\n",
    "            # 存储到缓冲区\n",
    "            SharedBuffers._buffers[key] = (mask, cos, sin)\n",
    "\n",
    "        # 返回缓冲区中的值\n",
    "        return SharedBuffers._buffers[key]\n",
    "\n",
    "\n",
    "# 分组查询注意力机制类\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(\n",
    "            self, d_in, d_out, context_length, num_heads,\n",
    "            num_kv_groups,\n",
    "            rope_base=10_000,\n",
    "            rope_config=None,\n",
    "            dtype=None\n",
    "        ):\n",
    "        super().__init__()\n",
    "        # 确保输出维度能被头数整除\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        # 确保头数能被KV组数整除\n",
    "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
    "\n",
    "        # 保存模型参数\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        # 创建Key、Value和Query的线性变换层\n",
    "        self.W_key = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)\n",
    "        self.W_value = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)\n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        self.group_size = num_heads // num_kv_groups\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        self.out_proj = nn.Linear(d_out, d_out, bias=False, dtype=dtype)\n",
    "\n",
    "        # 获取共享缓冲区\n",
    "        mask, cos, sin = SharedBuffers.get_buffers(context_length, self.head_dim, rope_base, rope_config, dtype)\n",
    "        # 注册缓冲区\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "        self.register_buffer(\"cos\", cos)\n",
    "        self.register_buffer(\"sin\", sin)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 获取输入张量的维度\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        # 计算查询、键和值\n",
    "        queries = self.W_query(x)  # 形状: (b, num_tokens, d_out)\n",
    "        keys = self.W_key(x)  # 形状: (b, num_tokens, num_kv_groups * head_dim)\n",
    "        values = self.W_value(x)  # 形状: (b, num_tokens, num_kv_groups * head_dim)\n",
    "\n",
    "        # 重塑张量维度\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_kv_groups, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_kv_groups, self.head_dim)\n",
    "\n",
    "        # 转置维度顺序\n",
    "        keys = keys.transpose(1, 2)  # 形状: (b, num_heads, num_tokens, head_dim)\n",
    "        values = values.transpose(1, 2)  # 形状: (b, num_heads, num_tokens, head_dim)\n",
    "        queries = queries.transpose(1, 2)  # 形状: (b, num_query_groups, num_tokens, head_dim)\n",
    "\n",
    "        # 应用RoPE位置编码\n",
    "        keys = compute_rope(keys, self.cos, self.sin)\n",
    "        queries = compute_rope(queries, self.cos, self.sin)\n",
    "\n",
    "        # 扩展键值对以匹配头数\n",
    "        keys = keys.repeat_interleave(self.group_size, dim=1)  # 形状: (b, num_heads, num_tokens, head_dim)\n",
    "        values = values.repeat_interleave(self.group_size, dim=1)  # 形状: (b, num_heads, num_tokens, head_dim)\n",
    "\n",
    "        # 计算注意力分数\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # 形状: (b, num_heads, num_tokens, num_tokens)\n",
    "\n",
    "        # 将原始掩码截断并转换为布尔类型\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # 使用掩码填充注意力分数\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        # 计算注意力权重\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        assert keys.shape[-1] == self.head_dim\n",
    "\n",
    "        # 计算上下文向量\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # 合并多头注意力的结果\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        # 应用输出投影\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "457cb2f8-50c1-4045-8a74-f181bfb5fea9",
   "metadata": {
    "id": "457cb2f8-50c1-4045-8a74-f181bfb5fea9"
   },
   "outputs": [],
   "source": [
    "# Transformer块类,实现了Transformer架构中的一个基本单元\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        # 初始化父类\n",
    "        super().__init__()\n",
    "        \n",
    "        # 初始化分组查询注意力层\n",
    "        self.att =  GroupedQueryAttention(\n",
    "            d_in=cfg[\"emb_dim\"],          # 输入维度\n",
    "            d_out=cfg[\"emb_dim\"],         # 输出维度\n",
    "            context_length=cfg[\"context_length\"],  # 上下文长度\n",
    "            num_heads=cfg[\"n_heads\"],      # 注意力头数\n",
    "            num_kv_groups=cfg[\"n_kv_groups\"], # KV分组数\n",
    "            rope_base=cfg[\"rope_base\"],    # RoPE基数\n",
    "            rope_config=cfg[\"rope_freq\"],  # RoPE频率配置\n",
    "            dtype=cfg[\"dtype\"]             # 数据类型\n",
    "        )\n",
    "        \n",
    "        # 初始化前馈网络层\n",
    "        self.ff = FeedForward(cfg)\n",
    "        \n",
    "        # 初始化两个RMSNorm层,用于注意力层和前馈层的归一化\n",
    "        self.norm1 = nn.RMSNorm(cfg[\"emb_dim\"], eps=1e-5)  # 第一个归一化层\n",
    "        self.norm2 = nn.RMSNorm(cfg[\"emb_dim\"], eps=1e-5)  # 第二个归一化层\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 注意力块的残差连接\n",
    "        shortcut = x                           # 保存输入用于残差连接\n",
    "        x = self.norm1(x)                      # 应用第一个归一化层\n",
    "        x = self.att(x.to(torch.bfloat16))    # 应用注意力层,转换为bfloat16类型\n",
    "        x = x + shortcut                       # 添加残差连接\n",
    "\n",
    "        # 前馈网络块的残差连接\n",
    "        shortcut = x                           # 保存中间结果用于残差连接\n",
    "        x = self.norm2(x)                      # 应用第二个归一化层\n",
    "        x = self.ff(x.to(torch.bfloat16))     # 应用前馈网络层,转换为bfloat16类型\n",
    "        x = x + shortcut                       # 添加残差连接\n",
    "\n",
    "        return x                               # 返回处理后的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e88de3e3-9f07-42cc-816b-28dbd46e96c4",
   "metadata": {
    "id": "e88de3e3-9f07-42cc-816b-28dbd46e96c4"
   },
   "outputs": [],
   "source": [
    "class Llama3Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        # 继承自nn.Module基类\n",
    "        super().__init__()\n",
    "        \n",
    "        # 创建词嵌入层,将词表索引映射为词向量\n",
    "        # 参数: 词表大小、嵌入维度、数据类型\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
    "\n",
    "        # 创建Transformer块的序列\n",
    "        # 使用列表推导式创建n_layers个TransformerBlock,然后展开为Sequential的参数\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        # 创建最终的RMSNorm归一化层\n",
    "        self.final_norm = nn.RMSNorm(cfg[\"emb_dim\"], eps=1e-5)\n",
    "        \n",
    "        # 创建输出层,将嵌入维度映射回词表大小\n",
    "        # bias=False表示不使用偏置项\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        # 将输入的词索引转换为词嵌入向量\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        \n",
    "        # 将词嵌入赋值给x用于后续处理\n",
    "        x = tok_embeds\n",
    "        \n",
    "        # 依次通过所有Transformer块\n",
    "        x = self.trf_blocks(x)\n",
    "        \n",
    "        # 通过最终的归一化层\n",
    "        x = self.final_norm(x)\n",
    "        \n",
    "        # 通过输出层得到logits,并转换为bfloat16类型\n",
    "        logits = self.out_head(x.to(torch.bfloat16))\n",
    "        \n",
    "        # 返回最终的logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2d201f-74ad-4d63-ab9c-601b00674a48",
   "metadata": {
    "id": "be2d201f-74ad-4d63-ab9c-601b00674a48"
   },
   "source": [
    "&nbsp;\n",
    "# 2. Initialize model\n",
    "# 2. 初始化模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dea40c-fe20-4a75-be25-d6fce5863c01",
   "metadata": {
    "id": "23dea40c-fe20-4a75-be25-d6fce5863c01"
   },
   "source": [
    "- The remainder of this notebook uses the Llama 3.2 1B model; to use the 3B model variant, just uncomment the second configuration file in the following code cell\n",
    "- 本笔记本的其余部分使用 Llama 3.2 1B 模型；如果要使用 3B 模型变体，只需取消下面代码单元中第二个配置文件的注释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caa142fa-b375-4e78-b392-2072ced666f3",
   "metadata": {
    "id": "caa142fa-b375-4e78-b392-2072ced666f3"
   },
   "outputs": [],
   "source": [
    "# Llama 3.2 1B\n",
    "\n",
    "LLAMA32_CONFIG = {\n",
    "    \"vocab_size\": 128_256,      # 词表大小\n",
    "    \"context_length\": 131_072,  # 上下文长度\n",
    "    \"emb_dim\": 2048,            # 嵌入维度\n",
    "    \"n_heads\": 32,              # 注意力头数量\n",
    "    \"n_layers\": 16,             # 层数\n",
    "    \"hidden_dim\": 8192,         # 前馈网络中间维度大小\n",
    "    \"n_kv_groups\": 8,           # 分组查询注意力的键值组数\n",
    "    \"rope_base\": 500_000.0,     # RoPE中\"theta\"的基数\n",
    "    \"dtype\": torch.bfloat16,    # 使用低精度数据类型以节省内存\n",
    "    \"rope_freq\": {              # RoPE频率缩放\n",
    "        \"factor\": 32.0,         # 缩放因子\n",
    "        \"low_freq_factor\": 1.0, # 低频因子\n",
    "        \"high_freq_factor\": 4.0,# 高频因子\n",
    "        \"original_context_length\": 8192, # 原始上下文长度\n",
    "    }\n",
    "}\n",
    "\n",
    "# Llama 3.2 3B\n",
    "\n",
    "# LLAMA32_CONFIG = {\n",
    "#     \"vocab_size\": 128_256,      # 词表大小\n",
    "#     \"context_length\": 131_072,  # 上下文长度\n",
    "#     \"emb_dim\": 3072,            # 嵌入维度\n",
    "#     \"n_heads\": 24,              # 注意力头数量\n",
    "#     \"n_layers\": 28,             # 层数\n",
    "#     \"hidden_dim\": 8192,         # 前馈网络中间维度大小\n",
    "#     \"n_kv_groups\": 8,           # 分组查询注意力的键值组数\n",
    "#     \"rope_base\": 500_000.0,     # RoPE中\"theta\"的基数\n",
    "#     \"dtype\": torch.bfloat16,    # 使用低精度数据类型以节省内存\n",
    "#     \"rope_freq\": {              # RoPE频率缩放\n",
    "#         \"factor\": 32.0,         # 缩放因子\n",
    "#         \"low_freq_factor\": 1.0, # 低频因子\n",
    "#         \"high_freq_factor\": 4.0,# 高频因子\n",
    "#         \"original_context_length\": 8192, # 原始上下文长度\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# 根据嵌入维度确定模型大小(1B或3B)\n",
    "LLAMA_SIZE_STR = \"1B\" if LLAMA32_CONFIG[\"emb_dim\"] == 2048 else \"3B\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34535172-797e-4dd0-84fb-65bc75ad5b06",
   "metadata": {
    "id": "34535172-797e-4dd0-84fb-65bc75ad5b06"
   },
   "source": [
    "- Reduce the context length so the model would work fine on a MacBook Air (if you have more RAM, feel free to comment out the lines below):\n",
    "- 减小上下文长度以使模型能在 MacBook Air 上正常运行（如果你有更多内存，可以注释掉下面的代码）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8bc2370-39d2-4bfe-b4c1-6bdd75fe101c",
   "metadata": {
    "id": "a8bc2370-39d2-4bfe-b4c1-6bdd75fe101c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New RoPE theta: 31250.0\n"
     ]
    }
   ],
   "source": [
    "# 保存原始上下文长度\n",
    "old_context_length = LLAMA32_CONFIG[\"context_length\"]\n",
    "# 将上下文长度设置为较小的值以节省内存\n",
    "LLAMA32_CONFIG[\"context_length\"] = 8192\n",
    "\n",
    "\n",
    "def rescale_theta(theta_old, context_length_old, context_length_new):\n",
    "    \"\"\"根据新的上下文长度重新缩放RoPE的theta值\n",
    "    \n",
    "    参数:\n",
    "        theta_old: 原始theta值\n",
    "        context_length_old: 原始上下文长度 \n",
    "        context_length_new: 新的上下文长度\n",
    "        \n",
    "    返回:\n",
    "        theta_new: 重新缩放后的theta值\n",
    "    \"\"\"\n",
    "    scaling_factor = context_length_new / context_length_old\n",
    "    theta_new = theta_old * scaling_factor\n",
    "    return theta_new\n",
    "\n",
    "# 根据新的上下文长度重新计算RoPE的base值\n",
    "LLAMA32_CONFIG[\"rope_base\"] = rescale_theta(\n",
    "    LLAMA32_CONFIG[\"rope_base\"],\n",
    "    old_context_length, \n",
    "    LLAMA32_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "# 打印新的RoPE theta值\n",
    "print(\"New RoPE theta:\", LLAMA32_CONFIG[\"rope_base\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "156253fe-aacd-4da2-8f13-705f05c4b11e",
   "metadata": {
    "id": "156253fe-aacd-4da2-8f13-705f05c4b11e"
   },
   "outputs": [],
   "source": [
    "# 使用配置创建Llama3模型实例\n",
    "model = Llama3Model(LLAMA32_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19de6c2c-83ce-456d-8be9-6ec415fe9eb1",
   "metadata": {
    "id": "19de6c2c-83ce-456d-8be9-6ec415fe9eb1"
   },
   "source": [
    "- The following is expected to print True to confirm buffers are reused instead of being (wastefully) recreated:\n",
    "- 下面预期会打印 True 以确认缓冲区被重用而不是(浪费地)重新创建:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e95db6d-2712-41a5-a5e0-86c49897f4cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e95db6d-2712-41a5-a5e0-86c49897f4cf",
    "outputId": "8efc4937-e616-40d0-cd59-670d7eb3e841"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 检查缓冲区是否被重用\n",
    "# 比较第一个和最后一个Transformer块的注意力掩码是否是同一个对象\n",
    "print(model.trf_blocks[0].att.mask is model.trf_blocks[-1].att.mask)\n",
    "# 比较第一个和最后一个Transformer块的余弦缓存是否是同一个对象\n",
    "print(model.trf_blocks[0].att.cos is model.trf_blocks[-1].att.cos)\n",
    "# 比较第一个和最后一个Transformer块的正弦缓存是否是同一个对象\n",
    "print(model.trf_blocks[0].att.sin is model.trf_blocks[-1].att.sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "364e76ca-52f8-4fa5-af37-c4069f9694bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "364e76ca-52f8-4fa5-af37-c4069f9694bc",
    "outputId": "00d7e983-262e-4c65-f322-f4d999311988"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 1,498,482,688\n",
      "\n",
      "Total number of unique parameters: 1,235,814,400\n"
     ]
    }
   ],
   "source": [
    "# 计算模型的总参数量\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "\n",
    "# 考虑权重共享(weight tying)的影响\n",
    "# 由于输出层和词嵌入层共享权重,需要减去词嵌入层的参数量\n",
    "total_params_normalized = total_params - model.tok_emb.weight.numel()\n",
    "print(f\"\\nTotal number of unique parameters: {total_params_normalized:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd5efb03-5a07-46e8-8607-93ed47549d2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fd5efb03-5a07-46e8-8607-93ed47549d2b",
    "outputId": "65c1a95e-b502-4150-9e2e-da619d9053d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 (PyTorch default): 11.42 GB\n",
      "bfloat16: 5.71 GB\n"
     ]
    }
   ],
   "source": [
    "# 计算模型内存占用大小的函数\n",
    "def model_memory_size(model, input_dtype=torch.float32):\n",
    "    # 初始化参数总量和梯度总量为0\n",
    "    total_params = 0\n",
    "    total_grads = 0\n",
    "    # 遍历模型的所有参数\n",
    "    for param in model.parameters():\n",
    "        # 计算每个参数的元素总数\n",
    "        param_size = param.numel()\n",
    "        # 累加参数总量\n",
    "        total_params += param_size\n",
    "        # 检查该参数是否需要存储梯度\n",
    "        if param.requires_grad:\n",
    "            # 累加梯度总量\n",
    "            total_grads += param_size\n",
    "\n",
    "    # 计算缓冲区大小(需要内存的非参数部分)\n",
    "    total_buffers = sum(buf.numel() for buf in model.buffers())\n",
    "\n",
    "    # 计算每个元素的字节大小\n",
    "    element_size = torch.tensor(0, dtype=input_dtype).element_size()\n",
    "    # 计算总内存字节数 = (元素总数) * (每个元素的字节大小)\n",
    "    total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n",
    "\n",
    "    # 将字节转换为GB\n",
    "    total_memory_gb = total_memory_bytes / (1024**3)\n",
    "\n",
    "    return total_memory_gb\n",
    "\n",
    "# 打印float32(PyTorch默认)数据类型下的内存占用\n",
    "print(f\"float32 (PyTorch default): {model_memory_size(model, input_dtype=torch.float32):.2f} GB\")\n",
    "# 打印bfloat16数据类型下的内存占用\n",
    "print(f\"bfloat16: {model_memory_size(model, input_dtype=torch.bfloat16):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31f12baf-f79b-499f-85c0-51328a6a20f5",
   "metadata": {
    "id": "31f12baf-f79b-499f-85c0-51328a6a20f5"
   },
   "outputs": [],
   "source": [
    "# 检查是否有可用的CUDA GPU设备\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "# 检查是否有可用的Apple M1/M2 GPU设备\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\") \n",
    "# 如果没有GPU设备,则使用CPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# 将模型移动到选定的设备上\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e091e1-afa8-4d23-9aea-cced86181bfd",
   "metadata": {
    "id": "78e091e1-afa8-4d23-9aea-cced86181bfd"
   },
   "source": [
    "&nbsp;\n",
    "# 3. Load tokenizer\n",
    "# 3. 加载分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9482b01c-49f9-48e4-ab2c-4a4c75240e77",
   "metadata": {
    "id": "9482b01c-49f9-48e4-ab2c-4a4c75240e77"
   },
   "outputs": [],
   "source": [
    "# 导入所需的Python标准库\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 导入tiktoken分词器相关库\n",
    "import tiktoken\n",
    "from tiktoken.load import load_tiktoken_bpe\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, model_path):\n",
    "        # 检查模型文件是否存在\n",
    "        assert os.path.isfile(model_path), f\"Model file {model_path} not found\"\n",
    "        # 加载BPE(字节对编码)合并规则\n",
    "        mergeable_ranks = load_tiktoken_bpe(model_path)\n",
    "\n",
    "        # 定义特殊token及其对应的ID\n",
    "        self.special_tokens = {\n",
    "            \"<|begin_of_text|>\": 128000,\n",
    "            \"<|end_of_text|>\": 128001,\n",
    "            \"<|start_header_id|>\": 128006,\n",
    "            \"<|end_header_id|>\": 128007,\n",
    "            \"<|eot_id|>\": 128009,\n",
    "        }\n",
    "        # 添加预留的特殊token\n",
    "        self.special_tokens.update({\n",
    "            f\"<|reserved_{i}|>\": 128002 + i for i in range(256) if (128002 + i) not in self.special_tokens.values()\n",
    "        })\n",
    "\n",
    "        # 创建tiktoken编码器实例\n",
    "        self.model = tiktoken.Encoding(\n",
    "            name=Path(model_path).name,\n",
    "            pat_str=r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\",\n",
    "            mergeable_ranks=mergeable_ranks,\n",
    "            special_tokens=self.special_tokens\n",
    "        )\n",
    "\n",
    "\n",
    "    def encode(self, text, bos=False, eos=False, allowed_special=set(), disallowed_special=()):\n",
    "        # 如果需要添加开始标记\n",
    "        if bos:\n",
    "            tokens = [self.special_tokens[\"<|begin_of_text|>\"]]\n",
    "        else:\n",
    "            tokens = []\n",
    "\n",
    "        # 对文本进行编码\n",
    "        tokens += self.model.encode(text, allowed_special=allowed_special, disallowed_special=disallowed_special)\n",
    "\n",
    "        # 如果需要添加结束标记\n",
    "        if eos:\n",
    "            tokens.append(self.special_tokens[\"<|end_of_text|>\"])\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        # 将token解码为文本\n",
    "        return self.model.decode(tokens)\n",
    "\n",
    "\n",
    "class ChatFormat:\n",
    "    def __init__(self, tokenizer):\n",
    "        # 初始化聊天格式化器\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def encode_header(self, message):\n",
    "        # 编码消息头部\n",
    "        tokens = []\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|start_header_id|>\"])\n",
    "        tokens.extend(self.tokenizer.encode(message[\"role\"], bos=False, eos=False))\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|end_header_id|>\"])\n",
    "        tokens.extend(self.tokenizer.encode(\"\\n\\n\", bos=False, eos=False))\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, text):\n",
    "        # 将文本格式化为聊天消息并编码\n",
    "        message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": text\n",
    "        }\n",
    "\n",
    "        tokens = self.encode_header(message)\n",
    "        tokens.extend(\n",
    "            self.tokenizer.encode(message[\"content\"].strip(), bos=False, eos=False)\n",
    "        )\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|eot_id|>\"])\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        # 将token解码为文本\n",
    "        return self.tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b771b60c-c198-4b30-bf10-42031197ae86",
   "metadata": {
    "id": "b771b60c-c198-4b30-bf10-42031197ae86"
   },
   "source": [
    "请注意,Meta AI要求您在下载文件之前接受Llama 3.2许可条款;为此,您需要创建一个Hugging Face Hub账户并访问[meta-llama/Llama-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B)仓库来接受条款\n",
    "- Please note that Meta AI requires that you accept the Llama 3.2 licensing terms before you can download the files; to do this, you have to create a Hugging Face Hub account and visit the [meta-llama/Llama-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B) repository to accept the terms\n",
    "\n",
    "接下来,您需要创建一个访问令牌;要生成具有READ权限的访问令牌,请点击右上角的个人资料图片,然后点击\"Settings\"\n",
    "- Next, you will need to create an access token; to generate an access token with READ permissions, click on the profile picture in the upper right and click on \"Settings\"\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/gpt-to-llama/settings.webp?1\" width=\"300px\">\n",
    "\n",
    "然后,创建并复制访问令牌,以便您可以将其复制并粘贴到下一个代码单元格中\n",
    "- Then, create and copy the access token so you can copy & paste it into the next code cell\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/gpt-to-llama/access-token.webp?1\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9d96dc8-603a-4cb5-8c3e-4d2ca56862ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9d96dc8-603a-4cb5-8c3e-4d2ca56862ed",
    "outputId": "e6e6dc05-7330-45bc-a9a7-331919155bdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /teamspace/studios/this_studio/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# 从huggingface_hub导入login函数\n",
    "from huggingface_hub import login\n",
    "\n",
    "# 登录Hugging Face Hub\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "986bc1a0-804f-4154-80f8-44cefbee1368",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141,
     "referenced_widgets": [
      "a1608feac06d4687967a3e398f01c489",
      "518fb202e4b44aaba47f07d1a61b6762",
      "672cdc5aea954de3af851c001a667ad3",
      "eebf8874618746b39cf4a21a2728dc7f",
      "5176834aa8784bba9ec21234b87a8948",
      "e2dc407afcd945c798e30597fddfcb3c",
      "0dccd57dcc5c43a588157cef957c07e8",
      "33ca0cdf2c7f41598a381c4ebe6a4ee1",
      "ee44487f58454dacb522b1e084ffb733",
      "d2c41e71a3f441deaed091b620ac5603",
      "3326b6141a1a4eba9f316df528a9b99a"
     ]
    },
    "id": "986bc1a0-804f-4154-80f8-44cefbee1368",
    "outputId": "5dd7334b-4c71-465a-94d2-c3e95b9ddc58"
   },
   "outputs": [],
   "source": [
    "# 从huggingface_hub导入下载函数\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# 从Hugging Face Hub下载tokenizer模型文件\n",
    "tokenizer_file_path = hf_hub_download(\n",
    "    repo_id=f\"meta-llama/Llama-3.2-{LLAMA_SIZE_STR}-Instruct\",  # 模型仓库ID\n",
    "    filename=\"original/tokenizer.model\",  # tokenizer文件名\n",
    "    local_dir=f\"Llama-3.2-{LLAMA_SIZE_STR}-Instruct\"  # 本地保存目录\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "_gBhxDtU_nxo",
   "metadata": {
    "id": "_gBhxDtU_nxo"
   },
   "outputs": [],
   "source": [
    "# 初始化tokenizer对象,用于文本分词\n",
    "tokenizer = Tokenizer(tokenizer_file_path)\n",
    "# 初始化chat格式化对象,用于处理对话格式\n",
    "chat_tokenizer = ChatFormat(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c172f89f-d301-439f-b809-46169e5f5945",
   "metadata": {
    "id": "c172f89f-d301-439f-b809-46169e5f5945"
   },
   "source": [
    "&nbsp;\n",
    "# 4. 加载预训练权重\n",
    "# 4. Load pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75166128-5899-4995-9b88-9672e135650e",
   "metadata": {
    "id": "75166128-5899-4995-9b88-9672e135650e"
   },
   "outputs": [],
   "source": [
    "def assign(left, right, tensor_name=\"unknown\"):\n",
    "    # 检查左右张量形状是否匹配,不匹配则抛出异常\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch in tensor '{tensor_name}'. Left: {left.shape}, Right: {right.shape}\")\n",
    "\n",
    "    # 如果right是张量,则克隆并分离,否则转换为张量\n",
    "    if isinstance(right, torch.Tensor):\n",
    "        return torch.nn.Parameter(right.clone().detach())\n",
    "    else:\n",
    "        return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "\n",
    "def load_weights_into_llama(model, param_config, params):\n",
    "    # 加载词嵌入层权重\n",
    "    model.tok_emb.weight = assign(model.tok_emb.weight, params[\"model.embed_tokens.weight\"], \"model.embed_tokens.weight\")\n",
    "\n",
    "    # 遍历每一层Transformer块\n",
    "    for l in range(param_config[\"n_layers\"]):\n",
    "\n",
    "        # 加载注意力层权重\n",
    "        model.trf_blocks[l].att.W_query.weight = assign(\n",
    "            model.trf_blocks[l].att.W_query.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.q_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.q_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].att.W_key.weight = assign(\n",
    "            model.trf_blocks[l].att.W_key.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.k_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.k_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].att.W_value.weight = assign(\n",
    "            model.trf_blocks[l].att.W_value.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.v_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.v_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].att.out_proj.weight = assign(\n",
    "            model.trf_blocks[l].att.out_proj.weight,\n",
    "            params[f\"model.layers.{l}.self_attn.o_proj.weight\"],\n",
    "            f\"model.layers.{l}.self_attn.o_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].norm1.weight = assign(\n",
    "            model.trf_blocks[l].norm1.weight,\n",
    "            params[f\"model.layers.{l}.input_layernorm.weight\"],\n",
    "            f\"model.layers.{l}.input_layernorm.weight\"\n",
    "        )\n",
    "\n",
    "        # 加载前馈网络层权重\n",
    "        model.trf_blocks[l].ff.fc1.weight = assign(\n",
    "            model.trf_blocks[l].ff.fc1.weight,\n",
    "            params[f\"model.layers.{l}.mlp.gate_proj.weight\"],\n",
    "            f\"model.layers.{l}.mlp.gate_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].ff.fc2.weight = assign(\n",
    "            model.trf_blocks[l].ff.fc2.weight,\n",
    "            params[f\"model.layers.{l}.mlp.up_proj.weight\"],\n",
    "            f\"model.layers.{l}.mlp.up_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].ff.fc3.weight = assign(\n",
    "            model.trf_blocks[l].ff.fc3.weight,\n",
    "            params[f\"model.layers.{l}.mlp.down_proj.weight\"],\n",
    "            f\"model.layers.{l}.mlp.down_proj.weight\"\n",
    "        )\n",
    "        model.trf_blocks[l].norm2.weight = assign(\n",
    "            model.trf_blocks[l].norm2.weight,\n",
    "            params[f\"model.layers.{l}.post_attention_layernorm.weight\"],\n",
    "            f\"model.layers.{l}.post_attention_layernorm.weight\"\n",
    "        )\n",
    "\n",
    "    # 加载最终归一化层权重\n",
    "    model.final_norm.weight = assign(model.final_norm.weight, params[\"model.norm.weight\"], \"model.norm.weight\")\n",
    "\n",
    "    # 加载输出层权重,如果存在lm_head则使用,否则使用词嵌入权重(权重共享)\n",
    "    if \"lm_head.weight\" in params.keys():\n",
    "        model.out_head.weight = assign(model.out_head.weight, params[\"lm_head.weight\"], \"lm_head.weight\")\n",
    "    else:\n",
    "        model.out_head.weight = assign(model.out_head.weight, params[\"model.embed_tokens.weight\"], \"model.embed_tokens.weight\")\n",
    "        print(\"Model uses weight tying.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "699cb1b8-a67d-49fb-80a6-0dad9d81f392",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "9881b6995c3f49dc89e6992fd9ab660b",
      "17a3174e65c54476b2e0d1faf8f011ca",
      "1bbf2e62c0754d1593beb4105a7f1ac1",
      "b82112e1dec645d98aa1c1ba64abcb61",
      "271e2bd6a35e4a8b92de8697f7c0be5f",
      "90a79523187446dfa692723b2e5833a7",
      "431ffb83b8c14bf182f0430e07ea6154",
      "a8f1b72a33dd4b548de23fbd95e0da18",
      "25cc36132d384189acfbecc59483134b",
      "bfd06423ad544218968648016e731a46",
      "d029630b63ff44cf807ade428d2eb421"
     ]
    },
    "id": "699cb1b8-a67d-49fb-80a6-0dad9d81f392",
    "outputId": "55b2f28c-142f-4698-9d23-d27456d3ed6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model uses weight tying.\n"
     ]
    }
   ],
   "source": [
    "# 从safetensors库导入load_file函数用于加载模型权重\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "\n",
    "# 如果模型大小是1B,则只需要加载一个权重文件\n",
    "if LLAMA_SIZE_STR == \"1B\":\n",
    "    # 从Hugging Face下载权重文件\n",
    "    weights_file = hf_hub_download(\n",
    "        repo_id=f\"meta-llama/Llama-3.2-{LLAMA_SIZE_STR}-Instruct\",\n",
    "        filename=f\"model.safetensors\", \n",
    "        local_dir=f\"Llama-3.2-{LLAMA_SIZE_STR}-Instruct\"\n",
    "    )\n",
    "    # 加载权重到combined_weights\n",
    "    combined_weights = load_file(weights_file)\n",
    "\n",
    "\n",
    "# 如果模型大小不是1B,需要加载多个权重文件\n",
    "else:\n",
    "    # 初始化空字典存储合并的权重\n",
    "    combined_weights = {}\n",
    "    # 遍历权重文件分片\n",
    "    for i in range(1, 3):\n",
    "        # 从Hugging Face下载每个分片\n",
    "        weights_file = hf_hub_download(\n",
    "            repo_id=f\"meta-llama/Llama-3.2-{LLAMA_SIZE_STR}-Instruct\",\n",
    "            filename=f\"model-0000{i}-of-00002.safetensors\",\n",
    "            local_dir=f\"Llama-3.2-{LLAMA_SIZE_STR}-Instruct\"\n",
    "        )\n",
    "        # 加载当前分片权重\n",
    "        current_weights = load_file(weights_file)\n",
    "        # 更新合并的权重字典\n",
    "        combined_weights.update(current_weights)\n",
    "\n",
    "\n",
    "# 将权重加载到模型中\n",
    "load_weights_into_llama(model, LLAMA32_CONFIG, combined_weights)\n",
    "# 将模型移动到指定设备(CPU/GPU)\n",
    "model.to(device)\n",
    "# 删除权重释放内存\n",
    "del combined_weights  # free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f9f7ccc-70cb-41ff-9c25-44336042fc37",
   "metadata": {
    "id": "7f9f7ccc-70cb-41ff-9c25-44336042fc37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight tying: True\n"
     ]
    }
   ],
   "source": [
    "# 检查模型是否使用了权重绑定(weight tying)\n",
    "# 通过比较token嵌入层和输出层的权重是否相等来验证\n",
    "print(\"Weight tying:\", torch.equal(model.tok_emb.weight, model.out_head.weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d07df1-4401-4792-b549-7c4cc5632323",
   "metadata": {
    "id": "57d07df1-4401-4792-b549-7c4cc5632323"
   },
   "source": [
    "&nbsp;\n",
    "# 5. Generate text\n",
    "# 5. 生成文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b8401c6-e244-4cb7-9849-2ba71ce758d5",
   "metadata": {
    "id": "7b8401c6-e244-4cb7-9849-2ba71ce758d5"
   },
   "outputs": [],
   "source": [
    "# 将文本转换为token ID的函数\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    # 使用tokenizer将文本编码为token ID序列\n",
    "    encoded = tokenizer.encode(text)\n",
    "    # 将编码后的序列转换为tensor,并添加batch维度\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # 添加batch维度\n",
    "    return encoded_tensor\n",
    "\n",
    "\n",
    "# 将token ID转换回文本的函数\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    # 移除batch维度\n",
    "    flat = token_ids.squeeze(0)  # 移除batch维度\n",
    "    # 使用tokenizer将token ID序列解码为文本\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "\n",
    "# 文本生成函数\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # 循环生成指定数量的新token\n",
    "    for _ in range(max_new_tokens):\n",
    "        # 获取最后context_size个token作为条件输入\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        # 使用模型进行推理,不计算梯度\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        # 只关注最后一个时间步的logits\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # 使用top-k采样过滤logits\n",
    "        if top_k is not None:\n",
    "            # 获取logits中最大的top_k个值\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            # 获取第k大的值作为阈值\n",
    "            min_val = top_logits[:, -1]\n",
    "            # 将小于阈值的logits设置为负无穷\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)\n",
    "\n",
    "        # 应用温度缩放\n",
    "        if temperature > 0.0:\n",
    "            # 将logits除以温度参数\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # 应用softmax获取概率分布\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # 从概率分布中采样下一个token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # 如果温度为0,直接选择概率最高的token\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        # 如果生成了结束符且指定了eos_id,则提前结束生成\n",
    "        if idx_next == eos_id:  \n",
    "            break\n",
    "\n",
    "        # 将新生成的token添加到序列中\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c7a04fa-6aac-416b-8f63-f1e19227633d",
   "metadata": {
    "id": "1c7a04fa-6aac-416b-8f63-f1e19227633d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Llamas are herbivores, which means they primarily eat plants. Their diet consists mainly of:\n",
      "\n",
      "1. Grasses: Llamas love to graze on various types of grasses, including tall grasses and grassy meadows.\n",
      "2. Hay: Llamas also eat hay, which is a dry, compressed form of grass or other plants.\n",
      "3. Alfalfa: Alfalfa is a legume that is commonly fed to llamas. It is high in protein and fiber.\n",
      "4. Other plants: Llamas will also eat other plants, such as wild grasses, shrubs, and trees.\n",
      "\n",
      "It's worth noting that the diet of llamas can vary depending on the region, climate,\n"
     ]
    }
   ],
   "source": [
    "# 设置提示词\n",
    "PROMPT = \"What do llamas eat?\"\n",
    "\n",
    "# 设置随机种子以确保结果可复现\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# 生成文本\n",
    "token_ids = generate(\n",
    "    model=model,  # 使用训练好的模型\n",
    "    idx=text_to_token_ids(PROMPT, chat_tokenizer).to(device),  # 将提示词转换为token ID并移至指定设备\n",
    "    max_new_tokens=150,  # 最多生成150个新token\n",
    "    context_size=LLAMA32_CONFIG[\"context_length\"],  # 使用配置中指定的上下文长度\n",
    "    top_k=1,  # 只保留概率最高的1个token\n",
    "    temperature=0.  # 温度为0,即始终选择概率最高的token\n",
    ")\n",
    "\n",
    "# 将生成的token ID转换回文本\n",
    "output_text = token_ids_to_text(token_ids, tokenizer)\n",
    "\n",
    "\n",
    "def clean_text(text, header_end=\"assistant<|end_header_id|>\\n\\n\"):\n",
    "    \"\"\"\n",
    "    清理生成的文本,移除头部标记\n",
    "    \n",
    "    参数:\n",
    "        text: 需要清理的原始文本\n",
    "        header_end: 头部标记的结束字符串,默认为\"assistant<|end_header_id|>\\n\\n\"\n",
    "    \"\"\"\n",
    "    # 查找头部标记结束位置\n",
    "    index = text.find(header_end)\n",
    "\n",
    "    if index != -1:\n",
    "        # 如果找到头部标记,返回其后的文本内容(去除首尾空白)\n",
    "        return text[index + len(header_end):].strip()\n",
    "    else:\n",
    "        # 如果未找到头部标记,返回原始文本\n",
    "        return text\n",
    "\n",
    "# 打印清理后的生成文本\n",
    "print(\"Output text:\\n\", clean_text(output_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549324d6-5c71-4147-ae21-2e67675faa3d",
   "metadata": {
    "id": "549324d6-5c71-4147-ae21-2e67675faa3d"
   },
   "source": [
    "&nbsp;\n",
    "# What's next?\n",
    "# 接下来是什么?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6edaaae-2de1-406c-8ffa-897cdfa3808c",
   "metadata": {
    "id": "e6edaaae-2de1-406c-8ffa-897cdfa3808c"
   },
   "source": [
    "- The notebook was kept purposefully minimal; if you are interested in additional explanation about the individual components, check out the following two companion notebooks:\n",
    "- 本笔记本保持简洁明了；如果您对各个组件的更多解释感兴趣，请查看以下两个配套笔记本：\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/gpt-to-llama/gpt-and-all-llamas.webp\">\n",
    "\n",
    "  1. [Converting a From-Scratch GPT Architecture to Llama 2](converting-gpt-to-llama2.ipynb)\n",
    "  1. [从零开始将GPT架构转换为Llama 2](converting-gpt-to-llama2.ipynb)\n",
    "  2. [Converting Llama 2 to Llama 3.2 From Scratch](converting-llama2-to-llama3.ipynb)\n",
    "  2. [从零开始将Llama 2转换为Llama 3.2](converting-llama2-to-llama3.ipynb)\n",
    "  \n",
    "- For those interested in a comprehensive guide on building a large language model from scratch and gaining a deeper understanding of its mechanics, you might like my [Build a Large Language Model (From Scratch)](http://mng.bz/orYv)\n",
    "- 对于那些想要全面了解如何从零开始构建大型语言模型并深入理解其机制的人来说，您可能会喜欢我的[从零开始构建大型语言模型](http://mng.bz/orYv)\n",
    "\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0dccd57dcc5c43a588157cef957c07e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "17a3174e65c54476b2e0d1faf8f011ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_90a79523187446dfa692723b2e5833a7",
      "placeholder": "​",
      "style": "IPY_MODEL_431ffb83b8c14bf182f0430e07ea6154",
      "tabbable": null,
      "tooltip": null,
      "value": "model.safetensors:  35%"
     }
    },
    "1bbf2e62c0754d1593beb4105a7f1ac1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_a8f1b72a33dd4b548de23fbd95e0da18",
      "max": 2471645608,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_25cc36132d384189acfbecc59483134b",
      "tabbable": null,
      "tooltip": null,
      "value": 880803840
     }
    },
    "25cc36132d384189acfbecc59483134b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "271e2bd6a35e4a8b92de8697f7c0be5f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3326b6141a1a4eba9f316df528a9b99a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "33ca0cdf2c7f41598a381c4ebe6a4ee1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "431ffb83b8c14bf182f0430e07ea6154": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "5176834aa8784bba9ec21234b87a8948": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "518fb202e4b44aaba47f07d1a61b6762": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_e2dc407afcd945c798e30597fddfcb3c",
      "placeholder": "​",
      "style": "IPY_MODEL_0dccd57dcc5c43a588157cef957c07e8",
      "tabbable": null,
      "tooltip": null,
      "value": "tokenizer.model: 100%"
     }
    },
    "672cdc5aea954de3af851c001a667ad3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_33ca0cdf2c7f41598a381c4ebe6a4ee1",
      "max": 2183982,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ee44487f58454dacb522b1e084ffb733",
      "tabbable": null,
      "tooltip": null,
      "value": 2183982
     }
    },
    "90a79523187446dfa692723b2e5833a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9881b6995c3f49dc89e6992fd9ab660b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_17a3174e65c54476b2e0d1faf8f011ca",
       "IPY_MODEL_1bbf2e62c0754d1593beb4105a7f1ac1",
       "IPY_MODEL_b82112e1dec645d98aa1c1ba64abcb61"
      ],
      "layout": "IPY_MODEL_271e2bd6a35e4a8b92de8697f7c0be5f",
      "tabbable": null,
      "tooltip": null
     }
    },
    "a1608feac06d4687967a3e398f01c489": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_518fb202e4b44aaba47f07d1a61b6762",
       "IPY_MODEL_672cdc5aea954de3af851c001a667ad3",
       "IPY_MODEL_eebf8874618746b39cf4a21a2728dc7f"
      ],
      "layout": "IPY_MODEL_5176834aa8784bba9ec21234b87a8948",
      "tabbable": null,
      "tooltip": null
     }
    },
    "a8f1b72a33dd4b548de23fbd95e0da18": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b82112e1dec645d98aa1c1ba64abcb61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_bfd06423ad544218968648016e731a46",
      "placeholder": "​",
      "style": "IPY_MODEL_d029630b63ff44cf807ade428d2eb421",
      "tabbable": null,
      "tooltip": null,
      "value": " 870M/2.47G [00:20&lt;00:37, 42.8MB/s]"
     }
    },
    "bfd06423ad544218968648016e731a46": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d029630b63ff44cf807ade428d2eb421": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "d2c41e71a3f441deaed091b620ac5603": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2dc407afcd945c798e30597fddfcb3c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee44487f58454dacb522b1e084ffb733": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "eebf8874618746b39cf4a21a2728dc7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_d2c41e71a3f441deaed091b620ac5603",
      "placeholder": "​",
      "style": "IPY_MODEL_3326b6141a1a4eba9f316df528a9b99a",
      "tabbable": null,
      "tooltip": null,
      "value": " 2.18M/2.18M [00:00&lt;00:00, 9.47MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
