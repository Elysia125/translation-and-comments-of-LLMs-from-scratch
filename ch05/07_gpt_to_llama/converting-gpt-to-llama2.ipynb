{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0_xya1nyDHfY",
   "metadata": {
    "id": "0_xya1nyDHfY"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l62zIRRSBy_R",
   "metadata": {
    "id": "l62zIRRSBy_R"
   },
   "source": [
    "# Converting a From-Scratch GPT Architecture to Llama 2\n",
    "# 从零开始将 GPT 架构转换为 Llama 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aFmxTQbwCUMl",
   "metadata": {
    "id": "aFmxTQbwCUMl"
   },
   "source": [
    "- In this notebook, we convert the original GPT architecture into a Llama 2 model step by step (note the GPT and GPT-2 share the same architecture)\n",
    "- 在本笔记本中,我们将原始 GPT 架构一步步转换为 Llama 2 模型(注意 GPT 和 GPT-2 共享相同的架构)\n",
    "\n",
    "- Why not Llama 1 or Llama 3?\n",
    "- 为什么不是 Llama 1 或 Llama 3?\n",
    "\n",
    "   - The Llama 1 architecture is similar to Llama 2, except that Llama 2 has a larger context window (which is nice); the Llama 1 weights are not readily available and have more usage restrictions, so it makes more sense to focus on Llama 2\n",
    "   - Llama 1 架构与 Llama 2 类似,只是 Llama 2 有更大的上下文窗口(这很好);Llama 1 的权重不容易获得,并且使用限制更多,所以专注于 Llama 2 更有意义\n",
    "\n",
    "   - Regarding Llama 3, I will share a separate notebook to convert Llama 2 to Llama 3 (there are only a few small additional changes)\n",
    "   - 关于 Llama 3,我将分享一个单独的笔记本来将 Llama 2 转换为 Llama 3(只有一些小的额外更改)\n",
    "\n",
    "- The explanations are purposefully kept minimal in this notebook not to bloat it unnecessarily and focus on the main code\n",
    "- 本笔记本中的解释特意保持最小化,以避免不必要的臃肿并专注于主要代码\n",
    "\n",
    "- For more information, please see the Llama 2 paper: [Llama 2: Open Foundation and Fine-Tuned Chat Models (2023)](https://arxiv.org/abs/2307.09288)\n",
    "- 更多信息,请参阅 Llama 2 论文:[Llama 2:开放基础和微调聊天模型(2023)](https://arxiv.org/abs/2307.09288)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ohhMKUWvGm9z",
   "metadata": {
    "id": "ohhMKUWvGm9z"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/gpt-to-llama/gpt2-to-llama2-llama3.webp?1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JBpQwU89ETA1",
   "metadata": {
    "id": "JBpQwU89ETA1"
   },
   "source": [
    "- Packages that are being used in this notebook:\n",
    "- 本章节中使用的包:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34a9a440-84c2-42cc-808b-38677cb6af8a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34a9a440-84c2-42cc-808b-38677cb6af8a",
    "outputId": "8118963b-3c72-43af-874b-439ffebdc94c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface_hub version: 0.24.7\n",
      "sentencepiece version: 0.2.0\n",
      "torch version: 2.4.1+cu121\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"huggingface_hub\",  # to download pretrained weights\n",
    "    \"sentencepiece\",    # to implement the tokenizer\n",
    "    \"torch\",            # to implement the model\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UJJneXpTEg4W",
   "metadata": {
    "id": "UJJneXpTEg4W"
   },
   "source": [
    "&nbsp;\n",
    "# 1. Convert the GPT model implementation step by step\n",
    "# 1. 逐步转换 GPT 模型实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v1zpfX2GHBKa",
   "metadata": {
    "id": "v1zpfX2GHBKa"
   },
   "source": [
    "- In this section, we go through the GPT model code from [chapter 4](../../ch04/01_main-chapter-code/ch04.ipynb) and modify it step by step to implement the Llama 2 architecture\n",
    "- 在本节中,我们将逐步修改[第4章](../../ch04/01_main-chapter-code/ch04.ipynb)中的 GPT 模型代码来实现 Llama 2 架构\n",
    "- Later, we load the original Llama 2 weights shared by Meta AI\n",
    "- 之后,我们将加载 Meta AI 共享的原始 Llama 2 权重"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979c7b6d-1370-4da1-8bfb-a2b27537bf2f",
   "metadata": {
    "id": "979c7b6d-1370-4da1-8bfb-a2b27537bf2f"
   },
   "source": [
    "&nbsp;\n",
    "## 1.1 Replace LayerNorm with RMSNorm layer\n",
    "## 1.1 用 RMSNorm 层替换 LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b27fc8-23a1-4e0e-a1ea-792e0428e5e6",
   "metadata": {
    "id": "f8b27fc8-23a1-4e0e-a1ea-792e0428e5e6"
   },
   "source": [
    "- First, we replace LayerNorm by Root Mean Square Layer Normalization (RMSNorm)\n",
    "- 首先,我们用均方根层归一化(RMSNorm)替换 LayerNorm\n",
    "- LayerNorm normalizes inputs using mean and variance, while RMSNorm uses only the root mean square, which improves computational efficiency  \n",
    "- LayerNorm 使用均值和方差来归一化输入,而 RMSNorm 只使用均方根,这提高了计算效率\n",
    "- The RMSNorm operation is as follows, where $x$ is the input $\\gamma$ is a trainable parameter (vector), and $\\epsilon$ is a small constant to avoid zero-division errors:\n",
    "- RMSNorm 操作如下,其中 $x$ 是输入,$\\gamma$ 是可训练参数(向量),$\\epsilon$ 是一个小常数,用于避免除零错误:\n",
    "\n",
    "$$y_i = \\frac{x_i}{\\text{RMS}(x)} \\gamma_i, \\quad \\text{where} \\quad \\text{RMS}(x) = \\sqrt{\\epsilon + \\frac{1}{n} \\sum x_i^2}$$\n",
    "\n",
    "- For more details, please see the paper [Root Mean Square Layer Normalization (2019)](https://arxiv.org/abs/1910.07467)\n",
    "- 更多详细信息,请参阅论文 [Root Mean Square Layer Normalization (2019)](https://arxiv.org/abs/1910.07467)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7094381-9499-4e9e-93f9-b79470da3771",
   "metadata": {
    "id": "d7094381-9499-4e9e-93f9-b79470da3771"
   },
   "outputs": [],
   "source": [
    "# 导入PyTorch库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "#####################################\n",
    "# Chapter 4\n",
    "#####################################\n",
    "\n",
    "# class LayerNorm(nn.Module):\n",
    "#     def __init__(self, emb_dim):\n",
    "#         super().__init__()\n",
    "#         self.eps = 1e-5\n",
    "#         self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "#         self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         mean = x.mean(dim=-1, keepdim=True)\n",
    "#         var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "#         norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "#         return self.scale * norm_x + self.shift\n",
    "\n",
    "\n",
    "# 定义RMSNorm层\n",
    "class RMSNorm(nn.Module):\n",
    "    # 初始化函数,接收嵌入维度和epsilon参数\n",
    "    def __init__(self, emb_dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps  # 设置epsilon值,避免除零错误\n",
    "        self.emb_dim = emb_dim  # 保存嵌入维度\n",
    "        self.weight = nn.Parameter(torch.ones(emb_dim)).float()  # 创建可训练的权重参数\n",
    "\n",
    "    # 前向传播函数\n",
    "    def forward(self, x):\n",
    "        means = x.pow(2).mean(dim=-1, keepdim=True)  # 计算输入的均方值\n",
    "        x_normed = x * torch.rsqrt(means + self.eps)  # 使用均方根进行归一化\n",
    "        return (x_normed * self.weight).to(dtype=x.dtype)  # 应用权重并返回结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mtWC8DOmIu0F",
   "metadata": {
    "id": "mtWC8DOmIu0F"
   },
   "source": [
    "- The following code cell checks that this implementation works the same as PyTorch's built-in implementation:\n",
    "- 下面的代码单元检查此实现与PyTorch的内置实现是否一致:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e41ade7a-bf06-48b1-8b7e-0e4037d5753f",
   "metadata": {
    "id": "e41ade7a-bf06-48b1-8b7e-0e4037d5753f"
   },
   "outputs": [],
   "source": [
    "# 设置随机种子以确保结果可重现\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# 创建一个随机张量作为示例输入,形状为(2,3,4)\n",
    "example_batch = torch.randn(2, 3, 4)\n",
    "\n",
    "# 实例化我们自定义的RMSNorm层\n",
    "rms_norm = RMSNorm(emb_dim=example_batch.shape[-1])\n",
    "# 实例化PyTorch内置的RMSNorm层作为对照\n",
    "rmsnorm_pytorch = torch.nn.RMSNorm(example_batch.shape[-1], eps=1e-5)\n",
    "\n",
    "# 验证两个实现的输出结果是否一致\n",
    "assert torch.allclose(rms_norm(example_batch), rmsnorm_pytorch(example_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb81f83-c38c-46a4-b763-aa630a32e357",
   "metadata": {
    "id": "5eb81f83-c38c-46a4-b763-aa630a32e357"
   },
   "source": [
    "&nbsp;\n",
    "## 1.2 Replace GELU with SiLU activation\n",
    "## 1.2 将GELU激活函数替换为SiLU激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8aa702-f118-4ff6-9135-90725ec8756c",
   "metadata": {
    "id": "0b8aa702-f118-4ff6-9135-90725ec8756c"
   },
   "source": [
    "- Llama uses the SiLU activation function (instead of GELU), which is also known as the Swish function:\n",
    "- Llama使用SiLU激活函数(代替GELU)，也被称为Swish函数:\n",
    "\n",
    "$$\n",
    "\\text{silu}(x) = x \\cdot \\sigma(x), \\quad \\text{where} \\quad \\sigma(x) \\text{ is the logistic sigmoid.}\n",
    "$$\n",
    "\n",
    "- For more information, see the SiLU paper: [Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning (2017)](https://arxiv.org/abs/1702.03118)\n",
    "- 更多信息请参见SiLU论文: [Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning (2017)](https://arxiv.org/abs/1702.03118)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a74f3757-c634-4a3a-a8f3-6334cde454fe",
   "metadata": {
    "id": "a74f3757-c634-4a3a-a8f3-6334cde454fe"
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Chapter 4\n",
    "#####################################\n",
    "\n",
    "# class GELU(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return 0.5 * x * (1 + torch.tanh(\n",
    "#             torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "#             (x + 0.044715 * torch.pow(x, 3))\n",
    "#         ))\n",
    "\n",
    "\n",
    "# SiLU激活函数类定义\n",
    "class SiLU(nn.Module):\n",
    "    # 初始化函数\n",
    "    def __init__(self):\n",
    "        super(SiLU, self).__init__()\n",
    "\n",
    "    # 前向传播函数\n",
    "    def forward(self, x):\n",
    "        # 返回输入x与其sigmoid函数值的逐元素乘积\n",
    "        return x * torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72ecbe2e-b6b7-4319-972b-1a7fefa3368c",
   "metadata": {
    "id": "72ecbe2e-b6b7-4319-972b-1a7fefa3368c"
   },
   "outputs": [],
   "source": [
    "# 创建SiLU激活函数实例\n",
    "silu = SiLU()\n",
    "\n",
    "# 验证自定义SiLU实现与PyTorch内置SiLU函数的输出一致\n",
    "assert torch.allclose(silu(example_batch), torch.nn.functional.silu(example_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9b5167-1da9-46c8-9964-8036b3b1deb9",
   "metadata": {
    "id": "4f9b5167-1da9-46c8-9964-8036b3b1deb9"
   },
   "source": [
    "&nbsp;\n",
    "## 1.3 Update the FeedForward module\n",
    "## 1.3 更新前馈神经网络模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a381e7a-b807-472e-91c9-3e4e3fc5ad91",
   "metadata": {
    "id": "3a381e7a-b807-472e-91c9-3e4e3fc5ad91"
   },
   "source": [
    "- In fact, Llama uses a \"Gates Linear Unit\" (GLU) variant of SiLU called SwiGLU, which essentially results in a slightly differently structured `FeedForward` module\n",
    "- 实际上，Llama使用了一种叫做SwiGLU的SiLU门控线性单元(GLU)变体，这本质上导致了`FeedForward`模块结构的略微不同\n",
    "\n",
    "- SwiGLU uses a gating mechanism in the feedforward layer, with the formula:\n",
    "- SwiGLU在前馈层中使用门控机制，其公式为:\n",
    "\n",
    "$$\\text{SwiGLU}(x) = \\text{SiLU}(\\text{Linear}_1(x)) * (\\text{Linear}_2(x))$$\n",
    "\n",
    "- Here, $\\text{Linear}_1$ and $\\text{Linear}_2$ are two linear layers, and $*$ denotes element-wise multiplication\n",
    "- 这里，$\\text{Linear}_1$和$\\text{Linear}_2$是两个线性层，$*$表示逐元素相乘\n",
    "\n",
    "- The third linear layer, $\\text{Linear}_3$, is applied after this gated activation\n",
    "- 第三个线性层$\\text{Linear}_3$在这个门控激活之后应用\n",
    "\n",
    "- For more information, see SwiGLU paper: [GLU Variants Improve Transformer (2020)](https://arxiv.org/abs/2002.05202)\n",
    "- 更多信息请参见SwiGLU论文: [GLU Variants Improve Transformer (2020)](https://arxiv.org/abs/2002.05202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d25fbe3d-b7c9-4772-ad67-bc0527e4e20a",
   "metadata": {
    "id": "d25fbe3d-b7c9-4772-ad67-bc0527e4e20a"
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Chapter 4\n",
    "#####################################\n",
    "# class FeedForward(nn.Module):\n",
    "#     def __init__(self, cfg):\n",
    "#         super().__init__()\n",
    "#         self.layers = nn.Sequential(\n",
    "#             nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "#             GELU(),\n",
    "#             nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "477568cb-03cd-4510-b663-a42ce3ec64a2",
   "metadata": {
    "id": "477568cb-03cd-4510-b663-a42ce3ec64a2"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        \"\"\"前馈神经网络模块\n",
    "        \n",
    "        参数:\n",
    "            cfg: 配置字典，包含以下键:\n",
    "                - emb_dim: 输入维度\n",
    "                - hidden_dim: 隐藏层维度 \n",
    "                - dtype: 权重数据类型\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
    "        self.silu = SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"前向传播\n",
    "        \n",
    "        参数:\n",
    "            x: 输入张量, shape: [batch_size, seq_len, emb_dim]\n",
    "            \n",
    "        返回:\n",
    "            输出张量, shape: [batch_size, seq_len, emb_dim]\n",
    "        \"\"\"\n",
    "        x_fc1 = self.fc1(x)  # 第一个线性变换\n",
    "        x_fc2 = self.fc2(x)  # 第二个线性变换\n",
    "        x = self.silu(x_fc1) * x_fc2  # SwiGLU激活\n",
    "        return self.fc3(x)  # 第三个线性变换"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qcD8LSHNhBRW",
   "metadata": {
    "id": "qcD8LSHNhBRW"
   },
   "source": [
    "- Note that we also added a `dtype=cfg[\"dtype\"]` setting above, which will allow us to load the model directly in lower precision formats later to save memory (versus instantiating it in the original 32-bit precision format and then converting it)\n",
    "- 注意我们在上面添加了 `dtype=cfg[\"dtype\"]` 设置,这使我们可以直接以较低精度格式加载模型以节省内存(而不是先以原始32位精度格式实例化然后再转换)\n",
    "\n",
    "- We also set `bias=False` since Llama doesn't use any bias units\n",
    "- 我们还设置了 `bias=False`,因为 Llama 不使用任何偏置单元"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b7bf4f-99d0-42c1-807c-5074d2cc1949",
   "metadata": {
    "id": "f6b7bf4f-99d0-42c1-807c-5074d2cc1949"
   },
   "source": [
    "&nbsp;\n",
    "## 1.4 Implement RoPE\n",
    "## 1.4 实现 RoPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3487a6f-0373-49d8-b2eb-f8ee05d42884",
   "metadata": {
    "id": "d3487a6f-0373-49d8-b2eb-f8ee05d42884"
   },
   "source": [
    "- In the GPT model, the positional embeddings are implemented as follows:\n",
    "- 在 GPT 模型中,位置嵌入的实现如下:\n",
    "\n",
    "```python\n",
    "self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "```\n",
    "- Instead of these absolute positional embeddings, Llama uses relative positional embeddings, called rotary position embeddings (RoPE for short)\n",
    "- 与这些绝对位置嵌入不同，Llama使用相对位置嵌入，称为旋转位置嵌入(简称RoPE)\n",
    "- The reference paper for RoPE is [RoFormer: Enhanced Transformer with Rotary Position Embedding (2021)](https://arxiv.org/abs/2104.09864)\n",
    "- RoPE的参考论文是[RoFormer: Enhanced Transformer with Rotary Position Embedding (2021)](https://arxiv.org/abs/2104.09864)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a34180fb-448f-44e9-a244-0c736051687b",
   "metadata": {
    "id": "a34180fb-448f-44e9-a244-0c736051687b"
   },
   "outputs": [],
   "source": [
    "def precompute_rope_params(head_dim, theta_base=10_000, context_length=4096):\n",
    "    \"\"\"预计算RoPE(旋转位置编码)的参数\n",
    "    \n",
    "    参数:\n",
    "        head_dim: 注意力头的维度\n",
    "        theta_base: RoPE的基础频率,默认为10000\n",
    "        context_length: 上下文长度,默认为4096\n",
    "        \n",
    "    返回:\n",
    "        cos, sin: 预计算的余弦和正弦值\n",
    "    \"\"\"\n",
    "    # 确保head_dim是偶数,因为需要成对处理\n",
    "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "\n",
    "    # 计算倒数频率: 1 / (theta_base^(2i/d)),其中i是位置索引,d是head_dim\n",
    "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2)[: (head_dim // 2)].float() / head_dim))\n",
    "\n",
    "    # 生成位置索引: [0, 1, 2, ..., context_length-1]\n",
    "    positions = torch.arange(context_length)\n",
    "\n",
    "    # 计算角度: 位置 * 倒数频率,得到形状为(context_length, head_dim//2)的矩阵\n",
    "    angles = positions[:, None] * inv_freq[None, :]  # Shape: (context_length, head_dim // 2)\n",
    "\n",
    "    # 将角度扩展到匹配head_dim,通过复制一次实现\n",
    "    angles = torch.cat([angles, angles], dim=1)  # Shape: (context_length, head_dim)\n",
    "\n",
    "    # 预计算每个位置的正弦和余弦值\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "\n",
    "    return cos, sin\n",
    "\n",
    "def compute_rope(x, cos, sin):\n",
    "    \"\"\"应用RoPE(旋转位置编码)到输入张量\n",
    "    \n",
    "    参数:\n",
    "        x: 输入张量,形状为(batch_size, num_heads, seq_len, head_dim)\n",
    "        cos: 预计算的余弦值\n",
    "        sin: 预计算的正弦值\n",
    "        \n",
    "    返回:\n",
    "        经过RoPE变换后的张量\n",
    "    \"\"\"\n",
    "    # 获取输入张量的维度\n",
    "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
    "    # 确保head_dim是偶数\n",
    "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
    "\n",
    "    # 将输入张量在最后一个维度上分成两半\n",
    "    x1 = x[..., : head_dim // 2]  # 前半部分\n",
    "    x2 = x[..., head_dim // 2 :]  # 后半部分\n",
    "\n",
    "    # 调整sin和cos的形状以匹配输入张量\n",
    "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)  # 扩展为(1, 1, seq_len, head_dim)\n",
    "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # 应用旋转变换: [x1*cos - x2*sin, x2*cos + x1*sin]\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)  # 将-x2和x1拼接\n",
    "    x_rotated = (x * cos) + (rotated * sin)  # 应用旋转\n",
    "\n",
    "    # 确保输出与输入具有相同的数据类型\n",
    "    return x_rotated.to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e841b8e-75aa-49db-b1a7-d5c2116dc299",
   "metadata": {
    "id": "8e841b8e-75aa-49db-b1a7-d5c2116dc299"
   },
   "source": [
    "- The following is an example of applying RoPE to the `q` and `k` tensors:\n",
    "- 下面是将RoPE应用于`q`和`k`张量的示例:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c89f022-7167-4001-8c21-8e012878733f",
   "metadata": {
    "id": "8c89f022-7167-4001-8c21-8e012878733f"
   },
   "outputs": [],
   "source": [
    "# 设置基本参数\n",
    "# 批次大小为2\n",
    "batch_size = 2\n",
    "# 上下文长度为5\n",
    "context_len = 5  \n",
    "# 注意力头数为4\n",
    "num_heads = 4\n",
    "# 每个头的维度为16\n",
    "head_dim = 16\n",
    "\n",
    "# 实例化RoPE参数\n",
    "# 使用预定义的函数计算余弦和正弦值\n",
    "cos, sin = precompute_rope_params(head_dim=head_dim, context_length=context_len)\n",
    "\n",
    "# 创建示例的查询和键张量\n",
    "# 设置随机种子以保证结果可复现\n",
    "torch.manual_seed(123)\n",
    "# 创建随机查询张量,形状为(batch_size, num_heads, context_len, head_dim)\n",
    "queries = torch.randn(batch_size, num_heads, context_len, head_dim)\n",
    "# 创建随机键张量,形状与查询张量相同\n",
    "keys = torch.randn(batch_size, num_heads, context_len, head_dim)\n",
    "\n",
    "# 应用旋转位置编码\n",
    "# 对查询张量应用RoPE变换\n",
    "queries_rot = compute_rope(queries, cos, sin)\n",
    "# 对键张量应用RoPE变换\n",
    "keys_rot = compute_rope(keys, cos, sin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78127b0-dda2-4c5a-98dd-bae8f5fe8297",
   "metadata": {
    "id": "f78127b0-dda2-4c5a-98dd-bae8f5fe8297"
   },
   "source": [
    "&nbsp;\n",
    "## 1.5 Add RoPE to MultiHeadAttention module\n",
    "## 1.5 为MultiHeadAttention模块添加RoPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RnmSHROLhhR3",
   "metadata": {
    "id": "RnmSHROLhhR3"
   },
   "source": [
    "- It's important to note that GPT applies the positional embeddings to the inputs, whereas Llama applies rotations to the query and key vectors in the self-attention mechanism itself\n",
    "- 需要注意的是，GPT将位置嵌入应用于输入，而Llama在自注意力机制中对查询和键向量应用旋转\n",
    "\n",
    "- Here, we modify the `MultiHeadAttention` class with the appropriate RoPE code\n",
    "- 在这里，我们使用适当的RoPE代码修改`MultiHeadAttention`类\n",
    "\n",
    "- In addition, we remove the `qkv_bias` option and hardcode the `bias=False` setting\n",
    "- 此外，我们移除了`qkv_bias`选项并将`bias=False`设置硬编码\n",
    "\n",
    "- Also, we add a dtype setting to be able to instantiate the model with a lower precision later\n",
    "- 同时，我们添加了dtype设置，以便稍后能够以较低精度实例化模型\n",
    "\n",
    "- Tip: since the `TransformerBlock`s (in the next section) are repeated exactly, we could simplify the code and only initialize the buffers once instead for each `MultiHeadAttention` module; however, we add the precomputed RoPE parameters to the `MultiHeadAttention` class so that it can function as a standalone module\n",
    "- 提示：由于`TransformerBlock`（在下一节中）是完全重复的，我们可以简化代码，只为每个`MultiHeadAttention`模块初始化一次缓冲区；但是，我们将预计算的RoPE参数添加到`MultiHeadAttention`类中，使其能够作为独立模块运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d81a441e-0b79-4a8b-8291-ea7f55d58c84",
   "metadata": {
    "id": "d81a441e-0b79-4a8b-8291-ea7f55d58c84"
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "# 第3章\n",
    "#####################################\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, num_heads, dtype=None):  # ,dropout, num_heads, qkv_bias=False):\n",
    "        # 继承父类初始化\n",
    "        super().__init__()\n",
    "        # 确保输出维度可以被注意力头数整除\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        # 保存输出维度、注意力头数和每个头的维度\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # 将投影维度减小以匹配所需的输出维度\n",
    "\n",
    "        ################################### NEW ###################################\n",
    "        # 为所有线性层设置bias=False和dtype=dtype\n",
    "        ###########################################################################\n",
    "        # 创建查询、键、值的线性变换层\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        self.out_proj = nn.Linear(d_out, d_out, bias=False, dtype=dtype)  # 用于组合头部输出的线性层\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 注册因果掩码作为缓冲区\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "        ################################### NEW ###################################\n",
    "        # 预计算RoPE参数并注册为缓冲区\n",
    "        cos, sin = precompute_rope_params(head_dim=self.head_dim, context_length=context_length)\n",
    "        self.register_buffer(\"cos\", cos)\n",
    "        self.register_buffer(\"sin\", sin)\n",
    "        ###########################################################################\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 获取输入张量的维度\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        # 通过线性层计算键、查询和值\n",
    "        keys = self.W_key(x)  # 形状: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # 重塑张量以添加num_heads维度\n",
    "        # 展开最后一个维度: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # 转置维度: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        ################################### NEW ###################################\n",
    "        # 对键和查询应用RoPE变换\n",
    "        keys = compute_rope(keys, self.cos, self.sin)\n",
    "        queries = compute_rope(queries, self.cos, self.sin)\n",
    "        ###########################################################################\n",
    "\n",
    "        # 使用因果掩码计算缩放点积注意力(即自注意力)\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # 计算每个头的点积\n",
    "\n",
    "        # 将原始掩码截断到token数量并转换为布尔值\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # 使用掩码填充注意力分数\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        # 计算注意力权重\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        # attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # 计算上下文向量 形状: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # 组合所有头部，其中self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # 可选的投影层\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-lt9SfnVioB3",
   "metadata": {
    "id": "-lt9SfnVioB3"
   },
   "source": [
    "- Below is an example using the `MultiHeadAttention` module on an example input:\n",
    "- 下面是一个使用`MultiHeadAttention`模块的输入示例:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03f15755-0083-483f-963b-99b599651638",
   "metadata": {
    "id": "03f15755-0083-483f-963b-99b599651638"
   },
   "outputs": [],
   "source": [
    "# 设置参数\n",
    "batch_size = 1  # 批次大小\n",
    "context_len = 100  # 上下文长度\n",
    "max_context_len = 4096  # 最大上下文长度\n",
    "embed_dim = 128  # 嵌入维度\n",
    "num_heads = 4  # 注意力头数\n",
    "\n",
    "# 创建一个随机输入张量作为示例\n",
    "example_batch = torch.randn((batch_size, context_len, embed_dim))\n",
    "\n",
    "# 初始化多头注意力模块\n",
    "mha = MultiHeadAttention(\n",
    "    d_in=embed_dim,  # 输入维度\n",
    "    d_out=embed_dim,  # 输出维度\n",
    "    context_length=max_context_len,  # 最大上下文长度\n",
    "    num_heads=num_heads  # 注意力头数\n",
    ")\n",
    "\n",
    "# 对示例输入进行前向传播\n",
    "mha(example_batch)\n",
    "\n",
    "del mha  # 删除模型以释放内存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a1a272-a038-4b8f-aaaa-f4b241e7f23f",
   "metadata": {
    "id": "e5a1a272-a038-4b8f-aaaa-f4b241e7f23f"
   },
   "source": [
    "&nbsp;\n",
    "## 1.6 Update the TransformerBlock module\n",
    "## 1.6 更新 TransformerBlock 模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255f70ac-9c2e-4328-8af7-1c298b8d4a18",
   "metadata": {
    "id": "255f70ac-9c2e-4328-8af7-1c298b8d4a18"
   },
   "source": [
    "- At this stage, most of the hard work is already done; we can now update the `TransformerBlock` to use the code we implemented above\n",
    "- 在这个阶段,大部分困难的工作已经完成;我们现在可以更新`TransformerBlock`来使用我们上面实现的代码\n",
    "- This means we\n",
    "- 这意味着我们需要:\n",
    " - replace LayerNorm with RMSNorm\n",
    " - 用RMSNorm替换LayerNorm\n",
    " - remove dropout\n",
    " - 移除dropout\n",
    " - remove the `qkv_bias` setting\n",
    " - 移除`qkv_bias`设置\n",
    " - add the `dtype` setting\n",
    " - 添加`dtype`设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e110721-bf2b-42b3-989a-1635b1658af0",
   "metadata": {
    "id": "2e110721-bf2b-42b3-989a-1635b1658af0"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        # 继承自nn.Module基类\n",
    "        super().__init__()\n",
    "        \n",
    "        # 初始化多头注意力层\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],      # 输入维度\n",
    "            d_out=cfg[\"emb_dim\"],     # 输出维度\n",
    "            context_length=cfg[\"context_length\"],  # 上下文长度\n",
    "            num_heads=cfg[\"n_heads\"],  # 注意力头数\n",
    "            dtype=cfg[\"dtype\"]         # 数据类型\n",
    "            # 移除dropout和qkv_bias参数\n",
    "        )\n",
    "        \n",
    "        # 初始化前馈神经网络层\n",
    "        self.ff = FeedForward(cfg)\n",
    "\n",
    "        # 使用RMSNorm替代LayerNorm进行归一化\n",
    "        self.norm1 = RMSNorm(cfg[\"emb_dim\"])  # 第一个归一化层\n",
    "        self.norm2 = RMSNorm(cfg[\"emb_dim\"])  # 第二个归一化层\n",
    "        \n",
    "        # 移除dropout层\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 注意力模块的残差连接\n",
    "        shortcut = x                   # 保存输入用于残差连接\n",
    "        x = self.norm1(x)             # 第一次归一化\n",
    "        x = self.att(x)               # 通过多头注意力层,输出形状为[batch_size, num_tokens, emb_size]\n",
    "        x = x + shortcut              # 添加残差连接\n",
    "\n",
    "        # 前馈网络模块的残差连接\n",
    "        shortcut = x                   # 保存输入用于残差连接\n",
    "        x = self.norm2(x)             # 第二次归一化\n",
    "        x = self.ff(x)                # 通过前馈神经网络\n",
    "        x = x + shortcut              # 添加残差连接\n",
    "\n",
    "        return x                       # 返回最终输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada953bc-e2c0-4432-a32d-3f7efa3f6e0f",
   "metadata": {
    "id": "ada953bc-e2c0-4432-a32d-3f7efa3f6e0f"
   },
   "source": [
    "&nbsp;\n",
    "## 1.7 Update the model class\n",
    "## 1.7 更新模型类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5d991a-559b-47be-96f4-31b881ab2da8",
   "metadata": {
    "id": "ba5d991a-559b-47be-96f4-31b881ab2da8"
   },
   "source": [
    "- As you may recall from [chapter 5](../01_main-chapter-code/ch05.ipynb), the `TransformerBlock` is a repeated block within the main model\n",
    "- 回顾第5章，`TransformerBlock`是主模型中的一个重复块\n",
    "\n",
    "- Our Llama model is almost complete; we just have to update the model code surrounding the `TransformerBlock`\n",
    "- 我们的Llama模型即将完成，只需要更新`TransformerBlock`周围的模型代码\n",
    "\n",
    "- This means we\n",
    "- 这意味着我们需要：\n",
    "\n",
    "  - remove absolute positional embeddings since we have RoPE embeddings now\n",
    "  - 由于现在使用了RoPE嵌入，移除绝对位置嵌入\n",
    "  \n",
    "  - replace LayerNorm with RMSNorm\n",
    "  - 用RMSNorm替换LayerNorm\n",
    "  \n",
    "  - remove dropout\n",
    "  - 移除dropout层\n",
    "  \n",
    "  - add the dtype setting\n",
    "  - 添加dtype设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf8240fe-5d7f-4e7e-b1ac-e0755aab5e79",
   "metadata": {
    "id": "cf8240fe-5d7f-4e7e-b1ac-e0755aab5e79"
   },
   "outputs": [],
   "source": [
    "# 将GPT模型类改为Llama2模型类\n",
    "class Llama2Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        # 初始化父类\n",
    "        super().__init__()\n",
    "        \n",
    "        # 创建词嵌入层,将词表大小映射到嵌入维度,指定数据类型\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
    "        \n",
    "        # 注释掉位置嵌入,因为使用RoPE代替\n",
    "        # self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        # 注释掉dropout层,Llama2不使用\n",
    "        # self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        # 创建transformer块的序列,重复n_layers次\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        # 将最后的LayerNorm替换为RMSNorm\n",
    "        # self.final_norm = LayerNorm(cfg[\"emb_dim\"]) \n",
    "        self.final_norm = RMSNorm(cfg[\"emb_dim\"])\n",
    "        \n",
    "        # 创建输出层,将嵌入维度映射回词表大小,不使用偏置项,指定数据类型\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        # 获取输入的batch大小和序列长度\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        \n",
    "        # 通过词嵌入层获取词向量\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        \n",
    "        # 注释掉位置嵌入的计算\n",
    "        # pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        \n",
    "        # 直接使用词嵌入作为输入,不再加入位置嵌入\n",
    "        x = tok_embeds  # + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        \n",
    "        # 注释掉dropout\n",
    "        # x = self.drop_emb(x)\n",
    "        \n",
    "        # 依次通过transformer块\n",
    "        x = self.trf_blocks(x)\n",
    "        \n",
    "        # 通过最终的RMSNorm层\n",
    "        x = self.final_norm(x)\n",
    "        \n",
    "        # 通过输出层得到logits\n",
    "        logits = self.out_head(x)\n",
    "        \n",
    "        # 返回预测结果\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc94940-aaeb-45b9-9399-3a69b8043e60",
   "metadata": {
    "id": "4bc94940-aaeb-45b9-9399-3a69b8043e60"
   },
   "source": [
    "&nbsp;\n",
    "## 2. Initialize model\n",
    "## 2. 初始化模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bG--zY-Ljj1f",
   "metadata": {
    "id": "bG--zY-Ljj1f"
   },
   "source": [
    "- The model code is now complete, and we are ready to initialize it\n",
    "- 模型代码现已完成，我们可以开始初始化它\n",
    "- In [chapter 5](../01_main-chapter-code/ch05.ipynb), we used the following config file to specify the 124M-parameter GPT model:\n",
    "- 在[第5章](../01_main-chapter-code/ch05.ipynb)中，我们使用了以下配置文件来指定124M参数的GPT模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b7428df-3d02-4ccd-97b5-a629bdabbe8f",
   "metadata": {
    "id": "4b7428df-3d02-4ccd-97b5-a629bdabbe8f"
   },
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # 词表大小\n",
    "    \"context_length\": 1024,  # 上下文长度\n",
    "    \"emb_dim\": 768,          # 嵌入维度\n",
    "    \"n_heads\": 12,           # 注意力头数量\n",
    "    \"n_layers\": 12,          # 层数\n",
    "    \"drop_rate\": 0.1,        # Dropout比率\n",
    "    \"qkv_bias\": False        # 查询-键-值偏置项\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bVi8uiBjw2T",
   "metadata": {
    "id": "8bVi8uiBjw2T"
   },
   "source": [
    "- For reference, the 1.5B parameter GPT model config is shown below as well:\n",
    "- 作为参考，下面也展示了15亿参数的GPT模型配置:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "tAOojV_mkEnd",
   "metadata": {
    "id": "tAOojV_mkEnd"
   },
   "outputs": [],
   "source": [
    "GPT_CONFIG_1558M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size 词表大小\n",
    "    \"context_length\": 1024,  # Context length 上下文长度\n",
    "    \"emb_dim\": 1600,         # Embedding dimension 嵌入维度\n",
    "    \"n_heads\": 25,           # Number of attention heads 注意力头数量\n",
    "    \"n_layers\": 48,          # Number of layers 层数\n",
    "    \"drop_rate\": 0.1,        # Dropout rate Dropout比率\n",
    "    \"qkv_bias\": False        # Query-Key-Value bias 查询-键-值偏置项\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HoGGRAGykQTE",
   "metadata": {
    "id": "HoGGRAGykQTE"
   },
   "source": [
    "- Similarly, we can define a Llama 2 config file for the 7B model (we ignore the other larger models for simplicity here):\n",
    "- 类似地，我们可以为7B模型定义一个Llama 2配置文件（为了简单起见，这里我们忽略其他更大的模型）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0564727-2d35-4f0c-b0fc-cde1e9134a18",
   "metadata": {
    "id": "e0564727-2d35-4f0c-b0fc-cde1e9134a18"
   },
   "outputs": [],
   "source": [
    "LLAMA2_CONFIG_7B = {\n",
    "    \"vocab_size\": 32000,     # 词表大小\n",
    "    \"context_length\": 4096,  # 上下文长度\n",
    "    \"emb_dim\": 4096,         # 嵌入维度\n",
    "    \"n_heads\": 32,           # 注意力头数量\n",
    "    \"n_layers\": 32,          # 层数\n",
    "    \"hidden_dim\": 11008,     # 新增：前馈网络中的中间维度大小\n",
    "    \"dtype\": torch.bfloat16  # 新增：使用低精度数据类型以节省内存\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FAP7fiBzkaBz",
   "metadata": {
    "id": "FAP7fiBzkaBz"
   },
   "source": [
    "- Using these settings, we can now initialize a Llama 2 7B model (note that this requires ~26 GB of memory)\n",
    "- 使用这些设置，我们现在可以初始化一个Llama 2 7B模型（注意这需要约26 GB内存）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7004d785-ac9a-4df5-8760-6807fc604686",
   "metadata": {
    "id": "7004d785-ac9a-4df5-8760-6807fc604686"
   },
   "outputs": [],
   "source": [
    "# 使用Llama2配置初始化一个7B参数的模型\n",
    "model = Llama2Model(LLAMA2_CONFIG_7B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6079f747-8f20-4c6b-8d38-7156f1101729",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6079f747-8f20-4c6b-8d38-7156f1101729",
    "outputId": "0a0eb34b-1a21-4c11-804f-b40007bda5a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 6,738,415,616\n"
     ]
    }
   ],
   "source": [
    "# 计算模型的总参数量\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "# 打印总参数量\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Bx14NtzWk2wj",
   "metadata": {
    "id": "Bx14NtzWk2wj"
   },
   "source": [
    "- As shown above, the model contains 6.7 billion parameters (commonly rounded and referred to as a 7B model)\n",
    "- 如上所示，该模型包含67亿个参数（通常四舍五入并称为7B模型）\n",
    "- Additionally, we can calculate the memory requirements for this model using the code below:\n",
    "- 此外，我们可以使用以下代码计算该模型的内存需求："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0df1c79e-27a7-4b0f-ba4e-167fe107125a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0df1c79e-27a7-4b0f-ba4e-167fe107125a",
    "outputId": "11ced939-556d-4511-d5c0-10a94ed3df32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 (PyTorch default): 52.33 GB\n",
      "bfloat16: 26.17 GB\n"
     ]
    }
   ],
   "source": [
    "# 定义一个函数来计算模型的内存占用大小\n",
    "def model_memory_size(model, input_dtype=torch.float32):\n",
    "    # 初始化参数总数为0\n",
    "    total_params = 0\n",
    "    # 初始化梯度总数为0 \n",
    "    total_grads = 0\n",
    "    # 遍历模型的所有参数\n",
    "    for param in model.parameters():\n",
    "        # 计算每个参数的元素总数\n",
    "        param_size = param.numel()\n",
    "        # 累加到参数总数中\n",
    "        total_params += param_size\n",
    "        # 检查该参数是否需要存储梯度\n",
    "        if param.requires_grad:\n",
    "            # 如果需要梯度,累加到梯度总数中\n",
    "            total_grads += param_size\n",
    "\n",
    "    # 计算缓冲区大小(需要内存的非参数部分)\n",
    "    total_buffers = sum(buf.numel() for buf in model.buffers())\n",
    "\n",
    "    # 计算每个元素的字节大小\n",
    "    element_size = torch.tensor(0, dtype=input_dtype).element_size()\n",
    "    # 计算总内存字节数 = (元素总数) * (每个元素的字节大小)\n",
    "    total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n",
    "\n",
    "    # 将字节转换为GB\n",
    "    total_memory_gb = total_memory_bytes / (1024**3)\n",
    "\n",
    "    # 返回总内存大小(GB)\n",
    "    return total_memory_gb\n",
    "\n",
    "# 打印使用float32数据类型时的内存占用\n",
    "print(f\"float32 (PyTorch default): {model_memory_size(model, input_dtype=torch.float32):.2f} GB\")\n",
    "# 打印使用bfloat16数据类型时的内存占用\n",
    "print(f\"bfloat16: {model_memory_size(model, input_dtype=torch.bfloat16):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zudd-5PulKFL",
   "metadata": {
    "id": "zudd-5PulKFL"
   },
   "source": [
    "- Lastly, we can also transfer the model to an NVIDIA or Apple Silicon GPU if applicable:\n",
    "- 最后，如果可用的话，我们还可以将模型转移到 NVIDIA 或 Apple Silicon GPU 上:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4c50e19-1402-45b6-8ccd-9077b2ba836d",
   "metadata": {
    "id": "a4c50e19-1402-45b6-8ccd-9077b2ba836d"
   },
   "outputs": [],
   "source": [
    "# 检查是否有CUDA GPU可用\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "# 检查是否有Apple Silicon GPU (MPS)可用    \n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\") \n",
    "# 如果都不可用则使用CPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# 将模型移动到选定的设备上\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc64a06-27dc-46ec-9e6d-1700a8227d34",
   "metadata": {
    "id": "5dc64a06-27dc-46ec-9e6d-1700a8227d34"
   },
   "source": [
    "&nbsp;\n",
    "## 3. Load tokenizer\n",
    "## 3. 加载分词器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb30f0c-6144-4bed-87d9-6b2bac377005",
   "metadata": {
    "id": "0eb30f0c-6144-4bed-87d9-6b2bac377005"
   },
   "source": [
    "- In this section, we are going to load the tokenizer for the model\n",
    "- 在本节中,我们将为模型加载分词器\n",
    "- Llama 2 uses Google's [SentencePiece](https://github.com/google/sentencepiece) tokenizer instead of OpenAI's [Tiktoken](https://github.com/openai/tiktoken) (but Llama 3 uses Tiktoken)\n",
    "- Llama 2 使用 Google 的 [SentencePiece](https://github.com/google/sentencepiece) 分词器而不是 OpenAI 的 [Tiktoken](https://github.com/openai/tiktoken) (但 Llama 3 使用 Tiktoken)\n",
    "- Meta AI shared the original Llama 2 model weights and tokenizer vocabulary on the Hugging Face Hub\n",
    "- Meta AI 在 Hugging Face Hub 上共享了原始的 Llama 2 模型权重和分词器词汇表\n",
    "- We will download the tokenizer vocabulary from the Hub and load it into SentencePiece\n",
    "- 我们将从 Hub 下载分词器词汇表并将其加载到 SentencePiece 中\n",
    "- Uncomment and run the following code to install the required libraries:\n",
    "- 取消注释并运行以下代码以安装所需的库:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "768989ea-dc60-4dc8-ae84-cbb3fd224422",
   "metadata": {
    "id": "768989ea-dc60-4dc8-ae84-cbb3fd224422"
   },
   "outputs": [],
   "source": [
    "# !pip install huggingface_hub sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KbnlzsbYmJU6",
   "metadata": {
    "id": "KbnlzsbYmJU6"
   },
   "source": [
    "- Please note that Meta AI requires that you accept the Llama 2 licensing terms before you can download the files; to do this, you have to create a Hugging Face Hub account and visit the [meta-llama/Llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b) repository to accept the terms\n",
    "- 请注意，Meta AI 要求您在下载文件之前接受 Llama 2 许可条款；为此，您需要创建一个 Hugging Face Hub 账户并访问 [meta-llama/Llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b) 仓库来接受条款\n",
    "\n",
    "- Next, you will need to create an access token; to generate an access token with READ permissions, click on the profile picture in the upper right and click on \"Settings\"\n",
    "- 接下来，您需要创建一个访问令牌；要生成具有读取权限的访问令牌，请点击右上角的个人头像，然后点击\"Settings\"（设置）\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/gpt-to-llama/settings.webp?1\" width=\"300px\">\n",
    "\n",
    "- Then, create and copy the access token so you can copy & paste it into the next code cell\n",
    "- 然后，创建并复制访问令牌，以便您可以将其复制并粘贴到下一个代码单元格中\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/gpt-to-llama/access-token.webp?1\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3357a230-b678-4691-a238-257ee4e80185",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3357a230-b678-4691-a238-257ee4e80185",
    "outputId": "768ed6af-ce14-40bc-ca18-117b4b448269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# 从 huggingface_hub 导入 login 函数\n",
    "from huggingface_hub import login\n",
    "# 导入 json 模块\n",
    "import json\n",
    "\n",
    "# 打开并读取配置文件\n",
    "with open(\"config.json\", \"r\") as config_file:\n",
    "    # 加载 JSON 配置\n",
    "    config = json.load(config_file)\n",
    "    # 从配置中获取访问令牌\n",
    "    access_token = config[\"HF_ACCESS_TOKEN\"]\n",
    "\n",
    "# 使用访问令牌登录 Hugging Face Hub\n",
    "login(token=access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IxGh6ZYQo0VN",
   "metadata": {
    "id": "IxGh6ZYQo0VN"
   },
   "source": [
    "- After login via the access token, which is necessary to verify that we accepted the Llama 2 licensing terms, we can now download the tokenizer vocabulary:\n",
    "- 通过访问令牌登录后，这是验证我们已接受 Llama 2 许可条款所必需的，现在我们可以下载分词器词汇表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69714ea8-b9b8-4687-8392-f3abb8f93a32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153,
     "referenced_widgets": [
      "e6c75a6aa7b942fe84160e286e3acb3d",
      "08f0bf9459bd425498a5cb236f9d4a72",
      "10251d6f724e43788c41d4b7879cbfd3",
      "53a973c0853b44418698136bd04df039",
      "bdb071e7145a4007ae01599333e72612",
      "6b1821a7f4574e3aba09c1e410cc81e4",
      "8c2873eaec3445888ad3d54ad7387950",
      "0c8f7044966e4207b12352503c67dcbb",
      "8b5951213c9e4798a258146d61d02d11",
      "2c05df3f91e64df7b33905b1065a76f7",
      "742ae5487f2648fcae7ca8e22c7f8db9"
     ]
    },
    "id": "69714ea8-b9b8-4687-8392-f3abb8f93a32",
    "outputId": "c230fec9-5c71-4a41-90ab-8a34d114ea01"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c75a6aa7b942fe84160e286e3acb3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 从 huggingface_hub 导入 hf_hub_download 函数\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# 下载分词器文件\n",
    "tokenizer_file = hf_hub_download(\n",
    "    repo_id=\"meta-llama/Llama-2-7b\",    # 模型仓库 ID\n",
    "    filename=\"tokenizer.model\",          # 分词器文件名\n",
    "    local_dir=\"Llama-2-7b\"              # 本地保存目录\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gp7iQ8cXAJLv",
   "metadata": {
    "id": "gp7iQ8cXAJLv"
   },
   "source": [
    "- To provide a more familiar interface for the tokenizer, we define a small `LlamaTokenizer` wrapper class:\n",
    "- 为了提供一个更熟悉的分词器接口，我们定义一个小型的 `LlamaTokenizer` 包装类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "Ef4WxhjOBOOc",
   "metadata": {
    "id": "Ef4WxhjOBOOc"
   },
   "outputs": [],
   "source": [
    "# 导入 sentencepiece 库并重命名为 spm\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "# 定义 LlamaTokenizer 类用于封装分词器功能\n",
    "class LlamaTokenizer:\n",
    "    # 初始化方法，接收分词器文件路径作为参数\n",
    "    def __init__(self, filepath):\n",
    "        # 创建 SentencePieceProcessor 实例\n",
    "        sp = spm.SentencePieceProcessor()\n",
    "        # 加载分词器模型文件\n",
    "        sp.load(tokenizer_file)\n",
    "        # 将分词器实例保存为类属性\n",
    "        self.tokenizer = sp\n",
    "\n",
    "    # 将文本编码为 token ID 序列的方法\n",
    "    def encode(self, text):\n",
    "        # 调用分词器的 encode_as_ids 方法进行编码\n",
    "        return self.tokenizer.encode_as_ids(text)\n",
    "\n",
    "    # 将 token ID 序列解码为文本的方法\n",
    "    def decode(self, ids):\n",
    "        # 调用分词器的 decode_pieces 方法进行解码\n",
    "        return self.tokenizer.decode_pieces(ids)\n",
    "\n",
    "\n",
    "# 使用分词器文件创建 LlamaTokenizer 实例\n",
    "tokenizer = LlamaTokenizer(tokenizer_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NVhmFeX3pT_M",
   "metadata": {
    "id": "NVhmFeX3pT_M"
   },
   "source": [
    "- We can now use the `generate` function to have the Llama 2 model generate new text:\n",
    "- 现在我们可以使用 `generate` 函数让 Llama 2 模型生成新的文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0a2b5cd-6cba-4d72-b8ff-04d8315d483e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0a2b5cd-6cba-4d72-b8ff-04d8315d483e",
    "outputId": "acd5065d-8900-4ba8-ef85-968365f3a0cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort movesαllRadius deletingpretcc否']; future eer napulate lackус während inter DES издаSchéon로жа Bass differencespadxsnu ;; ctx始\n"
     ]
    }
   ],
   "source": [
    "# 从前面章节导入所需函数\n",
    "from previous_chapters import generate, text_to_token_ids, token_ids_to_text\n",
    "\n",
    "\n",
    "# 设置随机种子以保证结果可复现\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# 生成文本\n",
    "token_ids = generate(\n",
    "    model=model,  # 使用加载的模型\n",
    "    idx=text_to_token_ids(\"Every effort moves\", tokenizer).to(device),  # 将输入文本转换为token ID并移至设备\n",
    "    max_new_tokens=30,  # 最多生成30个新token\n",
    "    context_size=LLAMA2_CONFIG_7B[\"context_length\"],  # 使用Llama 2配置中的上下文长度\n",
    "    top_k=1,  # 只选择概率最高的token\n",
    "    temperature=0.  # 温度为0,使输出确定性\n",
    ")\n",
    "\n",
    "# 将生成的token ID转换回文本并打印\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93WTtAA5paYV",
   "metadata": {
    "id": "93WTtAA5paYV"
   },
   "source": [
    " - Of course, as we can see above, the text is nonsensical since we haven't trained the Llama 2 model yet\n",
    " - 当然,正如我们在上面看到的,由于我们还没有训练 Llama 2 模型,生成的文本是没有意义的\n",
    " - In the next section, instead of training it ourselves, which would cost tens to hundreds of thousands of dollars, we load the pretrained weights from Meta AI\n",
    " - 在下一节中,我们不会自己训练模型(这将花费数十万到数十万美元),而是加载来自 Meta AI 的预训练权重"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63cc248-1d27-4eb6-aa50-173b436652f8",
   "metadata": {
    "id": "f63cc248-1d27-4eb6-aa50-173b436652f8"
   },
   "source": [
    "&nbsp;\n",
    "## 4. Load pretrained weights\n",
    "## 4. 加载预训练权重"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aKeN7rUfqZMI",
   "metadata": {
    "id": "aKeN7rUfqZMI"
   },
   "source": [
    "- We are loading the [\"meta-llama/Llama-2-7b\"](https://huggingface.co/meta-llama/Llama-2-7b) base model below, which is a simple text completion model before finetuning\n",
    "- 我们将在下面加载 [\"meta-llama/Llama-2-7b\"](https://huggingface.co/meta-llama/Llama-2-7b) 基础模型，这是一个在微调之前的简单文本补全模型\n",
    "- Alternatively, you can load the instruction-finetuned and aligned [\"meta-llama/Llama-2-7b-chat\"](https://huggingface.co/meta-llama/Llama-2-7b-chat) model by modifying the string in the next code cell accordingly\n",
    "- 或者，您可以通过修改下一个代码单元中的字符串来加载经过指令微调和对齐的 [\"meta-llama/Llama-2-7b-chat\"](https://huggingface.co/meta-llama/Llama-2-7b-chat) 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fa9c06c-7a53-4b4d-9ce4-acc027322ee4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "66e777955e8748df878f118f07f38dab",
      "da89ae3ea4d2474e98f64ada608f3cea",
      "93e6da39c25f4edfaa72056c89df1f7f",
      "b628603e4cb0405398c916587ee96756",
      "93bedcb9245e44a0a1eb7e4155070f66",
      "0723f467d37b4904819a8bb33ebda10f",
      "e54928776bc649339002adced63738b0",
      "d8e0f42068af4cb094e2f115f76e06e0",
      "0a939565b6e94f08bee0a66e0f9827d4",
      "a5fedbb7ec2e43d99711bb4cd84b9486",
      "0c186f6539714d8eab023969ce47c500"
     ]
    },
    "id": "5fa9c06c-7a53-4b4d-9ce4-acc027322ee4",
    "outputId": "0d8942cc-e5e2-4e77-ec41-1ac7bec7d94f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e777955e8748df878f118f07f38dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "consolidated.00.pth:   0%|          | 0.00/13.5G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 从 Hugging Face Hub 下载 Llama-2-7b 模型权重文件\n",
    "# repo_id: 模型仓库ID\n",
    "# filename: 权重文件名\n",
    "# local_dir: 本地保存目录\n",
    "weights_file = hf_hub_download(\n",
    "   repo_id=\"meta-llama/Llama-2-7b\",\n",
    "   filename=\"consolidated.00.pth\", \n",
    "   local_dir=\"Llama-2-7b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e67cca5c-ba4b-4be5-85c7-fdceae8a5701",
   "metadata": {
    "id": "e67cca5c-ba4b-4be5-85c7-fdceae8a5701"
   },
   "outputs": [],
   "source": [
    "# 加载模型权重文件\n",
    "# weights_only=True 表示只加载权重参数,不加载优化器状态等其他信息\n",
    "weights = torch.load(weights_file, weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-15SJ7btq2zE",
   "metadata": {
    "id": "-15SJ7btq2zE"
   },
   "source": [
    "- The `weights` contains the following tensors (only the first 15 are shown for simplicity):\n",
    "- `weights` 包含以下张量(为简单起见,仅显示前 15 个):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee26bd0b-fea9-4924-97f7-409c14f28e49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ee26bd0b-fea9-4924-97f7-409c14f28e49",
    "outputId": "fa83d38a-bb41-4cb2-d3c7-c573bfe1f8a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok_embeddings.weight',\n",
       " 'norm.weight',\n",
       " 'output.weight',\n",
       " 'layers.0.attention.wq.weight',\n",
       " 'layers.0.attention.wk.weight',\n",
       " 'layers.0.attention.wv.weight',\n",
       " 'layers.0.attention.wo.weight',\n",
       " 'layers.0.feed_forward.w1.weight',\n",
       " 'layers.0.feed_forward.w2.weight',\n",
       " 'layers.0.feed_forward.w3.weight',\n",
       " 'layers.0.attention_norm.weight',\n",
       " 'layers.0.ffn_norm.weight',\n",
       " 'layers.1.attention.wq.weight',\n",
       " 'layers.1.attention.wk.weight',\n",
       " 'layers.1.attention.wv.weight']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 列出权重字典中的前15个键\n",
    "list(weights.keys())[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UeeSpnunrDFB",
   "metadata": {
    "id": "UeeSpnunrDFB"
   },
   "source": [
    "- The following function, modeled after the `load_weights_into_gpt` function in [chapter 5](../01_main-chapter-code/ch05.ipynb), loads the pretrained weights into our Llama 2 model:\n",
    "- 以下函数参考了[第5章](../01_main-chapter-code/ch05.ipynb)中的`load_weights_into_gpt`函数，将预训练权重加载到我们的 Llama 2 模型中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3820e2a7-4f26-41bc-953b-f3879b0aff65",
   "metadata": {
    "id": "3820e2a7-4f26-41bc-953b-f3879b0aff65"
   },
   "outputs": [],
   "source": [
    "# 定义一个辅助函数,用于将权重参数赋值给模型\n",
    "def assign(left, right):\n",
    "    # 检查左右两个张量的形状是否匹配\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "\n",
    "    # 如果右侧是张量,则创建一个新的参数对象\n",
    "    if isinstance(right, torch.Tensor):\n",
    "        return torch.nn.Parameter(right.clone().detach())\n",
    "    # 如果右侧不是张量,则先转换为张量再创建参数对象\n",
    "    else:\n",
    "        return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "\n",
    "# 定义加载预训练权重到Llama模型的主函数\n",
    "def load_weights_into_llama(model, param_config, params):\n",
    "    # 加载词嵌入层权重\n",
    "    model.tok_emb.weight = assign(model.tok_emb.weight, params[\"tok_embeddings.weight\"])\n",
    "\n",
    "    # 遍历每一个transformer层\n",
    "    for l in range(param_config[\"n_layers\"]):\n",
    "\n",
    "        # 加载注意力层的权重\n",
    "        # 加载查询矩阵权重\n",
    "        model.trf_blocks[l].att.W_query.weight = assign(\n",
    "            model.trf_blocks[l].att.W_query.weight,\n",
    "            params[f\"layers.{l}.attention.wq.weight\"]\n",
    "        )\n",
    "        # 加载键矩阵权重\n",
    "        model.trf_blocks[l].att.W_key.weight = assign(\n",
    "            model.trf_blocks[l].att.W_key.weight,\n",
    "            params[f\"layers.{l}.attention.wk.weight\"]\n",
    "        )\n",
    "        # 加载值矩阵权重\n",
    "        model.trf_blocks[l].att.W_value.weight = assign(\n",
    "            model.trf_blocks[l].att.W_value.weight,\n",
    "            params[f\"layers.{l}.attention.wv.weight\"]\n",
    "        )\n",
    "        # 加载输出投影层权重\n",
    "        model.trf_blocks[l].att.out_proj.weight = assign(\n",
    "            model.trf_blocks[l].att.out_proj.weight,\n",
    "            params[f\"layers.{l}.attention.wo.weight\"]\n",
    "        )\n",
    "        # 加载第一个层归一化权重\n",
    "        model.trf_blocks[l].norm1.weight = assign(\n",
    "            model.trf_blocks[l].norm1.weight,\n",
    "            params[f\"layers.{l}.attention_norm.weight\"]\n",
    "        )\n",
    "\n",
    "        # 加载前馈网络层的权重\n",
    "        # 加载第一个全连接层权重\n",
    "        model.trf_blocks[l].ff.fc1.weight = assign(\n",
    "            model.trf_blocks[l].ff.fc1.weight,\n",
    "            params[f\"layers.{l}.feed_forward.w1.weight\"]\n",
    "        )\n",
    "        # 注意:w2和w3在权重文件中的顺序是相反的\n",
    "        # 加载第二个全连接层权重(使用w3)\n",
    "        model.trf_blocks[l].ff.fc2.weight = assign(\n",
    "            model.trf_blocks[l].ff.fc2.weight,\n",
    "            params[f\"layers.{l}.feed_forward.w3.weight\"]\n",
    "        )\n",
    "        # 加载第三个全连接层权重(使用w2)\n",
    "        model.trf_blocks[l].ff.fc3.weight = assign(\n",
    "            model.trf_blocks[l].ff.fc3.weight,\n",
    "            params[f\"layers.{l}.feed_forward.w2.weight\"]\n",
    "        )\n",
    "        # 加载第二个层归一化权重\n",
    "        model.trf_blocks[l].norm2.weight = assign(\n",
    "            model.trf_blocks[l].norm2.weight,\n",
    "            params[f\"layers.{l}.ffn_norm.weight\"]\n",
    "        )\n",
    "\n",
    "    # 加载最终输出层的权重\n",
    "    # 加载最后的层归一化权重\n",
    "    model.final_norm.weight = assign(model.final_norm.weight, params[\"norm.weight\"])\n",
    "    # 加载输出头层权重\n",
    "    model.out_head.weight = assign(model.out_head.weight, params[\"output.weight\"])\n",
    "\n",
    "\n",
    "# 使用上述函数加载权重到模型中\n",
    "load_weights_into_llama(model, LLAMA2_CONFIG_7B, weights)\n",
    "# 将模型移动到指定设备(CPU/GPU)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TDuv_Us2rNvk",
   "metadata": {
    "id": "TDuv_Us2rNvk"
   },
   "source": [
    "- Next, we are ready to use the model for text generation\n",
    "- 接下来，我们准备使用模型进行文本生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "240987e8-a023-462e-9376-9edfb27559ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "240987e8-a023-462e-9376-9edfb27559ec",
    "outputId": "044f24b3-4018-4860-834d-6c2731b9e47c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort has been made to ensure that the information contained in this website is accurate and up to date and correct at the time of publication\n"
     ]
    }
   ],
   "source": [
    "# 设置随机种子以确保可重复性\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# 生成文本\n",
    "token_ids = generate(\n",
    "    model=model,  # 使用加载的模型\n",
    "    idx=text_to_token_ids(\"Every effort\", tokenizer).to(device),  # 将输入文本转换为token并移至设备\n",
    "    max_new_tokens=25,  # 最多生成25个新token\n",
    "    context_size=LLAMA2_CONFIG_7B[\"context_length\"],  # 使用配置中定义的上下文长度\n",
    "    top_k=1,  # 只选择概率最高的token\n",
    "    temperature=0.  # 温度为0,使输出更确定性\n",
    ")\n",
    "\n",
    "# 将生成的token转换回文本并打印\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72ed949-b6c0-4966-922f-eb0da732c404",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## 5. Using the instruction-finetuned model\n",
    "## 5. 使用指令微调模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "akyo7WNyF_YL",
   "metadata": {
    "id": "akyo7WNyF_YL"
   },
   "source": [
    "- As mentioned earlier, above we used the pretrained base model; if you want to use a model capable of following instructions, use the `\"meta-llama/Llama-2-7b-chat\"` model instead, as shown below\n",
    "- 如前所述，上面我们使用了预训练的基础模型；如果你想使用一个能够遵循指令的模型，请改用`\"meta-llama/Llama-2-7b-chat\"`模型，如下所示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "nbvAV7vaz6yc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "3b2448a60f5f4ba5b2c686037c8ecd78",
      "60c5932944f24f5fad1d8da89c8e5ae9",
      "aa31aed1b8854a4281fd7e81c60e1205",
      "d4acf06c2414412f8f2fb4f48981c954",
      "693d69251d3d48219c084af17b54b851",
      "ff36d28c55dd4db3a0f76a87640fdfe2",
      "71c49ef820494d5f8908a3daf39f0755",
      "525dc406534f4369b11208816f8fd0d7",
      "865f39213a7341b68f2fe73caaf801b1",
      "eaf4c0231b6d4993b2f8e9e63d8b6921",
      "a11edf3b018e42c88a63a515cf7fe478"
     ]
    },
    "id": "nbvAV7vaz6yc",
    "outputId": "724f5508-d976-4e31-b3d7-95fa65b2c1e8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2448a60f5f4ba5b2c686037c8ecd78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "consolidated.00.pth:   0%|          | 0.00/13.5G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " What do llamas eat?\n",
      "Llamas and alpacas are herbivores, which means they eat grasses, leaves, grass\n"
     ]
    }
   ],
   "source": [
    "# 删除模型以释放内存\n",
    "del model  # to free up memory\n",
    "\n",
    "# 从Hugging Face Hub下载Llama-2-7b-chat模型权重文件\n",
    "weights_file = hf_hub_download(\n",
    "   repo_id=\"meta-llama/Llama-2-7b-chat\",\n",
    "   filename=\"consolidated.00.pth\", \n",
    "   local_dir=\"Llama-2-7b-chat\"\n",
    ")\n",
    "\n",
    "# 创建新的Llama2模型实例\n",
    "model = Llama2Model(LLAMA2_CONFIG_7B)\n",
    "# 将权重加载到模型中\n",
    "load_weights_into_llama(model, LLAMA2_CONFIG_7B, weights)\n",
    "# 将模型移动到指定设备\n",
    "model.to(device);\n",
    "\n",
    "# 设置随机种子以确保可重复性\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# 使用模型生成文本\n",
    "token_ids = generate(\n",
    "    model=model,  # 使用加载的模型\n",
    "    idx=text_to_token_ids(\"What do llamas eat?\", tokenizer).to(device),  # 将输入问题转换为token并移至设备\n",
    "    max_new_tokens=25,  # 最多生成25个新token\n",
    "    context_size=LLAMA2_CONFIG_7B[\"context_length\"],  # 使用配置中定义的上下文长度\n",
    "    top_k=1,  # 只选择概率最高的token\n",
    "    temperature=0.  # 温度为0,使输出更确定性\n",
    ")\n",
    "\n",
    "# 将生成的token转换回文本并打印输出\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f693da1-a07c-4e1d-af5a-c3923525f1e2",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "# What's next?\n",
    "# 接下来是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae93739-ca12-46ba-8ca7-7c07c59f669b",
   "metadata": {},
   "source": [
    "- This notebook converted the original GPT-2 architecture into a Llama 2 model\n",
    "- 本笔记本将原始的GPT-2架构转换为Llama 2模型\n",
    "- If you are interested in how to convert Llama 2 into Llama 3, Llama 3.1, and Llama 3.2, check out the [converting-llama2-to-llama3.ipynb](converting-llama2-to-llama3.ipynb) notebook\n",
    "- 如果你对如何将Llama 2转换为Llama 3、Llama 3.1和Llama 3.2感兴趣，请查看[converting-llama2-to-llama3.ipynb](converting-llama2-to-llama3.ipynb)笔记本"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0723f467d37b4904819a8bb33ebda10f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08f0bf9459bd425498a5cb236f9d4a72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6b1821a7f4574e3aba09c1e410cc81e4",
      "placeholder": "​",
      "style": "IPY_MODEL_8c2873eaec3445888ad3d54ad7387950",
      "value": "tokenizer.model: 100%"
     }
    },
    "0a939565b6e94f08bee0a66e0f9827d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0c186f6539714d8eab023969ce47c500": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0c8f7044966e4207b12352503c67dcbb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "10251d6f724e43788c41d4b7879cbfd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c8f7044966e4207b12352503c67dcbb",
      "max": 499723,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8b5951213c9e4798a258146d61d02d11",
      "value": 499723
     }
    },
    "2c05df3f91e64df7b33905b1065a76f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b2448a60f5f4ba5b2c686037c8ecd78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_60c5932944f24f5fad1d8da89c8e5ae9",
       "IPY_MODEL_aa31aed1b8854a4281fd7e81c60e1205",
       "IPY_MODEL_d4acf06c2414412f8f2fb4f48981c954"
      ],
      "layout": "IPY_MODEL_693d69251d3d48219c084af17b54b851"
     }
    },
    "525dc406534f4369b11208816f8fd0d7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53a973c0853b44418698136bd04df039": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c05df3f91e64df7b33905b1065a76f7",
      "placeholder": "​",
      "style": "IPY_MODEL_742ae5487f2648fcae7ca8e22c7f8db9",
      "value": " 500k/500k [00:00&lt;00:00, 3.39MB/s]"
     }
    },
    "60c5932944f24f5fad1d8da89c8e5ae9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff36d28c55dd4db3a0f76a87640fdfe2",
      "placeholder": "​",
      "style": "IPY_MODEL_71c49ef820494d5f8908a3daf39f0755",
      "value": "consolidated.00.pth: 100%"
     }
    },
    "66e777955e8748df878f118f07f38dab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_da89ae3ea4d2474e98f64ada608f3cea",
       "IPY_MODEL_93e6da39c25f4edfaa72056c89df1f7f",
       "IPY_MODEL_b628603e4cb0405398c916587ee96756"
      ],
      "layout": "IPY_MODEL_93bedcb9245e44a0a1eb7e4155070f66"
     }
    },
    "693d69251d3d48219c084af17b54b851": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b1821a7f4574e3aba09c1e410cc81e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "71c49ef820494d5f8908a3daf39f0755": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "742ae5487f2648fcae7ca8e22c7f8db9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "865f39213a7341b68f2fe73caaf801b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8b5951213c9e4798a258146d61d02d11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8c2873eaec3445888ad3d54ad7387950": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93bedcb9245e44a0a1eb7e4155070f66": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93e6da39c25f4edfaa72056c89df1f7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8e0f42068af4cb094e2f115f76e06e0",
      "max": 13476925163,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0a939565b6e94f08bee0a66e0f9827d4",
      "value": 13476925163
     }
    },
    "a11edf3b018e42c88a63a515cf7fe478": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a5fedbb7ec2e43d99711bb4cd84b9486": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa31aed1b8854a4281fd7e81c60e1205": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_525dc406534f4369b11208816f8fd0d7",
      "max": 13476925163,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_865f39213a7341b68f2fe73caaf801b1",
      "value": 13476925163
     }
    },
    "b628603e4cb0405398c916587ee96756": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5fedbb7ec2e43d99711bb4cd84b9486",
      "placeholder": "​",
      "style": "IPY_MODEL_0c186f6539714d8eab023969ce47c500",
      "value": " 13.5G/13.5G [01:40&lt;00:00, 111MB/s]"
     }
    },
    "bdb071e7145a4007ae01599333e72612": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4acf06c2414412f8f2fb4f48981c954": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eaf4c0231b6d4993b2f8e9e63d8b6921",
      "placeholder": "​",
      "style": "IPY_MODEL_a11edf3b018e42c88a63a515cf7fe478",
      "value": " 13.5G/13.5G [02:52&lt;00:00, 81.1MB/s]"
     }
    },
    "d8e0f42068af4cb094e2f115f76e06e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da89ae3ea4d2474e98f64ada608f3cea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0723f467d37b4904819a8bb33ebda10f",
      "placeholder": "​",
      "style": "IPY_MODEL_e54928776bc649339002adced63738b0",
      "value": "consolidated.00.pth: 100%"
     }
    },
    "e54928776bc649339002adced63738b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e6c75a6aa7b942fe84160e286e3acb3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_08f0bf9459bd425498a5cb236f9d4a72",
       "IPY_MODEL_10251d6f724e43788c41d4b7879cbfd3",
       "IPY_MODEL_53a973c0853b44418698136bd04df039"
      ],
      "layout": "IPY_MODEL_bdb071e7145a4007ae01599333e72612"
     }
    },
    "eaf4c0231b6d4993b2f8e9e63d8b6921": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff36d28c55dd4db3a0f76a87640fdfe2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
