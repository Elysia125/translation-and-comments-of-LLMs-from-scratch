{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba450fb1-8a26-4894-ab7a-5d7bfefe90ce",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c9672d-8d0c-470d-ac2d-1271f8ec3f14",
   "metadata": {},
   "source": [
    "# Chapter 5 Exercise solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37aa4692-2357-4d88-b072-6d2d988d7f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.26.4\n",
      "tiktoken version: 0.7.0\n",
      "torch version: 2.4.0\n",
      "tensorflow version: 2.16.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"numpy\", \n",
    "        \"tiktoken\", \n",
    "        \"torch\",\n",
    "        \"tensorflow\" # For OpenAI's pretrained weights\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea8be3-30a1-4623-a6d7-b095c6c1092e",
   "metadata": {},
   "source": [
    "# Exercise 5.1: Temperature-scaled softmax scores and sampling probabilities\n",
    "# 练习 5.1: 温度缩放的 softmax 分数和采样概率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5860ba9f-2db3-4480-b96b-4be1c68981eb",
   "metadata": {},
   "source": [
    " - We can print the number of times the word \"pizza\" is sampled using the `print_sampled_tokens` function we defined in this section\n",
    "- 我们可以使用本节中定义的 `print_sampled_tokens` 函数来打印单词\"pizza\"被采样的次数\n",
    " - Let's start with the code we defined in section 5.3.1\n",
    "- 让我们从 5.3.1 节中定义的代码开始\n",
    " \n",
    " - It is sampled 0x if the temperature is 0 or 0.1, and it is sampled 32x if the temperature is scaled up to 5. The estimated probability is 32/1000 * 100% = 3.2%\n",
    "- 当温度为 0 或 0.1 时采样次数为 0 次，当温度增加到 5 时采样次数为 32 次。估计概率为 32/1000 * 100% = 3.2%\n",
    "\n",
    "- The actual probability is 4.3% and contained in the rescaled softmax probability tensor (`scaled_probas[2][6]`)\n",
    "- 实际概率为 4.3%，包含在重新缩放的 softmax 概率张量中 (`scaled_probas[2][6]`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cba59c2-a8a3-4af3-add4-70230795225e",
   "metadata": {},
   "source": [
    "- Below is a self-contained example using code from chapter 5:\n",
    "- 以下是使用第 5 章代码的独立示例:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42dda298-3014-4c36-8d63-97c210bcf4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入PyTorch库\n",
    "import torch\n",
    "\n",
    "# 定义词汇表,将单词映射到索引\n",
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "# 创建反向词汇表,将索引映射回单词\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# 定义下一个token的logits分数\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "# 定义函数来打印采样的token及其频率\n",
    "def print_sampled_tokens(probas):\n",
    "    # 设置随机种子以保证可重复性\n",
    "    torch.manual_seed(123)\n",
    "    # 从概率分布中采样1000次\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    # 统计每个token被采样的次数\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    # 打印每个token的采样频率\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "\n",
    "# 定义带温度参数的softmax函数\n",
    "def softmax_with_temperature(logits, temperature):\n",
    "    # 对logits进行温度缩放\n",
    "    scaled_logits = logits / temperature\n",
    "    # 返回softmax概率\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "\n",
    "# 定义不同的温度值\n",
    "temperatures = [1, 0.1, 5]  # 原始温度、较高温度和较低温度\n",
    "# 计算不同温度下的概率分布\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee0f9f3-4132-42c7-8324-252fd8f59145",
   "metadata": {},
   "source": [
    "- Now, we can iterate over the `scaled_probas` and print the sampling frequencies in each case:\n",
    "- 现在，我们可以遍历 `scaled_probas` 并打印每种情况下的采样频率："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5605236-e300-4844-aea7-509d868efbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Temperature: 1\n",
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n",
      "\n",
      "\n",
      "Temperature: 0.1\n",
      "0 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "985 x forward\n",
      "0 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "15 x toward\n",
      "\n",
      "\n",
      "Temperature: 5\n",
      "165 x closer\n",
      "75 x every\n",
      "42 x effort\n",
      "239 x forward\n",
      "71 x inches\n",
      "46 x moves\n",
      "32 x pizza\n",
      "227 x toward\n",
      "103 x you\n"
     ]
    }
   ],
   "source": [
    "# 遍历不同温度下的概率分布\n",
    "for i, probas in enumerate(scaled_probas):\n",
    "    # 打印当前温度值\n",
    "    print(\"\\n\\nTemperature:\", temperatures[i])\n",
    "    # 打印在当前温度下采样的token及其频率\n",
    "    print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf88c97-19c4-462c-924a-411c8c765d2c",
   "metadata": {},
   "source": [
    "- Note that sampling offers an approximation of the actual probabilities when the word \"pizza\" is sampled\n",
    "- 注意,当采样\"pizza\"这个词时,采样提供了实际概率的近似值\n",
    "- E.g., if it is sampled 32/1000 times, the estimated probability is 3.2%\n",
    "- 例如,如果在1000次采样中出现32次,估计概率为3.2%\n",
    "- To obtain the actual probability, we can check the probabilities directly by accessing the corresponding entry in `scaled_probas`\n",
    "- 要获得实际概率,我们可以通过访问`scaled_probas`中的相应条目直接检查概率\n",
    " \n",
    "- Since \"pizza\" is the 7th entry in the vocabulary, for the temperature of 5, we obtain it as follows:\n",
    "- 由于\"pizza\"是词汇表中的第7个条目,对于温度为5的情况,我们按如下方式获得:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d4163c0-22ad-4f5b-8e20-b7420e9dbfc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0430)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取温度为5的概率分布的索引\n",
    "temp5_idx = 2\n",
    "# 获取\"pizza\"在词汇表中的索引\n",
    "pizza_idx = 6\n",
    "\n",
    "# 获取温度为5时\"pizza\"被采样的概率\n",
    "scaled_probas[temp5_idx][pizza_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dcb438-5f18-4332-9627-66009f30a1a4",
   "metadata": {},
   "source": [
    "There is a 4.3% probability that the word \"pizza\" is sampled if the temperature is set to 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b510ffb0-adca-4d64-8a12-38c4646fd736",
   "metadata": {},
   "source": [
    "# Exercise 5.2: Different temperature and top-k settings\n",
    "# 练习 5.2: 不同的温度和 top-k 设置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884990db-d1a6-4c4e-8e36-2c1e4c1e67c7",
   "metadata": {},
   "source": [
    "- Both temperature and top-k settings have to be adjusted based on the individual LLM (a kind of trial and error process until it generates desirable outputs)\n",
    "- 温度和top-k设置需要根据具体的LLM模型进行调整(这是一个反复试错的过程,直到生成理想的输出)\n",
    "- The desirable outcomes are also application-specific, though\n",
    "- 不过,理想的结果也取决于具体应用\n",
    "  - Lower top-k and temperatures result in less random outcomes, which is desired when creating educational content, technical writing or question answering, data analyses, code generation, and so forth\n",
    "  - 较低的top-k和温度会产生较少的随机性输出,这在创建教育内容、技术写作、问答、数据分析、代码生成等方面是理想的\n",
    "  - Higher top-k and temperatures result in more diverse and random outputs, which is more desirable for brainstorming tasks, creative writing, and so forth\n",
    "  - 较高的top-k和温度会产生更多样化和随机的输出,这在头脑风暴任务、创意写作等方面更为理想"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f35425d-529d-4179-a1c4-63cb8b25b156",
   "metadata": {},
   "source": [
    "# Exercise 5.3: Deterministic behavior in the decoding functions\n",
    "# 练习 5.3: 解码函数中的确定性行为"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12229a2-1d52-46ff-b1e8-198f2e58a7d2",
   "metadata": {},
   "source": [
    "There are multiple ways to force deterministic behavior with the `generate` function:\n",
    "有多种方法可以强制`generate`函数产生确定性行为:\n",
    "\n",
    "1. Setting to `top_k=None` and applying no temperature scaling;\n",
    "1. 设置`top_k=None`且不应用温度缩放;\n",
    "\n",
    "2. Setting `top_k=1`.\n",
    "2. 设置`top_k=1`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391c5dc8-8dd7-4a0a-90bd-519b72f528c7",
   "metadata": {},
   "source": [
    "Below is a self-contained example using code from chapter 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a61a4034-797a-4635-bf42-ddfff1b07125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的库\n",
    "import tiktoken  # 用于分词\n",
    "import torch  # PyTorch深度学习框架\n",
    "from previous_chapters import GPTModel  # 导入之前定义的GPT模型\n",
    "\n",
    "\n",
    "# GPT-124M模型的配置参数\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # 词汇表大小\n",
    "    \"context_length\": 256,       # 上下文长度(原始为1024)\n",
    "    \"emb_dim\": 768,       # 嵌入维度\n",
    "    \"n_heads\": 12,        # 注意力头数量\n",
    "    \"n_layers\": 12,       # 层数\n",
    "    \"drop_rate\": 0.1,     # Dropout比率\n",
    "    \"qkv_bias\": False     # 是否使用Query-Key-Value偏置\n",
    "}\n",
    "\n",
    "\n",
    "# 设置随机种子以确保可重复性\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# 初始化tokenizer和模型\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")  # 使用GPT-2的分词器\n",
    "model = GPTModel(GPT_CONFIG_124M)  # 创建模型实例\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))  # 加载预训练权重\n",
    "model.eval();  # 将模型设置为评估模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee95a272-b852-43b4-9827-ea7e1dbd5724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从gpt_generate模块导入生成文本所需的函数\n",
    "from gpt_generate import generate, text_to_token_ids, token_ids_to_text\n",
    "# 从previous_chapters模块导入简单的文本生成函数\n",
    "from previous_chapters import generate_text_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ab43658-3240-484a-9072-a40a0ed85be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed lun\n"
     ]
    }
   ],
   "source": [
    "# 使用torch.argmax的确定性函数\n",
    "\n",
    "# 设置起始文本\n",
    "start_context = \"Every effort moves you\"\n",
    "\n",
    "# 使用简单的文本生成函数生成文本\n",
    "# model: 预训练的GPT模型\n",
    "# idx: 将起始文本转换为token ID\n",
    "# max_new_tokens: 生成25个新token\n",
    "# context_size: 使用模型配置中定义的上下文长度\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "# 将生成的token ID转换回文本并打印\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebb22d06-393a-42d3-ab64-66646d33b39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed lun\n"
     ]
    }
   ],
   "source": [
    "# 确定性行为：不使用top_k采样，不使用温度缩放\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=None,  # 不限制候选token数量\n",
    "    temperature=0.0  # 温度为0时总是选择概率最高的token\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85b1f11-37a5-477d-9c2d-170a6865e669",
   "metadata": {},
   "source": [
    "- Note that re-executing the previous code cell will produce the exact same generated text:\n",
    "- 注意重新执行前面的代码单元将产生完全相同的生成文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75469f24-47cc-458d-a200-fe64c648131d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed lun\n"
     ]
    }
   ],
   "source": [
    "# 确定性行为：不使用top_k采样，不使用温度缩放\n",
    "# 这段代码将生成完全相同的文本，因为:\n",
    "# - top_k=None: 不限制候选token的数量\n",
    "# - temperature=0.0: 温度为0时总是选择概率最高的token\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=None,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0480e5-fb4e-41f8-a161-7ac980d71d47",
   "metadata": {},
   "source": [
    "# Exercise 5.4: Continued pretraining\n",
    "# 练习 5.4: 继续预训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40044e8-a0f5-476c-99fd-489b999fd80a",
   "metadata": {},
   "source": [
    "- If we are still in the Python session where you first trained the model in chapter 5, to continue the pretraining for one more epoch, we just have to load the model and optimizer that we saved in the main chapter and call the `train_model_simple` function again\n",
    "- 如果我们仍在第5章首次训练模型的Python会话中,要继续预训练一个epoch,我们只需要加载在主章节中保存的模型和优化器,然后再次调用`train_model_simple`函数\n",
    "\n",
    "- It takes a couple more steps to make this reproducible in this new code environment\n",
    "- 在这个新的代码环境中,需要几个额外的步骤来使其可重现\n",
    "\n",
    "- First, we load the tokenizer, model, and optimizer:\n",
    "- 首先,我们加载分词器、模型和优化器:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94eae6ba-d9fd-417a-8e31-fc39e9299870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入tiktoken库用于分词\n",
    "import tiktoken\n",
    "# 导入PyTorch库\n",
    "import torch\n",
    "# 从previous_chapters导入GPTModel类\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "\n",
    "# 定义GPT-124M模型的配置参数\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # 词汇表大小\n",
    "    \"context_length\": 256, # 缩短的上下文长度(原始为1024)\n",
    "    \"emb_dim\": 768,        # 嵌入维度\n",
    "    \"n_heads\": 12,         # 注意力头数量\n",
    "    \"n_layers\": 12,        # 层数\n",
    "    \"drop_rate\": 0.1,      # Dropout比率\n",
    "    \"qkv_bias\": False      # 是否使用Query-Key-Value偏置\n",
    "}\n",
    "\n",
    "# 设置设备为GPU(如果可用)或CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 初始化GPT-2分词器\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# 加载保存的模型检查点\n",
    "checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
    "# 初始化GPT模型\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "# 加载模型状态\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "# 将模型移至指定设备\n",
    "model.to(device)\n",
    "\n",
    "# 初始化AdamW优化器\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "# 加载优化器状态\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "# 将模型设置为训练模式\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688fce4a-9ab2-4d97-a95c-fef02c32b4f3",
   "metadata": {},
   "source": [
    "- Next, we initialize the data loader:\n",
    " - 接下来,我们初始化数据加载器:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5a78470-0652-4abd-875a-664e23c07c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的库\n",
    "import os\n",
    "import urllib.request\n",
    "from previous_chapters import create_dataloader_v1\n",
    "\n",
    "\n",
    "# 定义文件路径和URL\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "# 如果文件不存在则下载\n",
    "if not os.path.exists(file_path):\n",
    "    # 从URL下载文本数据\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    # 将文本数据写入文件\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    # 如果文件存在则直接读取\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "\n",
    "\n",
    "# 设置训练集和验证集的比例\n",
    "train_ratio = 0.90\n",
    "# 计算分割索引\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "# 分割训练集和验证集\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "# 设置随机种子以确保可重复性\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# 创建训练数据加载器\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,                                    # 批次大小\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],    # 最大序列长度\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],        # 步长\n",
    "    drop_last=True,                                  # 丢弃最后不完整的批次\n",
    "    shuffle=True,                                    # 打乱数据\n",
    "    num_workers=0                                    # 数据加载的工作进程数\n",
    ")\n",
    "\n",
    "# 创建验证数据加载器\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,                                    # 批次大小\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],    # 最大序列长度\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],        # 步长\n",
    "    drop_last=False,                                 # 保留最后不完整的批次\n",
    "    shuffle=False,                                   # 不打乱数据\n",
    "    num_workers=0                                    # 数据加载的工作进程数\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76598ef8-165c-4bcc-af5e-b6fe72398365",
   "metadata": {},
   "source": [
    "- Lastly, we use the `train_model_simple` function to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab4693dc-1359-47a7-8110-1e90f514a49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 0.271, Val loss 6.545\n",
      "Ep 1 (Step 000005): Train loss 0.244, Val loss 6.614\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "# 导入训练模型的函数\n",
    "from gpt_train import train_model_simple\n",
    "\n",
    "# 设置训练轮数\n",
    "num_epochs = 1\n",
    "\n",
    "# 训练模型并获取训练损失、验证损失和已处理的token数\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model,                                  # 模型\n",
    "    train_loader,                          # 训练数据加载器\n",
    "    val_loader,                            # 验证数据加载器 \n",
    "    optimizer,                             # 优化器\n",
    "    device,                                # 设备(CPU/GPU)\n",
    "    num_epochs=num_epochs,                 # 训练轮数\n",
    "    eval_freq=5,                           # 评估频率\n",
    "    eval_iter=5,                           # 每次评估的迭代次数\n",
    "    start_context=\"Every effort moves you\", # 生成文本的起始上下文\n",
    "    tokenizer=tokenizer                    # 分词器\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3384e788-f5a1-407c-8dd1-87959b75026d",
   "metadata": {},
   "source": [
    "# Exercise 5.5: Training and validation set losses of the pretrained model\n",
    "# 练习 5.5: 预训练模型的训练集和验证集损失"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb1140b-2027-4156-8d19-600ac849edbe",
   "metadata": {},
   "source": [
    " - We can use the following code to calculate the training and validation set losses of the GPT model:\n",
    "- 我们可以使用以下代码来计算 GPT 模型在训练集和验证集上的损失:\n",
    "\n",
    "```python\n",
    "train_loss = calc_loss_loader(train_loader, gpt, device)\n",
    "val_loss = calc_loss_loader(val_loader, gpt, device)\n",
    "```\n",
    "\n",
    "- The resulting losses for the 124M parameter are as follows:\n",
    "- 124M 参数模型的损失结果如下:\n",
    "\n",
    "```\n",
    "Training loss: 3.754748503367106\n",
    "Validation loss: 3.559617757797241\n",
    "```\n",
    "\n",
    "- The main observation is that the training and validation set performances are in the same ballpark\n",
    "- 主要观察到训练集和验证集的性能在同一水平\n",
    "- This can have multiple explanations:\n",
    "- 这可能有多种解释：\n",
    "\n",
    "1. The Verdict was not part of the pretraining dataset when OpenAI trained GPT-2. Hence, the model is not explicitly overfitting to the training set and performs similarly well on The Verdict's training and validation set portions. (The validation set loss is slightly lower than the training set loss, which is unusual in deep learning. However, it's likely due to random noise since the dataset is relatively small. In practice, if there is no overfitting, the training and validation set performances are expected to be roughly identical).\n",
    "1. 当OpenAI训练GPT-2时，《判决》(The Verdict)不是预训练数据集的一部分。因此，模型并没有明显地过拟合训练集，在《判决》的训练集和验证集部分表现相似。(验证集损失略低于训练集损失，这在深度学习中比较罕见。但是，由于数据集相对较小，这可能是由随机噪声造成的。实际上，如果没有过拟合，训练集和验证集的性能预期应该大致相同)。\n",
    "\n",
    "2. The Verdict was part of GPT -2's training dataset. In this case, we can't tell whether the model is overfitting the training data because the validation set would have been used for training as well. To evaluate the degree of overfitting, we'd need a new dataset generated after OpenAI finished training GPT-2 to make sure that it couldn't have been part of the pretraining.\n",
    "2. 《判决》是GPT-2训练数据集的一部分。在这种情况下，我们无法判断模型是否过拟合训练数据，因为验证集也被用于训练。要评估过拟合程度，我们需要在OpenAI完成GPT-2训练后生成的新数据集，以确保它不可能是预训练的一部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bb4316-a57c-437f-9a01-fe99b1678524",
   "metadata": {},
   "source": [
    "The code below is a reproducible standalone example for this new notebook.\n",
    "以下代码是本新笔记本的可重现独立示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68d162d6-bbb9-4d6d-82ee-1c410694f872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的库\n",
    "import tiktoken\n",
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "\n",
    "# 定义124M参数GPT模型的配置\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # 词汇表大小\n",
    "    \"context_length\": 256, # 缩短的上下文长度(原始:1024)\n",
    "    \"emb_dim\": 768,        # 嵌入维度\n",
    "    \"n_heads\": 12,         # 注意力头数量\n",
    "    \"n_layers\": 12,        # 层数\n",
    "    \"drop_rate\": 0.1,      # Dropout比率\n",
    "    \"qkv_bias\": False      # 查询-键-值偏置\n",
    "}\n",
    "\n",
    "\n",
    "# 设置随机种子以确保可重复性\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# 初始化GPT-2分词器\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8373461-7dad-47da-a489-3e23f0799b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "# 从gpt_download模块导入下载和加载GPT-2模型的函数\n",
    "from gpt_download import download_and_load_gpt2\n",
    "\n",
    "# 下载并加载124M参数的GPT-2模型\n",
    "# model_size: 模型大小(\"124M\")\n",
    "# models_dir: 保存模型的目录(\"gpt2\")\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdd44873-d6c2-4471-a20f-f639b09fdcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在字典中定义模型配置以保持代码简洁\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16}, \n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# 复制基础配置并使用特定模型设置更新\n",
    "model_name = \"gpt2-small (124M)\"  # 示例模型名称\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7d562e4-33f6-4611-9b75-6ad1cb441d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从gpt_generate模块导入加载权重的函数\n",
    "from gpt_generate import load_weights_into_gpt\n",
    "\n",
    "\n",
    "# 设置设备(GPU如果可用,否则使用CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 将预训练权重加载到GPT模型中\n",
    "load_weights_into_gpt(gpt, params)\n",
    "# 将模型移动到指定设备上\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46eda9ea-ccb0-46ee-931b-3c07502b2544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的模块\n",
    "import os\n",
    "import urllib.request\n",
    "from previous_chapters import create_dataloader_v1\n",
    "\n",
    "\n",
    "# 定义文件路径和URL\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "# 如果文件不存在则下载并保存,否则直接读取\n",
    "if not os.path.exists(file_path):\n",
    "    # 从URL下载文本数据\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    # 将文本数据写入文件\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    # 如果文件已存在,直接读取文本数据\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "\n",
    "\n",
    "# 设置训练集和验证集的比例\n",
    "train_ratio = 0.90\n",
    "# 计算分割索引\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "# 分割数据为训练集和验证集\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "# 设置随机种子以确保可重复性\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# 创建训练数据加载器\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,                                    # 训练数据\n",
    "    batch_size=2,                                  # 批次大小\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],  # 最大序列长度\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],      # 步长\n",
    "    drop_last=True,                                # 是否丢弃最后不完整的批次\n",
    "    shuffle=True,                                  # 是否打乱数据\n",
    "    num_workers=0                                  # 数据加载的工作进程数\n",
    ")\n",
    "\n",
    "# 创建验证数据加载器\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,                                      # 验证数据\n",
    "    batch_size=2,                                  # 批次大小\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],  # 最大序列长度\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],      # 步长\n",
    "    drop_last=False,                               # 保留最后不完整的批次\n",
    "    shuffle=False,                                 # 不打乱数据\n",
    "    num_workers=0                                  # 数据加载的工作进程数\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e3574a2-687d-47a2-a2f6-457fe9d595f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.7547486888037787\n",
      "Validation loss: 3.5596182346343994\n"
     ]
    }
   ],
   "source": [
    "# 导入计算损失函数的工具\n",
    "from gpt_train import calc_loss_loader\n",
    "\n",
    "# 设置随机种子以确保可重复性(因为数据加载器中的数据打乱)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# 计算训练集的损失\n",
    "train_loss = calc_loss_loader(train_loader, gpt, device)\n",
    "# 计算验证集的损失\n",
    "val_loss = calc_loss_loader(val_loader, gpt, device)\n",
    "\n",
    "# 打印训练和验证损失\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96485d6b-bf1f-4bc0-a53f-73b08d85726e",
   "metadata": {},
   "source": [
    " We can also repeat this for the largest GPT-2 model, but don't forget to update the context length:\n",
    "我们也可以对最大的 GPT-2 模型重复这个过程，但不要忘记更新上下文长度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a79a4b6-fe8f-40c2-a018-e731dcf391b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|███████████████████████████| 77.0/77.0 [00:00<00:00, 43.5kiB/s]\n",
      "encoder.json: 100%|███████████████████████| 1.04M/1.04M [00:00<00:00, 2.75MiB/s]\n",
      "hparams.json: 100%|█████████████████████████| 91.0/91.0 [00:00<00:00, 60.2kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|█████| 6.23G/6.23G [06:02<00:00, 17.2MiB/s]\n",
      "model.ckpt.index: 100%|████████████████████| 20.7k/20.7k [00:00<00:00, 171kiB/s]\n",
      "model.ckpt.meta: 100%|████████████████████| 1.84M/1.84M [00:00<00:00, 4.27MiB/s]\n",
      "vocab.bpe: 100%|████████████████████████████| 456k/456k [00:00<00:00, 1.73MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.3046312861972384\n",
      "Validation loss: 3.1195147037506104\n"
     ]
    }
   ],
   "source": [
    "# 下载并加载1558M参数的GPT-2模型\n",
    "settings, params = download_and_load_gpt2(model_size=\"1558M\", models_dir=\"gpt2\")\n",
    "\n",
    "# 设置模型名称为GPT2-XL\n",
    "model_name = \"gpt2-xl (1558M)\"\n",
    "# 复制基础配置\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "# 更新模型特定配置\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "# 更新上下文长度和qkv偏置设置\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "# 使用新配置初始化GPT模型\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "# 将模型设置为评估模式\n",
    "gpt.eval()\n",
    "\n",
    "# 将预训练权重加载到模型中\n",
    "load_weights_into_gpt(gpt, params)\n",
    "# 将模型移至指定设备\n",
    "gpt.to(device)\n",
    "\n",
    "# 设置随机种子以确保结果可重复\n",
    "torch.manual_seed(123)\n",
    "# 计算训练集损失\n",
    "train_loss = calc_loss_loader(train_loader, gpt, device)\n",
    "# 计算验证集损失\n",
    "val_loss = calc_loss_loader(val_loader, gpt, device)\n",
    "\n",
    "# 打印训练损失和验证损失\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a76a1e0-9635-480a-9391-3bda7aea402d",
   "metadata": {},
   "source": [
    "# Exercise 5.6: Trying larger models\n",
    "# 练习 5.6: 尝试更大的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d313f4-0038-4bc9-a340-84b3b55dc0e3",
   "metadata": {},
   "source": [
    "- In the main chapter, we experimented with the smallest GPT-2 model, which has only 124M parameters\n",
    "- 在主章节中,我们实验了最小的GPT-2模型,它只有1.24亿个参数\n",
    "- The reason was to keep the resource requirements as low as possible\n",
    "- 这样做的原因是为了尽可能降低资源需求\n",
    "- However, you can easily experiment with larger models with minimal code changes\n",
    "- 然而,你可以通过最少的代码更改轻松地尝试更大的模型\n",
    "- For example, instead of loading the 1558M instead of 124M model in chapter 5, the only 2 lines of code that we have to change are\n",
    "- 例如,在第5章中加载1558M而不是124M的模型时,我们只需要更改以下2行代码\n",
    "\n",
    "```python\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\n",
    "model_name = \"gpt2-small (124M)\"\n",
    "```\n",
    "\n",
    "- The updated code becomes\n",
    "- 更新后的代码\n",
    "\n",
    "```python\n",
    "settings, params = download_and_load_gpt2(model_size=\"1558M\", models_dir=\"gpt2\")\n",
    "model_name = \"gpt2-xl (1558M)\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31e0972b-e85e-4904-a0f5-24c3eacd5fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的库\n",
    "import tiktoken\n",
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "\n",
    "# 定义GPT-2小型模型(124M参数)的配置\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # 词汇表大小\n",
    "    \"context_length\": 256, # 缩短的上下文长度(原始:1024)\n",
    "    \"emb_dim\": 768,        # 嵌入维度\n",
    "    \"n_heads\": 12,         # 注意力头数量\n",
    "    \"n_layers\": 12,        # 层数\n",
    "    \"drop_rate\": 0.1,      # Dropout比率\n",
    "    \"qkv_bias\": False      # 是否使用Query-Key-Value偏置\n",
    "}\n",
    "\n",
    "\n",
    "# 初始化GPT-2分词器\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b641ee88-f9d4-43ec-a787-e34199eed356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/1558M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/1558M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/1558M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/1558M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/1558M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/1558M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/1558M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "# 导入GPT-2模型下载和权重加载相关函数\n",
    "from gpt_download import download_and_load_gpt2\n",
    "from gpt_generate import load_weights_into_gpt\n",
    "\n",
    "\n",
    "# 定义不同规模GPT-2模型的配置参数\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},    # 小型模型配置\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},  # 中型模型配置\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},   # 大型模型配置\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},     # 超大型模型配置\n",
    "}\n",
    "\n",
    "# 选择使用GPT-2超大型模型\n",
    "model_name = \"gpt2-xl (1558M)\"\n",
    "# 复制基础配置并更新为选定模型的配置\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "# 更新上下文长度和QKV偏置设置\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "# 使用更新后的配置初始化GPT模型\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "# 将模型设置为评估模式\n",
    "gpt.eval()\n",
    "\n",
    "# 下载并加载GPT-2超大型模型的权重\n",
    "settings, params = download_and_load_gpt2(model_size=\"1558M\", models_dir=\"gpt2\")\n",
    "# 将预训练权重加载到模型中\n",
    "load_weights_into_gpt(gpt, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c98f56f4-98fc-43b4-9ee5-726e9d17c73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从gpt_generate模块导入文本生成相关函数:\n",
    "# generate: 用于生成文本的主函数\n",
    "# text_to_token_ids: 将文本转换为token ID序列\n",
    "# token_ids_to_text: 将token ID序列转换回文本\n",
    "from gpt_generate import generate, text_to_token_ids, token_ids_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1f7853c-6e81-4f1f-a1d0-61e2c7d33a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you toward finding an ideal life. You don't have to accept your current one at once, because if you do you'll never\n"
     ]
    }
   ],
   "source": [
    "# 设置随机种子以确保结果可重现\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# 使用GPT模型生成文本\n",
    "# model: 使用已加载的GPT模型\n",
    "# idx: 将输入提示文本转换为token ID序列\n",
    "# max_new_tokens: 生成25个新token\n",
    "# context_size: 使用模型配置中定义的上下文长度\n",
    "# top_k: 仅考虑概率最高的50个token\n",
    "# temperature: 温度参数1.5,增加采样随机性\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "# 将生成的token ID序列转换回可读文本并打印\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
