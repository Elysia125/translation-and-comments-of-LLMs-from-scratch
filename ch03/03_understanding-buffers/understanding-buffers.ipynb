{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dlv8N4uWtXcN"
      },
      "source": [
        "<table style=\"width:100%\">\n",
        "<tr>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<font size=\"2\">\n",
        "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
        "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
        "</font>\n",
        "</td>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6BXGeEJ_s-8"
      },
      "source": [
        "# Understanding PyTorch Buffers\n",
        "# 理解 PyTorch 缓冲区"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQt9Ob1Y_8EH"
      },
      "source": [
        "In essence, PyTorch buffers are tensor attributes associated with a PyTorch module or model similar to parameters, but unlike parameters, buffers are not updated during training.\n",
        "本质上，PyTorch 缓冲区是与 PyTorch 模块或模型相关联的张量属性，类似于参数，但与参数不同的是，缓冲区在训练期间不会更新。\n",
        "\n",
        "Buffers in PyTorch are particularly useful when dealing with GPU computations, as they need to be transferred between devices (like from CPU to GPU) alongside the model's parameters. Unlike parameters, buffers do not require gradient computation, but they still need to be on the correct device to ensure that all computations are performed correctly.\n",
        "PyTorch 中的缓冲区在处理 GPU 计算时特别有用，因为它们需要与模型参数一起在设备之间传输（如从 CPU 传输到 GPU）。与参数不同，缓冲区不需要梯度计算，但它们仍需要在正确的设备上以确保所有计算正确执行。\n",
        "\n",
        "In chapter 3, we use PyTorch buffers via `self.register_buffer`, which is only briefly explained in the book. Since the concept and purpose are not immediately clear, this code notebook offers a longer explanation with a hands-on example.\n",
        "在第 3 章中，我们通过 `self.register_buffer` 使用 PyTorch 缓冲区，这在书中只是简单解释。由于概念和目的并不是很清晰，这个代码笔记本提供了更详细的解释和实践示例。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAwGo_gYLY45"
      },
      "source": [
        "## An example without buffers\n",
        "## 不使用缓冲区的示例"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qBQC9IPAJVZ"
      },
      "source": [
        "Suppose we have the following code, which is based on code from chapter 3. This version has been modified to exclude buffers. It implements the causal self-attention mechanism used in LLMs:\n",
        "假设我们有以下代码，它基于第3章的代码。这个版本已经修改为不包含缓冲区。它实现了LLM中使用的因果自注意力机制："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7wx-_rokAN04"
      },
      "outputs": [],
      "source": [
        "# 导入PyTorch和神经网络模块\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 定义一个不使用缓冲区的因果注意力模块类\n",
        "class CausalAttentionWithoutBuffers(nn.Module):\n",
        "\n",
        "    # 初始化函数,接收输入维度、输出维度、上下文长度、dropout率和是否使用偏置等参数\n",
        "    def __init__(self, d_in, d_out, context_length,\n",
        "                 dropout, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        # 保存输出维度\n",
        "        self.d_out = d_out\n",
        "        # 创建查询、键、值的线性变换层\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        # 创建dropout层\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # 创建上三角掩码矩阵,用于实现因果注意力\n",
        "        self.mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "\n",
        "    # 前向传播函数\n",
        "    def forward(self, x):\n",
        "        # 获取输入张量的形状:批次大小、token数量和输入维度\n",
        "        b, num_tokens, d_in = x.shape\n",
        "        # 计算键、查询和值向量\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # 计算注意力分数\n",
        "        attn_scores = queries @ keys.transpose(1, 2)\n",
        "        # 使用掩码将未来token的注意力分数设为负无穷\n",
        "        attn_scores.masked_fill_(\n",
        "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
        "        # 计算注意力权重(使用缩放点积注意力)\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "        )\n",
        "        # 应用dropout\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # 计算并返回上下文向量\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNrK-wLaNSi7"
      },
      "source": [
        "We can initialize and run the module as follows on some example data:\n",
        "我们可以按如下方式初始化并运行该模块的示例数据:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1MZiIsPA0Py",
        "outputId": "ce1407c6-c082-4755-b8ad-d9adcc9f153a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[-0.4519,  0.2216],\n",
            "         [-0.5874,  0.0058],\n",
            "         [-0.6300, -0.0632],\n",
            "         [-0.5675, -0.0843],\n",
            "         [-0.5526, -0.0981],\n",
            "         [-0.5299, -0.1081]],\n",
            "\n",
            "        [[-0.4519,  0.2216],\n",
            "         [-0.5874,  0.0058],\n",
            "         [-0.6300, -0.0632],\n",
            "         [-0.5675, -0.0843],\n",
            "         [-0.5526, -0.0981],\n",
            "         [-0.5299, -0.1081]]])\n"
          ]
        }
      ],
      "source": [
        "# 设置随机种子以确保结果可重现\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# 创建输入张量,每行代表一个词向量\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")\n",
        "\n",
        "# 创建批次数据,将inputs复制两份并堆叠\n",
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "# 获取上下文长度(序列长度)\n",
        "context_length = batch.shape[1]\n",
        "# 获取输入维度\n",
        "d_in = inputs.shape[1]\n",
        "# 设置输出维度\n",
        "d_out = 2\n",
        "\n",
        "# 初始化因果注意力模块(不带缓冲区版本)\n",
        "ca_without_buffer = CausalAttentionWithoutBuffers(d_in, d_out, context_length, 0.0)\n",
        "\n",
        "# 在无梯度计算模式下运行前向传播\n",
        "with torch.no_grad():\n",
        "    context_vecs = ca_without_buffer(batch)\n",
        "\n",
        "# 打印输出的上下文向量\n",
        "print(context_vecs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_hqz6AgCCc1"
      },
      "source": [
        "So far, everything has worked fine so far.\n",
        "到目前为止,一切运行正常。\n",
        "\n",
        "However, when training LLMs, we typically use GPUs to accelerate the process. Therefore, let's transfer the `CausalAttentionWithoutBuffers` module onto a GPU device.\n",
        "然而,在训练大型语言模型时,我们通常使用GPU来加速处理过程。因此,让我们将`CausalAttentionWithoutBuffers`模块转移到GPU设备上。\n",
        "\n",
        "Please note that this operation requires the code to be run in an environment equipped with GPUs.\n",
        "请注意,此操作需要在配备GPU的环境中运行代码。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYwn44HWCPJS",
        "outputId": "d7236e0c-2a43-4770-ccc1-03c9d5d11421"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Machine has GPU: True\n"
          ]
        }
      ],
      "source": [
        "# 检查机器是否有可用的GPU\n",
        "print(\"Machine has GPU:\", torch.cuda.is_available())\n",
        "\n",
        "# 将输入数据batch移动到GPU上\n",
        "batch = batch.to(\"cuda\")\n",
        "\n",
        "# 将模型移动到GPU上\n",
        "ca_without_buffer.to(\"cuda\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_lMki2_CoIR"
      },
      "source": [
        "Now, let's run the code again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "KE9iLcjGC1V1",
        "outputId": "ab6921c7-d7dd-44ea-9b92-1911037e3dcc"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "expected self and mask to be on the same device, but got mask on cpu and self on cuda:0",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1e0d2e6638f6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mcontext_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mca_without_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-cf1dad0dd611>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mattn_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueries\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         attn_scores.masked_fill_(\n\u001b[0m\u001b[1;32m     24\u001b[0m             self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n\u001b[1;32m     25\u001b[0m         attn_weights = torch.softmax(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: expected self and mask to be on the same device, but got mask on cpu and self on cuda:0"
          ]
        }
      ],
      "source": [
        "# 在无梯度计算模式下运行前向传播\n",
        "with torch.no_grad():\n",
        "    context_vecs = ca_without_buffer(batch)\n",
        "\n",
        "# 打印输出的上下文向量\n",
        "print(context_vecs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7V26PLrC2gk"
      },
      "source": [
        "Running the code resulted in an error. What happened? It seems like we attempted a matrix multiplication between a tensor on a GPU and a tensor on a CPU. But we moved the module to the GPU!?\n",
        "运行代码时出现了错误。发生了什么?看起来我们试图在GPU上的张量和CPU上的张量之间进行矩阵乘法。但是我们不是已经将模块移动到GPU上了吗!?\n",
        "\n",
        "Let's double-check the device locations of some of the tensors:\n",
        "让我们再次检查一下一些张量所在的设备位置:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvYDPBRIDHfU",
        "outputId": "4b9703a8-7035-4a2d-8643-c64d37b7abd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W_query.device: cuda:0\n",
            "mask.device: cpu\n"
          ]
        }
      ],
      "source": [
        "# 打印W_query权重张量所在的设备\n",
        "print(\"W_query.device:\", ca_without_buffer.W_query.weight.device)\n",
        "# 打印mask张量所在的设备 \n",
        "print(\"mask.device:\", ca_without_buffer.mask.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d11nX-FFOJ3C",
        "outputId": "1e92b0e8-dbc6-41f9-e88f-5d06e0726050"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 检查mask属性的类型\n",
        "type(ca_without_buffer.mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ojay-KY-DL5M"
      },
      "source": [
        "As we can see, the `mask` was not moved onto the GPU. That's because it's not a PyTorch parameter like the weights (e.g., `W_query.weight`).\n",
        "正如我们所看到的，`mask`没有被移动到GPU上。这是因为它不像权重(例如`W_query.weight`)那样是PyTorch参数。\n",
        "\n",
        "This means we  have to manually move it to the GPU via `.to(\"cuda\")`:\n",
        "这意味着我们必须通过`.to(\"cuda\")`手动将其移动到GPU上:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYirQ63zDYsW",
        "outputId": "304628ac-bc4c-49c2-a0e1-ecf9385ddcd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mask.device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# 将mask张量移动到GPU设备上\n",
        "ca_without_buffer.mask = ca_without_buffer.mask.to(\"cuda\")\n",
        "# 打印移动后mask张量所在的设备\n",
        "print(\"mask.device:\", ca_without_buffer.mask.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OoTqzkpDfAm"
      },
      "source": [
        "Let's try our code again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfF0yBZODdAZ",
        "outputId": "291cfb54-86e6-45f9-99d1-fa145319f379"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[-0.4519,  0.2216],\n",
            "         [-0.5874,  0.0058],\n",
            "         [-0.6300, -0.0632],\n",
            "         [-0.5675, -0.0843],\n",
            "         [-0.5526, -0.0981],\n",
            "         [-0.5299, -0.1081]],\n",
            "\n",
            "        [[-0.4519,  0.2216],\n",
            "         [-0.5874,  0.0058],\n",
            "         [-0.6300, -0.0632],\n",
            "         [-0.5675, -0.0843],\n",
            "         [-0.5526, -0.0981],\n",
            "         [-0.5299, -0.1081]]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# 禁用梯度计算\n",
        "with torch.no_grad():\n",
        "    # 通过ca_without_buffer模型处理batch数据\n",
        "    context_vecs = ca_without_buffer(batch)\n",
        "\n",
        "# 打印输出上下文向量\n",
        "print(context_vecs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUrVgWuuD7UE"
      },
      "source": [
        "This time, it worked!\n",
        "这次成功了！\n",
        "\n",
        "However, remembering to move individual tensors to the GPU can be tedious. As we will see in the next section, it's easier to use `register_buffer` to register the `mask` as a buffer.\n",
        "然而，记住要将单个张量移动到GPU上可能很繁琐。正如我们将在下一节看到的，使用`register_buffer`来注册`mask`作为缓冲区会更容易。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StS2wUrBLeuW"
      },
      "source": [
        "## An example with buffers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEqD2NFzPO6l"
      },
      "source": [
        "Let's now modify the causal attention class to register the causal `mask` as a buffer:\n",
        "现在让我们修改因果注意力类来将因果`mask`注册为缓冲区:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ndsYj3Zf6N8U"
      },
      "outputs": [],
      "source": [
        "# 导入PyTorch相关包\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 定义带有缓冲区的因果注意力类,继承自nn.Module\n",
        "class CausalAttentionWithBuffer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, context_length,\n",
        "                 dropout, qkv_bias=False):\n",
        "        # 调用父类初始化\n",
        "        super().__init__()\n",
        "        # 保存输出维度\n",
        "        self.d_out = d_out\n",
        "        # 初始化查询(query)权重矩阵\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        # 初始化键(key)权重矩阵\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        # 初始化值(value)权重矩阵\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        # 初始化dropout层\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Old:\n",
        "        # self.mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "\n",
        "        # New:\n",
        "        # 注册上三角掩码矩阵作为缓冲区\n",
        "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 获取输入张量的形状:批次大小、序列长度、输入维度\n",
        "        b, num_tokens, d_in = x.shape\n",
        "        # 计算键向量\n",
        "        keys = self.W_key(x)\n",
        "        # 计算查询向量\n",
        "        queries = self.W_query(x)\n",
        "        # 计算值向量\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # 计算注意力分数:查询和键的矩阵乘法\n",
        "        attn_scores = queries @ keys.transpose(1, 2)\n",
        "        # 使用掩码将未来位置的注意力分数设为负无穷\n",
        "        attn_scores.masked_fill_(\n",
        "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
        "        # 计算注意力权重:使用softmax并进行缩放\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "        )\n",
        "        # 对注意力权重应用dropout\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # 计算上下文向量:注意力权重和值的矩阵乘法\n",
        "        context_vec = attn_weights @ values\n",
        "        # 返回上下文向量\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AL1X6y3Eb7S"
      },
      "source": [
        "现在,很方便的是,如果我们将模块移动到GPU上,掩码也会被一同移动到GPU上:\n",
        "Now, conveniently, if we move the module to the GPU, the mask will be located on the GPU as well:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_VCxEa76j00",
        "outputId": "4d1af501-5a9e-46aa-b1ac-63bf0c68e02a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W_query.device: cuda:0\n",
            "mask.device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# 创建一个带缓冲区的因果注意力模型实例\n",
        "ca_with_buffer = CausalAttentionWithBuffer(d_in, d_out, context_length, 0.0)\n",
        "# 将模型移动到CUDA设备(GPU)上\n",
        "ca_with_buffer.to(\"cuda\")\n",
        "\n",
        "# 打印查询权重矩阵和掩码的设备位置\n",
        "print(\"W_query.device:\", ca_with_buffer.W_query.weight.device)\n",
        "print(\"mask.device:\", ca_with_buffer.mask.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBWvKlMe7bbB",
        "outputId": "e43bf8ab-3fb9-417e-d087-560858332d86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[0.4772, 0.1063],\n",
            "         [0.5891, 0.3257],\n",
            "         [0.6202, 0.3860],\n",
            "         [0.5478, 0.3589],\n",
            "         [0.5321, 0.3428],\n",
            "         [0.5077, 0.3493]],\n",
            "\n",
            "        [[0.4772, 0.1063],\n",
            "         [0.5891, 0.3257],\n",
            "         [0.6202, 0.3860],\n",
            "         [0.5478, 0.3589],\n",
            "         [0.5321, 0.3428],\n",
            "         [0.5077, 0.3493]]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# 使用torch.no_grad()上下文管理器禁用梯度计算\n",
        "with torch.no_grad():\n",
        "    # 使用带缓冲区的因果注意力模型处理输入批次\n",
        "    context_vecs = ca_with_buffer(batch)\n",
        "\n",
        "# 打印输出上下文向量\n",
        "print(context_vecs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvOTh4NNPjef"
      },
      "source": [
        "As we can see above, registering a tensor as a buffer can make our lives a lot easier: We don't have to remember to move tensors to a target device like a GPU manually.\n",
        "正如我们在上面看到的,将张量注册为缓冲区可以让我们的工作变得更加轻松:我们不需要手动将张量移动到GPU等目标设备上。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-5YYKmJte3h"
      },
      "source": [
        "## Buffers and `state_dict`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIHHawPbtjfp"
      },
      "source": [
        "- Another advantage of PyTorch buffers, over regular tensors, is that they get included in a model's `state_dict`\n",
        "- PyTorch缓冲区相对于普通张量的另一个优势是它们会被包含在模型的`state_dict`中\n",
        "- For example, consider the `state_dict` of the causal attention object without buffers  \n",
        "- 例如,让我们看看不带缓冲区的因果注意力对象的`state_dict`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c217juzqtxsS",
        "outputId": "dbae3c3d-f4f8-4c70-a64f-90906561d8d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('W_query.weight',\n",
              "              tensor([[-0.2354,  0.0191, -0.2867],\n",
              "                      [ 0.2177, -0.4919,  0.4232]], device='cuda:0')),\n",
              "             ('W_key.weight',\n",
              "              tensor([[-0.4196, -0.4590, -0.3648],\n",
              "                      [ 0.2615, -0.2133,  0.2161]], device='cuda:0')),\n",
              "             ('W_value.weight',\n",
              "              tensor([[-0.4900, -0.3503, -0.2120],\n",
              "                      [-0.1135, -0.4404,  0.3780]], device='cuda:0'))])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 打印不带缓冲区的因果注意力模型的状态字典\n",
        "# 这将只包含模型的权重参数,不包含掩码\n",
        "ca_without_buffer.state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdmZuPaqt6aO"
      },
      "source": [
        "- The mask is not included in the `state_dict` above\n",
        "- 上面的`state_dict`中不包含掩码\n",
        "- However, the mask *is* included in the `state_dict` below, thanks to registering it as a buffer  \n",
        "- 然而,由于将掩码注册为缓冲区,下面的`state_dict`中包含了掩码"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGIGQAwPt1Pl",
        "outputId": "00f9bc44-63f9-4ebc-87ea-d4b8cafd81c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('mask',\n",
              "              tensor([[0., 1., 1., 1., 1., 1.],\n",
              "                      [0., 0., 1., 1., 1., 1.],\n",
              "                      [0., 0., 0., 1., 1., 1.],\n",
              "                      [0., 0., 0., 0., 1., 1.],\n",
              "                      [0., 0., 0., 0., 0., 1.],\n",
              "                      [0., 0., 0., 0., 0., 0.]], device='cuda:0')),\n",
              "             ('W_query.weight',\n",
              "              tensor([[-0.1362,  0.1853,  0.4083],\n",
              "                      [ 0.1076,  0.1579,  0.5573]], device='cuda:0')),\n",
              "             ('W_key.weight',\n",
              "              tensor([[-0.2604,  0.1829, -0.2569],\n",
              "                      [ 0.4126,  0.4611, -0.5323]], device='cuda:0')),\n",
              "             ('W_value.weight',\n",
              "              tensor([[ 0.4929,  0.2757,  0.2516],\n",
              "                      [ 0.2377,  0.4800, -0.0762]], device='cuda:0'))])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 打印带缓冲区的因果注意力模型的状态字典\n",
        "# 这将包含模型的权重参数和掩码缓冲区\n",
        "ca_with_buffer.state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACC-a1Hnt4Zv"
      },
      "source": [
        "- A `state_dict` is useful when saving and loading trained PyTorch models, for example\n",
        "- `state_dict`在保存和加载训练好的PyTorch模型时很有用\n",
        "- In this particular case, saving and loading the `mask` is maybe not super useful, because it remains unchanged during training; so, for demonstration purposes, let's assume it was modified where all `1`'s were changed to `2`'s:\n",
        "- 在这个特定的例子中,保存和加载`mask`可能不是特别有用,因为它在训练过程中保持不变;所以为了演示目的,让我们假设将所有的`1`都修改为`2`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLm1Sw0cuhvy",
        "outputId": "4b2cc70f-1709-44e4-aa17-4e01353b86f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., 2., 2., 2., 2., 2.],\n",
              "        [0., 0., 2., 2., 2., 2.],\n",
              "        [0., 0., 0., 2., 2., 2.],\n",
              "        [0., 0., 0., 0., 2., 2.],\n",
              "        [0., 0., 0., 0., 0., 2.],\n",
              "        [0., 0., 0., 0., 0., 0.]], device='cuda:0')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 将带缓冲区的因果注意力模型中掩码值为1的位置修改为2\n",
        "ca_with_buffer.mask[ca_with_buffer.mask == 1.] = 2.\n",
        "# 打印修改后的掩码\n",
        "ca_with_buffer.mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIkGgGqqvp4S"
      },
      "source": [
        "- Then, if we save and load the model, we can see that the mask is restored with the modified value\n",
        " - 然后,如果我们保存并加载模型,我们可以看到掩码会以修改后的值被恢复"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8g0QHUhuVBw",
        "outputId": "cc7ee348-7f94-4117-e5cc-e0e01a94e906"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., 2., 2., 2., 2., 2.],\n",
              "        [0., 0., 2., 2., 2., 2.],\n",
              "        [0., 0., 0., 2., 2., 2.],\n",
              "        [0., 0., 0., 0., 2., 2.],\n",
              "        [0., 0., 0., 0., 0., 2.],\n",
              "        [0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 保存带缓冲区的因果注意力模型的状态字典到文件\n",
        "torch.save(ca_with_buffer.state_dict(), \"model.pth\")\n",
        "\n",
        "# 创建一个新的带缓冲区的因果注意力模型实例\n",
        "new_ca_with_buffer = CausalAttentionWithBuffer(d_in, d_out, context_length, 0.0)\n",
        "# 从文件加载保存的状态字典到新模型\n",
        "new_ca_with_buffer.load_state_dict(torch.load(\"model.pth\"))\n",
        "\n",
        "# 打印新模型的掩码,验证掩码值是否正确恢复\n",
        "new_ca_with_buffer.mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pPaJk7bvBD7"
      },
      "source": [
        "- This is not true if we don't use buffers:\n",
        "- 如果我们不使用缓冲区,情况就不是这样了:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D03w8vDyvBRS",
        "outputId": "28071601-120c-42da-b327-bb293793839f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., 1., 1., 1., 1., 1.],\n",
              "        [0., 0., 1., 1., 1., 1.],\n",
              "        [0., 0., 0., 1., 1., 1.],\n",
              "        [0., 0., 0., 0., 1., 1.],\n",
              "        [0., 0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 将不带缓冲区的因果注意力模型中掩码值为1的位置修改为2\n",
        "ca_without_buffer.mask[ca_without_buffer.mask == 1.] = 2.\n",
        "\n",
        "# 保存不带缓冲区的因果注意力模型的状态字典到文件\n",
        "torch.save(ca_without_buffer.state_dict(), \"model.pth\")\n",
        "\n",
        "# 创建一个新的不带缓冲区的因果注意力模型实例\n",
        "new_ca_without_buffer = CausalAttentionWithoutBuffers(d_in, d_out, context_length, 0.0)\n",
        "# 从文件加载保存的状态字典到新模型\n",
        "new_ca_without_buffer.load_state_dict(torch.load(\"model.pth\"))\n",
        "\n",
        "# 打印新模型的掩码,验证掩码值是否正确恢复\n",
        "new_ca_without_buffer.mask"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
