{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba450fb1-8a26-4894-ab7a-5d7bfefe90ce",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c9672d-8d0c-470d-ac2d-1271f8ec3f14",
   "metadata": {},
   "source": [
    "# Chapter 7 Exercise solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2625ddc4-9cce-42bd-947d-4e2203fdc55c",
   "metadata": {},
   "source": [
    "## Exercise 7.1: Changing prompt styles\n",
    "## 练习 7.1: 更改提示样式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be25a95-2a33-433b-a698-2365b5fc9357",
   "metadata": {},
   "source": [
    "Suppose we have the following data entry:\n",
    "\n",
    "假设我们有以下数据条目:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"instruction\": \"Identify the correct spelling of the following word.\",\n",
    "  \"input\": \"Ocassion\",\n",
    "  \"output\": \"The correct spelling is 'Occasion.'\"\n",
    "}\n",
    "```\n",
    "\n",
    "In the main chapter, we formatted it according to the Alpaca-style prompt template:\n",
    "\n",
    "在主章节中,我们按照 Alpaca 风格的提示模板对其进行了格式化:\n",
    "\n",
    "```\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Identify the correct spelling of the following word.\n",
    "\n",
    "### Input:\n",
    "Occassion\n",
    "\n",
    "### Response:\n",
    "The correct spelling is 'Occasion.'\n",
    "```\n",
    "\n",
    "In this exercise, we now use the Phi-3 prompt template instead, which formats the data entry as follows:\n",
    "在这个练习中,我们将使用 Phi-3 提示模板来替代,它按如下方式格式化数据条目:\n",
    "\n",
    "```\n",
    "<user>\n",
    "Identify the correct spelling of the following word: 'Occasion'\n",
    "\n",
    "<assistant>\n",
    "The correct spelling is 'Occasion'.\n",
    "```\n",
    "\n",
    "Note that this prompt template is substantially shorter, which reduces the runtime and hardware requirements for finetuning the LLM and generating text since the input prompts are shorter.\n",
    "\n",
    "请注意,这个提示模板明显更短,由于输入提示更短,这减少了微调 LLM 和生成文本的运行时间和硬件要求。\n",
    "\n",
    "To make this change, we update the `format_input` function as follows:\n",
    "\n",
    "为了进行这个更改,我们更新 `format_input` 函数如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f99baa1e-c24c-417f-89d0-13e6d061ea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    # 构建用户指令文本,以<|user|>标签开头\n",
    "    instruction_text = (\n",
    "        f\"<|user|>\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    # 如果存在输入文本则添加,否则为空字符串\n",
    "    input_text = f\"\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    # 返回完整的格式化文本\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ba538f-64b9-495d-847b-d9f1d324bc50",
   "metadata": {},
   "source": [
    "Let's make sure that it works as intended by applying it to two input samples, one with and one without content in the `'input'` field:\n",
    "让我们通过应用两个输入样本来确保它按预期工作,一个在 `'input'` 字段中有内容,另一个没有内容:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "877a57e2-535f-4363-b32a-a093edd951b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Identify the correct spelling of the following word.\n",
      "Ocassion\n",
      "\n",
      "<|user|>\n",
      "What is an antonym of 'complicated'?\n"
     ]
    }
   ],
   "source": [
    "# 创建两个示例数据条目\n",
    "sample_data = [\n",
    "    # 第一个示例:拼写检查任务\n",
    "    {'instruction': 'Identify the correct spelling of the following word.', \n",
    "     'input': 'Ocassion', \n",
    "     'output': \"The correct spelling is 'Occasion.'\"}, \n",
    "    \n",
    "    # 第二个示例:反义词任务(无输入文本)\n",
    "    {'instruction': \"What is an antonym of 'complicated'?\", \n",
    "     'input': '', \n",
    "     'output': \"An antonym of 'complicated' is 'simple'.\"}\n",
    "]\n",
    "\n",
    "# 打印第一个示例的格式化输入\n",
    "print(format_input(sample_data[0]))\n",
    "print()\n",
    "# 打印第二个示例的格式化输入\n",
    "print(format_input(sample_data[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2a6704-6c61-4a09-b8f5-ffc5a77d6aa3",
   "metadata": {},
   "source": [
    "Next, we also update the `InstructionDataset` class to use the <|assistant|> prompt template for the response:\n",
    "接下来,我们还要更新 `InstructionDataset` 类以使用 <|assistant|> 提示模板来生成响应:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17f1a42c-7cc0-4746-8a6d-3a4cb37e2ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入tiktoken分词器库\n",
    "import tiktoken\n",
    "# 导入PyTorch数据集基类\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 定义指令数据集类,继承自Dataset\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        # 保存数据\n",
    "        self.data = data\n",
    "\n",
    "        # 预先对文本进行分词\n",
    "        self.encoded_texts = []\n",
    "        # 遍历数据集中的每个条目\n",
    "        for entry in data:\n",
    "\n",
    "            ###################################################################\n",
    "            # 新增: 使用format_input_phi并调整响应文本模板\n",
    "            # 获取格式化后的指令和输入文本\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            # 构建响应文本,添加assistant标签\n",
    "            response_text = f\"\\n<|assistant|>:\\n{entry['output']}\"\n",
    "            ###################################################################\n",
    "            # 将指令输入和响应文本拼接\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            # 对完整文本进行编码并添加到列表中\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 返回指定索引的编码文本\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回数据集长度\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "# 初始化GPT-2分词器\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0650926-c39f-4442-8116-cb7494416f28",
   "metadata": {},
   "source": [
    "最后,我们还需要更新在收集测试集响应时提取生成响应的方式:\n",
    "\n",
    "Lastly, we also have to update the way we extract the generated response when we collect the test set responses:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9253041-812f-4a5f-9ab1-d7e4cb1407fb",
   "metadata": {},
   "source": [
    " ```python\n",
    " # 遍历测试数据集中的每个条目\n",
    " for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    " \n",
    "     # 获取格式化后的输入文本\n",
    "     input_text = format_input(entry)\n",
    "     tokenizer=tokenizer\n",
    " \n",
    "     # 使用模型生成响应\n",
    "     token_ids = generate(\n",
    "         model=model,                                      # 模型\n",
    "         idx=text_to_token_ids(input_text, tokenizer).to(device),  # 输入token ids\n",
    "         max_new_tokens=256,                              # 最大生成token数\n",
    "         context_size=BASE_CONFIG[\"context_length\"],      # 上下文长度\n",
    "         eos_id=50256                                     # 结束符token id\n",
    "     )\n",
    "     # 将生成的token ids转换回文本\n",
    "     generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    " \n",
    "     # 提取响应文本:去除输入文本和assistant标签\n",
    "     response_text = generated_text[len(input_text):].replace(\"<|assistant|>:\", \"\").strip()\n",
    " \n",
    "     # 将生成的响应保存到测试数据中\n",
    "     test_data[i][\"model_response\"] = response_text\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cd557c-3838-45e4-a26a-baed4b11175a",
   "metadata": {},
   "source": [
    "为了方便起见,练习解决方案已在 [exercise_experiments.py](exercise_experiments.py) 脚本中实现,您可以按如下方式运行:\n",
    "For your convenience, the exercise solution is implemented in the [exercise_experiments.py](exercise_experiments.py) script, which you can run as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8158e9-cc70-4e0f-88b0-73c3e1d8c030",
   "metadata": {},
   "source": [
    "```bash\n",
    "python exercise_experiments.py --exercise_solution phi3_prompt\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "matplotlib version: 3.7.1\n",
    "tiktoken version: 0.7.0\n",
    "torch version: 2.3.0+cu121\n",
    "tqdm version: 4.66.4\n",
    "tensorflow version: 2.15.0\n",
    "--------------------------------------------------\n",
    "Training set length: 935\n",
    "Validation set length: 55\n",
    "Test set length: 110\n",
    "--------------------------------------------------\n",
    "Device: cuda\n",
    "--------------------------------------------------\n",
    "...\n",
    "Loaded model: gpt2-medium (355M)\n",
    "--------------------------------------------------\n",
    "Initial losses\n",
    "   Training loss: 3.71630220413208\n",
    "   Validation loss: 3.6440994262695314\n",
    "Ep 1 (Step 000000): Train loss 2.633, Val loss 2.622\n",
    "...\n",
    "Ep 2 (Step 000230): Train loss 0.424, Val loss 0.928\n",
    "<|user|> Convert the active sentence to passive: 'The chef cooks the meal every day.' <|assistant|>: The meal is prepared every day by the chef....\n",
    "Training completed in 1.50 minutes.\n",
    "Plot saved as loss-plot-phi3-prompt.pdf\n",
    "--------------------------------------------------\n",
    "Generating responses\n",
    "100% 110/110 [00:11<00:00,  9.27it/s]\n",
    "Responses saved as instruction-data-with-response-phi3-prompt.json\n",
    "Model saved as gpt2-medium355M-sft-phi3-prompt.pth\n",
    "```\n",
    "\n",
    "\n",
    "For comparison, you can run the original chapter 7 finetuning code via `python exercise_experiments.py --exercise_solution baseline`. \n",
    "\n",
    "作为对比,你可以通过运行 `python exercise_experiments.py --exercise_solution baseline` 来执行原始的第7章微调代码。\n",
    "\n",
    "Note that on an Nvidia L4 GPU, the code above, using the Phi-3 prompt template, takes 1.5 min to run. In comparison, the Alpaca-style template takes 1.80 minutes to run. So, the Phi-3 template is approximately 17% faster since it results in shorter model inputs. \n",
    "\n",
    "注意在Nvidia L4 GPU上,使用Phi-3提示模板的上述代码运行需要1.5分钟。相比之下,Alpaca风格的模板需要1.80分钟运行。由于产生更短的模型输入,Phi-3模板大约快17%。\n",
    "\n",
    "Let's take a look at some of the responses to make sure they have been formatted correctly:\n",
    "\n",
    "让我们看一下一些响应,确保它们已被正确格式化:\n",
    "```json\n",
    "    {\n",
    "        \"instruction\": \"Rewrite the sentence using a simile.\",\n",
    "        \"input\": \"The car is very fast.\",\n",
    "        \"output\": \"The car is as fast as lightning.\",\n",
    "        \"model_response\": \"The car is as fast as a cheetah.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What type of cloud is typically associated with thunderstorms?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"The type of cloud typically associated with thunderstorms is cumulonimbus.\",\n",
    "        \"model_response\": \"The type of cloud associated with thunderstorms is a cumulus cloud.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Name the author of 'Pride and Prejudice'.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Jane Austen.\",\n",
    "        \"model_response\": \"The author of 'Pride and Prejudice' is Jane Austen.\"\n",
    "    },\n",
    "```\n",
    "\n",
    "\n",
    "We can evaluate the performance using the Ollama Llama 3 method, which is for your convenience, also implemented in the `python exercise_experiments.py` script, which we can run as follows:\n",
    "\n",
    "我们可以使用Ollama Llama 3方法来评估性能,为了方便起见,该方法也已在`python exercise_experiments.py`脚本中实现,我们可以按如下方式运行:\n",
    "```bash\n",
    "python ollama_evaluate.py --file_path instruction-data-with-response-phi3-prompt.json\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "Ollama running: True\n",
    "Scoring entries: 100%|████████████████████████| 110/110 [01:08<00:00,  1.60it/s]\n",
    "Number of scores: 110 of 110\n",
    "Average score: 48.87\n",
    "```\n",
    "\n",
    "The score is close to 50, which is in the same ballpark as the score we previously achieved with the Alpaca-style prompts.\n",
    "\n",
    "分数接近50，这与我们之前使用Alpaca风格提示词所达到的分数在同一水平。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea8be3-30a1-4623-a6d7-b095c6c1092e",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## Exercise 7.2: Instruction and input masking\n",
    "## 练习 7.2: 指令和输入掩码\n",
    "\n",
    "To mask out the instructions as shown in the following figure, we need to make slight modifications to the `InstructionDataset` class and `custom_collate_fn`.\n",
    "\n",
    "如下图所示,为了掩码掉指令,我们需要对`InstructionDataset`类和`custom_collate_fn`做一些细微的修改。\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/mask-instructions.webp\" width=600px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4405196a-db81-470b-be39-167a059587b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个`format_input`函数是从原始第7章代码复制过来的\n",
    "\n",
    "def format_input(entry):\n",
    "    # 构建指令文本,包含任务描述和具体指令\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"  # 任务描述开头\n",
    "        f\"Write a response that appropriately completes the request.\"  # 请求完成响应\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"  # 添加具体指令内容\n",
    "    )\n",
    "\n",
    "    # 如果存在输入文本则添加,否则为空字符串\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    # 返回完整的格式化文本(指令+输入)\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83658c09-af8a-425a-b940-eb1f06e43c0b",
   "metadata": {},
   "source": [
    "We can modify the `InstructionDataset` class to collect the lengths of the instructions, which we will use in the collate function to locate the instruction content positions in the targets when we code the collate function, as follows:\n",
    "我们可以修改`InstructionDataset`类来收集指令的长度，当我们编写collate函数时，将使用这些长度在目标中定位指令内容的位置，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5e6188a-f182-4f26-b9e5-ccae3ecadae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入PyTorch库\n",
    "import torch\n",
    "# 导入PyTorch数据集基类\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        # 存储原始数据\n",
    "        self.data = data\n",
    "\n",
    "        ##########################################################################################\n",
    "        # 新增:创建单独的列表存储指令长度\n",
    "        self.instruction_lengths = []\n",
    "        ##########################################################################################\n",
    "        \n",
    "        # 存储编码后的文本\n",
    "        self.encoded_texts = []\n",
    "        \n",
    "        # 遍历数据集中的每个条目\n",
    "        for entry in data:\n",
    "            # 格式化指令和输入文本\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            # 格式化响应文本\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            # 组合完整文本\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            \n",
    "            # 将完整文本编码并添加到列表中\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "            ##########################################################################################\n",
    "            # 新增:计算并收集指令长度\n",
    "            instruction_length = len(tokenizer.encode(instruction_plus_input))\n",
    "            self.instruction_lengths.append(instruction_length)\n",
    "            ##########################################################################################\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # 新增:同时返回指令长度和编码文本\n",
    "        return self.instruction_lengths[index], self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回数据集长度\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0163b7d1-acb8-456c-8efe-86307b58f4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入tiktoken分词器库\n",
    "import tiktoken\n",
    "\n",
    "# 初始化GPT-2分词器\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a186394-4960-424d-bb6a-f58459dd5994",
   "metadata": {},
   "source": [
    "Next, we update the `custom_collate_fn` where each `batch` is now a tuple containing `(instruction_length, item)` instead of just `item` due to the changes in the `InstructionDataset` dataset. In addition, we now mask the corresponding instruction tokens in the target ID list.\n",
    "\n",
    "接下来,我们更新`custom_collate_fn`函数,由于`InstructionDataset`数据集的变化,每个`batch`现在是一个包含`(instruction_length, item)`的元组而不是仅仅一个`item`。此外,我们现在会在目标ID列表中屏蔽相应的指令标记。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f815e6fc-8e54-4105-aecd-d4c6e890ff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "    batch,                      # 输入的批次数据\n",
    "    pad_token_id=50256,        # 填充标记的ID,默认为50256\n",
    "    ignore_index=-100,         # 忽略的索引值,默认为-100\n",
    "    allowed_max_length=None,   # 允许的最大序列长度,默认为None\n",
    "    device=\"cpu\"               # 设备类型,默认为CPU\n",
    "):\n",
    "    # 找出批次中最长序列的长度(加1是为了添加结束标记)\n",
    "    batch_max_length = max(len(item)+1 for instruction_length, item in batch)   \n",
    "\n",
    "    # 初始化输入和目标列表\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    # 遍历批次中的每个样本\n",
    "    for instruction_length, item in batch:  \n",
    "        # 复制当前样本以避免修改原始数据\n",
    "        new_item = item.copy()\n",
    "        # 在序列末尾添加结束标记\n",
    "        new_item += [pad_token_id]\n",
    "        # 使用填充标记将序列补齐到最大长度\n",
    "        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        # 创建输入张量,去掉最后一个标记\n",
    "        inputs = torch.tensor(padded[:-1])  \n",
    "        # 创建目标张量,向右移动一位\n",
    "        targets = torch.tensor(padded[1:])  \n",
    "\n",
    "        # 在目标中将除第一个外的所有填充标记替换为ignore_index\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        # 在目标中屏蔽所有输入和指令标记\n",
    "        targets[:instruction_length-1] = -100\n",
    "        \n",
    "        # 如果指定了最大长度,则截断序列\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "        \n",
    "        # 将处理后的序列添加到列表中\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # 将输入和目标列表转换为张量,并移至指定设备\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    # 返回处理后的输入和目标张量\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4a4815-850e-42c4-b70d-67e8ce5ebd57",
   "metadata": {},
   "source": [
    "Let's try it out on some sample data below:\n",
    "\n",
    "让我们在下面的示例数据上试一试:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8da8a5b1-a8e2-4389-b21c-25b67be6dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建示例数据列表,包含三个指令-输入-输出样本\n",
    "sample_data = [\n",
    "    # 示例1: 寻找反义词\n",
    "    {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"},\n",
    "    \n",
    "    # 示例2: 按字母顺序排序\n",
    "    {'instruction': 'Sort the following list in alphabetical order.', 'input': 'Zebra, Elephant, Crocodile', 'output': 'Crocodile, Elephant, Zebra'},\n",
    "    \n",
    "    # 示例3: 按降序排列数字\n",
    "    {'instruction': 'Arrange the given numbers in descending order.', 'input': '5, 12, 8, 3, 15', 'output': '15, 12, 8, 5, 3.'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "435b0816-0fc8-4650-a84a-eceffa4d85e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从PyTorch导入DataLoader类\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 创建训练数据集实例\n",
    "train_dataset = InstructionDataset(sample_data, tokenizer)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,  # 数据集\n",
    "    batch_size=len(sample_data),  # 批次大小设为样本总数\n",
    "    collate_fn=custom_collate_fn,  # 自定义的数据整理函数\n",
    "    num_workers=0  # 不使用多进程加载数据\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "106bbbd7-7286-4eb6-b343-43419332a80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([3, 64]) torch.Size([3, 64])\n"
     ]
    }
   ],
   "source": [
    "# 打印训练数据加载器的信息\n",
    "print(\"Train loader:\")\n",
    "# 遍历数据加载器,获取输入和目标张量\n",
    "for inputs, targets in train_loader:\n",
    "    # 打印输入和目标张量的形状\n",
    "    print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bb3288b-84a9-4962-ae59-a7a29fd34bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
      "          257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
      "        21017, 46486,    25,   198, 42758,   262,  1708,  1351,   287, 24830,\n",
      "          605,  1502,    13,   198,   198, 21017, 23412,    25,   198,    57,\n",
      "        37052,    11, 42651,    11,  9325, 19815,   576,   198,   198, 21017,\n",
      "        18261,    25,   198,    34, 12204,   375,   576,    11, 42651,    11,\n",
      "         1168, 37052, 50256, 50256])\n",
      "\n",
      "\n",
      "Targets:\n",
      " tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,   198,   198, 21017, 18261,\n",
      "           25,   198,    34, 12204,   375,   576,    11, 42651,    11,  1168,\n",
      "        37052, 50256,  -100,  -100])\n"
     ]
    }
   ],
   "source": [
    "# 打印第二个样本的输入张量\n",
    "print(\"Inputs:\\n\", inputs[1])\n",
    "# 打印第二个样本的目标张量\n",
    "print(\"\\n\\nTargets:\\n\", targets[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc40347b-2ca7-44e1-862d-0fd0c92f0628",
   "metadata": {},
   "source": [
    "\n",
    "As we can see based on the `targets` tensor, both the instruction and padding tokens are now masked using the -100 placeholder tokens. \n",
    "\n",
    "从targets张量可以看出,指令和填充标记现在都使用-100占位符标记进行了掩码处理。\n",
    "\n",
    "Let's decode the inputs just to make sure that they look correct:\n",
    "\n",
    "让我们解码输入以确保它们看起来正确:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76a9e6fa-3d75-4e39-b139-c3e05048f42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Sort the following list in alphabetical order.\n",
      "\n",
      "### Input:\n",
      "Zebra, Elephant, Crocodile\n",
      "\n",
      "### Response:\n",
      "Crocodile, Elephant, Zebra<|endoftext|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# 使用tokenizer解码第二个样本的输入序列\n",
    "print(tokenizer.decode(list(inputs[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845ebd36-f63f-4b58-a76e-7767e4d2ccbd",
   "metadata": {},
   "source": [
    "Next, let's decode the non-masked target token IDS:\n",
    "\n",
    "接下来,让我们解码未被掩码的目标标记ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d54a152-b778-455a-8941-e375e2a17e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "### Response:\n",
      "Crocodile, Elephant, Zebra<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# 获取非掩码目标标记(排除值为-100的标记)\n",
    "non_masked_targets = targets[1][targets[1] != -100]\n",
    "\n",
    "# 解码并打印非掩码目标标记\n",
    "print(tokenizer.decode(list(non_masked_targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3912bbf5-e9e2-474b-9552-d522e7510aa6",
   "metadata": {},
   "source": [
    "As shown above, the non-masked target tokens exclude the `\"Instruction\"` and `\"Input\"` fields, as intended. Now, we can run the modified code to see how well the LLM performs when finetuned using this masking strategy.\n",
    "\n",
    "如上所示,未掩码的目标标记按预期排除了\"Instruction\"和\"Input\"字段。现在,我们可以运行修改后的代码,看看使用这种掩码策略微调时LLM的表现如何。\n",
    "\n",
    "For your convenience, you can use the `exercise_experiments.py` code to run a comparison as follows:\n",
    "\n",
    "为了方便起见,您可以使用`exercise_experiments.py`代码运行如下比较:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a76097-9114-479d-8803-443b0ff48581",
   "metadata": {},
   "source": [
    "```bash\n",
    "python exercise_experiments.py --exercise_solution mask_instructions\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "matplotlib version: 3.7.1\n",
    "tiktoken version: 0.7.0\n",
    "torch version: 2.3.0+cu121\n",
    "tqdm version: 4.66.4\n",
    "tensorflow version: 2.15.0\n",
    "--------------------------------------------------\n",
    "Training set length: 935\n",
    "Validation set length: 55\n",
    "Test set length: 110\n",
    "--------------------------------------------------\n",
    "Device: cuda\n",
    "--------------------------------------------------\n",
    "...\n",
    "Loaded model: gpt2-medium (355M)\n",
    "--------------------------------------------------\n",
    "Initial losses\n",
    "   Training loss: 2.280539035797119\n",
    "   Validation loss: 2.262560224533081\n",
    "Ep 1 (Step 000000): Train loss 1.636, Val loss 1.620\n",
    "...\n",
    "Ep 2 (Step 000230): Train loss 0.143, Val loss 0.727\n",
    "...\n",
    "Training completed in 1.77 minutes.\n",
    "Plot saved as loss-plot-mask-instructions.pdf\n",
    "--------------------------------------------------\n",
    "Generating responses\n",
    "100% 110/110 [02:10<00:00,  1.19s/it]\n",
    "Responses saved as instruction-data-with-response-mask-instructions.json\n",
    "Model saved as gpt2-medium355M-sft-mask-instructions.pth\n",
    "```\n",
    "\n",
    "Next, let's evaluate the performance of the resulting LLM:\n",
    "\n",
    "接下来,让我们评估生成的LLM的性能:\n",
    "\n",
    "```bash\n",
    "python ollama_evaluate.py --file_path instruction-data-with-response-mask-instructions.json\n",
    "```\n",
    "\n",
    "```\n",
    "Ollama running: True\n",
    "Scoring entries: 100%|██████████████████████████████████████████████████████████████████████████████████████| 110/110 [01:23<00:00,  1.31it/s]\n",
    "Number of scores: 110 of 110\n",
    "Average score: 47.73\n",
    "```\n",
    "\n",
    "As we can see based on the scores, the instruction masking does perform slightly worse, which is consistent with the observation in the \"Instruction Tuning With Loss Over Instructions\" paper (https://arxiv.org/abs/2405.14394)\n",
    "\n",
    "根据分数可以看出，指令掩码的表现略差，这与\"基于指令损失的指令调优\"论文(https://arxiv.org/abs/2405.14394)中的观察结果一致"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a0f758-29da-44ee-b7af-32473b3c086e",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## Exercise 7.3: Finetuning on the original Alpaca dataset\n",
    "## 练习 7.3: 在原始 Alpaca 数据集上进行微调"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68df7616-679f-4e53-954d-6e7cf2e2ef55",
   "metadata": {},
   "source": [
    "To finetune the model on the original Stanford Alpaca dataset ([https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)), you just need to change the file URL from\n",
    "\n",
    "要在原始 Stanford Alpaca 数据集上微调模型([https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca))，你只需要更改文件 URL，从\n",
    "\n",
    "```python\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    "```\n",
    "\n",
    "to\n",
    "\n",
    "到\n",
    "\n",
    "```python\n",
    "url = \"https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\"\n",
    "```\n",
    "\n",
    "Note that the dataset contains 52k entries (50x more than in chapter 7), and the entries are longer than the ones we worked with in chapter 7.\n",
    "\n",
    "请注意，该数据集包含52k个条目(比第7章多50倍)，并且条目比我们在第7章中处理的更长。\n",
    "\n",
    "Thus, it's highly recommended that the training be run on a GPU.\n",
    "\n",
    "因此，强烈建议在GPU上运行训练。\n",
    "\n",
    "If you encounter out-of-memory errors, consider reducing the batch size from 8 to 4, 2, or 1. In addition to lowering the batch size, you may also want to consider lowering the `allowed_max_length` from 1024 to 512 or 256.\n",
    "\n",
    "如果遇到内存不足错误，请考虑将批量大小从8减少到4、2或1。除了降低批量大小外，你可能还需要考虑将`allowed_max_length`从1024降低到512或256。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94c9621-2c3f-4551-b5b8-87cd96e38c9c",
   "metadata": {},
   "source": [
    "For your convenience, you can use the `exercise_experiments.py` code to finetune the model on the 52k Alpaca dataset with a batch size of 4 and an `allowed_max_length` of 512 as follows:\n",
    "\n",
    "为了方便起见，你可以使用`exercise_experiments.py`代码在52k Alpaca数据集上微调模型，批量大小为4，`allowed_max_length`为512，具体如下:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a76486-73e6-4415-94dc-bfe2aa36ea52",
   "metadata": {},
   "source": [
    "```bash\n",
    "python exercise_experiments.py --exercise_solution alpaca_52k\n",
    "```\n",
    "\n",
    "```\n",
    "matplotlib version: 3.7.1\n",
    "tiktoken version: 0.7.0\n",
    "torch version: 2.3.0+cu121\n",
    "tqdm version: 4.66.4\n",
    "tensorflow version: 2.15.0\n",
    "--------------------------------------------------\n",
    "Training set length: 44201\n",
    "Validation set length: 2601\n",
    "Test set length: 5200\n",
    "--------------------------------------------------\n",
    "Device: cuda\n",
    "--------------------------------------------------\n",
    "...\n",
    "Loaded model: gpt2-medium (355M)\n",
    "--------------------------------------------------\n",
    "Initial losses\n",
    "   Training loss: 3.3681655883789063\n",
    "   Validation loss: 3.4122894287109373\n",
    "Ep 1 (Step 000000): Train loss 2.477, Val loss 2.750\n",
    "...\n",
    "Ep 2 (Step 022095): Train loss 0.761, Val loss 1.557\n",
    "...\n",
    "Training completed in 196.38 minutes.\n",
    "Plot saved as loss-plot-alpaca52k.pdf\n",
    "--------------------------------------------------\n",
    "Generating responses\n",
    "100% 5200/5200 [2:56:33<00:00,  2.04s/it]\n",
    "Responses saved as instruction-data-with-response-alpaca52k.json\n",
    "Model saved as gpt2-medium355M-sft-alpaca52k.pth\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8a1bd1-15f4-4a35-87b5-369038db0aa7",
   "metadata": {},
   "source": [
    "Below are a few examples from the Alpaca dataset, including the generated model responses:\n",
    "\n",
    "以下是来自Alpaca数据集的一些示例，包括生成的模型响应："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9ab66f-5819-4b01-9a03-c45aa3b7c5b8",
   "metadata": {},
   "source": [
    "```json\n",
    "[\n",
    "    {\n",
    "        \"instruction\": \"Edit the following sentence to increase readability: \\\"He made a huge effort and was so successful.\\\"\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"He exerted a tremendous effort, and thus enjoyed great success.\",\n",
    "        \"model_response\": \"He put in an immense effort and was rewarded with success.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Rewrite the following sentence to make it more concise: \\\"I was displeased with the result of the experiment that I conducted.\\\"\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"I was unhappy with my experiment's outcome.\",\n",
    "        \"model_response\": \"I was displeased with the results of the experiment.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"How can we build a more efficient GPT model?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"We can build a more efficient GPT model by optimizing the architecture of the model, using smaller model sizes and training with fewer parameters. We can also leverage techniques such as knowledge distillation, transfer learning, dynamic sparsity and hybrid computing to further improve the efficiency of the model.\",\n",
    "        \"model_response\": \"Building a more efficient GPT model requires careful planning and optimization. First, it is important to identify the target language and the context in which the model is used. Then, it is important to select the appropriate model architecture, such as backpropagation, hyperparameters, and hyperparameters. Finally, it is important to select the appropriate model weights and optimizers, such as backpropagation, hyperparameters, and hyperparameters.\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840e2076-f2e6-44a2-86fd-f191f9674267",
   "metadata": {},
   "source": [
    " Finally, we can evaluate the finetuned LLM using the [ollama_evaluate.py](ollama_evaluate.py) utility function:\n",
    " \n",
    " 最后，我们可以使用 [ollama_evaluate.py](ollama_evaluate.py) 工具函数来评估微调后的 LLM：\n",
    "\n",
    "```bash\n",
    "python ollama_evaluate.py --file_path instruction-data-with-response-alpaca52k.json\n",
    "```\n",
    "\n",
    "```\n",
    "Scoring entries: 100%|████████████████████| 5200/5200 [1:07:52<00:00, 1.28it/s]\n",
    "Number of scores: 5188 of 5200\n",
    "Average score: 48.16\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14b3c60-00a1-43a9-9fcd-592aaadf1ef4",
   "metadata": {},
   "source": [
    "The score is slightly lower than the score we obtained on the dataset we used in this chapter. However, note that the Alpaca test set contains more diverse and partly more challenging instructions than the dataset we used in the main chapter.\n",
    "\n",
    "该分数略低于我们在本章使用的数据集上获得的分数。但是请注意，Alpaca测试集包含了比我们在本章主要使用的数据集更加多样化且部分更具挑战性的指令。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca61fa6c-4e1d-4618-9e5e-d091f8303e30",
   "metadata": {},
   "source": [
    "## Exercise 7.4: Parameter-efficient finetuning with LoRA\n",
    "## 练习 7.4：使用 LoRA 进行参数高效的微调"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01742cec-1f41-4415-8788-009d31b1ad38",
   "metadata": {},
   "source": [
    "To instruction finetune the model using LoRA, use the relevant classes and functions from appendix E:\n",
    "\n",
    "要使用 LoRA 对模型进行指令微调，请使用附录 E 中的相关类和函数：\n",
    "\n",
    "```python\n",
    "from appendix_E import LoRALayer, LinearWithLoRA, replace_linear_with_lora\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871dca8f-3411-4735-b7b0-9d0e6e0599ac",
   "metadata": {},
   "source": [
    "Next, add the following lines of code below the model loading code in section 7.5:\n",
    "\n",
    "接下来，在 7.5 节的模型加载代码下面添加以下代码行：\n",
    "\n",
    "\n",
    "```python\n",
    "# 计算模型中可训练参数的总数\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters before: {total_params:,}\")\n",
    "\n",
    "# 冻结所有模型参数\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 再次计算可训练参数数量(应该为0)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters after: {total_params:,}\")\n",
    "\n",
    "# 使用LoRA替换线性层,rank和alpha都设为16\n",
    "replace_linear_with_lora(model, rank=16, alpha=16)\n",
    "\n",
    "# 计算LoRA可训练参数的数量\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable LoRA parameters: {total_params:,}\")\n",
    "\n",
    "# 将模型移至指定设备\n",
    "model.to(device)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b26b925-dc95-4b91-b050-9676dd9608a4",
   "metadata": {},
   "source": [
    "For your convenience, you can use the `exercise_experiments.py` code to finetune the model, using LoRA with rank 16 and alpa 16, as follows:\n",
    "\n",
    "为了方便起见，您可以使用 `exercise_experiments.py` 代码来微调模型，使用 rank 为 16 和 alpha 为 16 的 LoRA，具体如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f02c7e-3b15-44b8-bf41-7892cd755766",
   "metadata": {},
   "source": [
    "```bash\n",
    "python exercise_experiments.py --exercise_solution lora\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "matplotlib version: 3.7.1\n",
    "tiktoken version: 0.7.0\n",
    "torch version: 2.3.0+cu121\n",
    "tqdm version: 4.66.4\n",
    "tensorflow version: 2.15.0\n",
    "--------------------------------------------------\n",
    "Training set length: 935\n",
    "Validation set length: 55\n",
    "Test set length: 110\n",
    "--------------------------------------------------\n",
    "Device: cuda\n",
    "--------------------------------------------------\n",
    "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
    "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
    "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
    "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
    "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
    "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
    "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n",
    "Loaded model: gpt2-medium (355M)\n",
    "--------------------------------------------------\n",
    "Total trainable parameters before: 406,286,336\n",
    "Total trainable parameters after: 0\n",
    "Total trainable LoRA parameters: 7,898,384\n",
    "Initial losses\n",
    "   Training loss: 3.7684114456176756\n",
    "   Validation loss: 3.7619335651397705\n",
    "Ep 1 (Step 000000): Train loss 2.509, Val loss 2.519\n",
    "...\n",
    "Ep 2 (Step 000230): Train loss 0.308, Val loss 0.652\n",
    "...\n",
    "--------------------------------------------------\n",
    "Generating responses\n",
    "100% 110/110 [01:52<00:00,  1.03s/it]\n",
    "Responses saved as instruction-data-with-response-lora.json\n",
    "Model saved as gpt2-medium355M-sft-lora.pth\n",
    "```\n",
    "\n",
    "For comparison, you can run the original chapter 7 finetuning code via `python exercise_experiments.py --exercise_solution baseline`. \n",
    "\n",
    "作为对比，您可以通过运行 `python exercise_experiments.py --exercise_solution baseline` 来执行第7章的原始微调代码。\n",
    "\n",
    "Note that on an Nvidia L4 GPU, the code above, using LoRA, takes 1.30 min to run. In comparison, the baseline takes 1.80 minutes to run. So, LoRA is approximately 28% faster.\n",
    "\n",
    "请注意，在 Nvidia L4 GPU 上，使用 LoRA 的上述代码需要运行1.30分钟。相比之下，基准测试需要1.80分钟。因此，LoRA 大约快28%。\n",
    "\n",
    "We can evaluate the performance using the Ollama Llama 3 method, which is for your convenience, also implemented in the `python exercise_experiments.py` script, which we can run as follows:\n",
    "\n",
    "我们可以使用 Ollama Llama 3 方法评估性能，为了方便起见，该方法也已在 `python exercise_experiments.py` 脚本中实现，我们可以按如下方式运行：\n",
    "\n",
    "```bash\n",
    "python ollama_evaluate.py --file_path instruction-data-with-response-lora.json\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "Ollama running: True\n",
    "Scoring entries: 100%|████████████████████████| 110/110 [01:13<00:00,  1.50it/s]\n",
    "Number of scores: 110 of 110\n",
    "Average score: 50.23\n",
    "```\n",
    "\n",
    "The score is around 50, which is in the same ballpark as the original model.\n",
    "\n",
    "分数大约在50左右，与原始模型在同一水平。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
