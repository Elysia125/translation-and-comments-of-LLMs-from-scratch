{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "136a4efe-fb99-4311-8679-e0a5b6282755",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1910a06-e8a3-40ac-8201-ff70615b1ba4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generating A Preference Dataset With Llama 3.1 70B And Ollama\n",
    "# 使用 Llama 3.1 70B 和 Ollama 生成偏好数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a128651b-f326-4232-a994-42f38b7ed520",
   "metadata": {},
   "source": [
    "- Preference finetuning is a process to align an instruction-finetuned LLM with human preferences\n",
    "- 偏好微调是一个将指令微调的LLM与人类偏好对齐的过程\n",
    "\n",
    "- There are multiple ways to create a dataset for preference finetuning an LLM\n",
    "- 有多种方法可以创建用于LLM偏好微调的数据集\n",
    "\n",
    "  1. We use the instruction-finetuned LLM to generate multiple responses and have humans rank them based on their preference and/or given preference criteria\n",
    "  \n",
    "  1. 我们使用指令微调的LLM生成多个回答，并让人类根据他们的偏好和/或给定的偏好标准对其进行排名\n",
    "\n",
    "  2. We use the instruction-finetuned LLM to generate multiple responses and have LLMs rank them based on given preference criteria\n",
    "  \n",
    "  2. 我们使用指令微调的LLM生成多个回答，并让LLM根据给定的偏好标准对其进行排名\n",
    "\n",
    "  3. We use an LLM to generate preferred and dispreferred responses given certain preference criteria\n",
    "  \n",
    "  3. 我们使用LLM根据特定的偏好标准生成首选和非首选的回答\n",
    "\n",
    "- In this notebook, we consider approach 3\n",
    "- 在本笔记本中，我们考虑方法3\n",
    "\n",
    "- This notebook uses a 70-billion-parameter Llama 3.1-Instruct model through ollama to generate preference labels for an instruction dataset\n",
    "- 本笔记本通过ollama使用700亿参数的Llama 3.1-Instruct模型为指令数据集生成偏好标签\n",
    "\n",
    "- The expected format of the instruction dataset is as follows:\n",
    "- 指令数据集的预期格式如下：\n",
    "\n",
    "### Input\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"instruction\": \"What is the state capital of California?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"The state capital of California is Sacramento.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Provide a synonym for 'fast'.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"A synonym for 'fast' is 'quick'.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is the capital of Greece?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"The capital of Greece is Athens.\",\n",
    "\n",
    "    },\n",
    "...\n",
    "]\n",
    "```\n",
    "\n",
    "The output dataset will look as follows, where more polite responses are preferred (`'chosen'`), and more impolite responses are dispreferred (`'rejected'`):\n",
    "\n",
    "输出数据集将如下所示，其中更有礼貌的回答被标记为首选(`'chosen'`)，而不太有礼貌的回答被标记为非首选(`'rejected'`):\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"instruction\": \"What is the state capital of California?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"The state capital of California is Sacramento.\",\n",
    "        \"rejected\": \"Look, the state capital of California is obviously Sacramento.\",\n",
    "        \"chosen\": \"The state capital of California is Sacramento.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Provide a synonym for 'fast'.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"A synonym for 'fast' is 'quick'.\",\n",
    "        \"chosen\": \"A suitable alternative to 'fast' would be 'quick'.\",\n",
    "        \"rejected\": \"A synonym for 'fast' is 'quick'.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is the capital of Greece?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"The capital of Greece is Athens.\",\n",
    "        \"chosen\": \"I'd be happy to help! The capital of Greece is indeed Athens.\",\n",
    "        \"rejected\": \"The capital of Greece is Athens.\"\n",
    "    },\n",
    "...\n",
    "]\n",
    "```\n",
    "\n",
    "### Output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- The code doesn't require a GPU and runs on a laptop given enough RAM\n",
    "- 代码不需要 GPU，只要有足够的内存就可以在笔记本电脑上运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63610acc-db94-437f-8d38-e99dca0299cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tqdm version: 4.66.4\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"tqdm\",    # Progress bar\n",
    "        ]\n",
    "\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcdcb34-ac75-4f4f-9505-3ce0666c42d5",
   "metadata": {},
   "source": [
    "## Installing Ollama and Downloading Llama 3.1\n",
    "## 安装 Ollama 并下载 Llama 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a092280-5462-4709-a3fe-8669a4a8a0a6",
   "metadata": {},
   "source": [
    "- Ollama is an application to run LLMs efficiently\n",
    "- Ollama 是一个高效运行大语言模型的应用程序\n",
    "- It is a wrapper around [llama.cpp](https://github.com/ggerganov/llama.cpp), which implements LLMs in pure C/C++ to maximize efficiency  \n",
    "- 它是 [llama.cpp](https://github.com/ggerganov/llama.cpp) 的封装，llama.cpp 使用纯 C/C++ 实现大语言模型以最大化效率\n",
    "- Note that it is a tool for using LLMs to generate text (inference), not training or finetuning LLMs\n",
    "- 请注意，它是一个用于使用大语言模型生成文本(推理)的工具，而不是用于训练或微调大语言模型\n",
    "- Prior to running the code below, install ollama by visiting [https://ollama.com](https://ollama.com) and following the instructions (for instance, clicking on the \"Download\" button and downloading the ollama application for your operating system)\n",
    "- 在运行以下代码之前，请访问 [https://ollama.com](https://ollama.com) 并按照说明安装 ollama(例如，点击\"Download\"按钮并下载适用于您操作系统的 ollama 应用程序)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9558a522-650d-401a-84fc-9fd7b1f39da7",
   "metadata": {},
   "source": [
    "- For macOS and Windows users, click on the ollama application you downloaded; if it prompts you to install the command line usage, say \"yes\"\n",
    "- 对于 macOS 和 Windows 用户，点击下载的 ollama 应用程序；如果提示安装命令行用法，请选择\"是\"\n",
    "- Linux users can use the installation command provided on the ollama website\n",
    "- Linux 用户可以使用 ollama 网站提供的安装命令\n",
    "\n",
    "- In general, before we can use ollama from the command line, we have to either start the ollama application or run `ollama serve` in a separate terminal\n",
    "- 通常，在使用命令行的 ollama 之前，我们需要启动 ollama 应用程序或在单独的终端中运行 `ollama serve`\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/ollama-eval/ollama-serve.webp?1\">\n",
    "\n",
    "\n",
    "- With the ollama application or `ollama serve` running, in a different terminal, on the command line, execute the following command to try out the 70-billion-parameter Llama 3.1 model \n",
    "- 在运行 ollama 应用程序或 `ollama serve` 的情况下，在另一个终端中，在命令行执行以下命令来尝试使用 700 亿参数的 Llama 3.1 模型\n",
    "\n",
    "```bash\n",
    "# 70B model\n",
    "ollama run llama3.1:70b\n",
    "```\n",
    "\n",
    "\n",
    "The output looks like as follows:\n",
    "\n",
    "输出如下所示：\n",
    "\n",
    "```\n",
    "$ ollama run llama3.1:70b\n",
    "pulling manifest\n",
    "pulling aa81b541aae6... 100% ▕████████████████▏ 39 GB\n",
    "pulling 8cf247399e57... 100% ▕████████████████▏ 1.7 KB\n",
    "pulling f1cd752815fc... 100% ▕████████████████▏ 12 KB\n",
    "pulling 56bb8bd477a5... 100% ▕████████████████▏ 96 B\n",
    "pulling 3c1c2d3df5b3... 100% ▕████████████████▏ 486 B\n",
    "verifying sha256 digest\n",
    "writing manifest\n",
    "removing any unused layers\n",
    "success\n",
    "```\n",
    "\n",
    "- Note that `llama3.1:70b` refers to the instruction finetuned 70-billion-parameter Llama 3.1 model\n",
    "- 注意 `llama3.1:70b` 指的是经过指令微调的700亿参数的 Llama 3.1 模型\n",
    "\n",
    "- Alternatively, you can also use the smaller, more resource-effiicent 8-billion-parameters Llama 3.1 model, by replacing `llama3.1:70b` with `llama3.1`\n",
    "- 另外，你也可以使用更小、更节省资源的80亿参数的 Llama 3.1 模型，只需将 `llama3.1:70b` 替换为 `llama3.1`\n",
    "\n",
    "- After the download has been completed, you will see a command line prompt that allows you to chat with the model\n",
    "- 下载完成后，你会看到一个命令行提示符，可以用它与模型进行对话\n",
    "\n",
    "- Try a prompt like \"What do llamas eat?\", which should return an output similar to the following:\n",
    "- 试试输入类似 \"What do llamas eat?\" 这样的提示词，它会返回类似下面的输出：\n",
    "\n",
    "```\n",
    ">>> What do llamas eat?\n",
    "Llamas are ruminant animals, which means they have a four-chambered \n",
    "stomach and eat plants that are high in fiber. In the wild, llamas \n",
    "typically feed on:\n",
    "1. Grasses: They love to graze on various types of grasses, including tall \n",
    "grasses, wheat, oats, and barley.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5addcb-fc7d-455d-bee9-6cc7a0d684c7",
   "metadata": {},
   "source": [
    "- You can end this session using the input `/bye`\n",
    "- 你可以使用输入 `/bye` 来结束会话"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda155ee-cf36-44d3-b634-20ba8e1ca38a",
   "metadata": {},
   "source": [
    "## Using Ollama's REST API\n",
    "## 使用 Ollama 的 REST API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89343a84-0ddc-42fc-bf50-298a342b93c0",
   "metadata": {},
   "source": [
    "- Now, an alternative way to interact with the model is via its REST API in Python via the following function\n",
    "- 现在，另一种与模型交互的方式是通过Python中的REST API使用以下函数\n",
    "- Before you run the next cells in this notebook, make sure that ollama is still running, as described above, via\n",
    "- 在运行本笔记本中的下一个单元格之前，请确保按照上述方式运行ollama，通过：\n",
    "  - `ollama serve` in a terminal\n",
    "  - 在终端中运行 `ollama serve`\n",
    "  - the ollama application\n",
    "  - ollama应用程序\n",
    "- Next, run the following code cell to query the model\n",
    "- 接下来，运行以下代码单元格来查询模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16642a48-1cab-40d2-af08-ab8c2fbf5876",
   "metadata": {},
   "source": [
    "- First, let's try the API with a simple example to make sure it works as intended:\n",
    "- 首先，让我们用一个简单的例子来测试 API，确保它能按预期工作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65b0ba76-1fb1-4306-a7c2-8f3bb637ccdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are herbivores, which means they primarily eat plants and plant-based foods. Their diet consists of:\n",
      "\n",
      "1. **Grasses**: Various types of grasses, including timothy grass, orchard grass, and brome grass.\n",
      "2. **Hay**: High-quality hay, such as alfalfa or clover hay, is a staple in a llama's diet.\n",
      "3. **Leaves**: Leaves from trees and shrubs, like willow, cottonwood, and mesquite, are also eaten.\n",
      "4. **Fruits and vegetables**: Llamas enjoy fruits like apples, carrots, and sweet potatoes, as well as leafy greens like kale and spinach.\n",
      "5. **Grains**: In moderation, llamas can eat grains like oats, barley, and corn.\n",
      "\n",
      "It's essential to note that llamas have a unique digestive system, with a three-part stomach and a large cecum (a specialized part of the large intestine). This allows them to break down and extract nutrients from plant material more efficiently than many other animals.\n",
      "\n",
      "A typical llama diet might consist of:\n",
      "\n",
      "* 1-2% of their body weight in hay per day\n",
      "* 0.5-1% of their body weight in grains per day (if fed)\n",
      "* Free-choice access to fresh water\n",
      "* Limited amounts of fruits and vegetables as treats\n",
      "\n",
      "It's also important to ensure that llamas have access to a mineral supplement, such as a salt lick or loose minerals, to help maintain optimal health.\n",
      "\n",
      "Remember, every llama is different, and their dietary needs may vary depending on factors like age, size, and activity level. Consult with a veterinarian or experienced llama breeder for specific guidance on feeding your llama.\n"
     ]
    }
   ],
   "source": [
    "# 导入urllib.request库用于发送HTTP请求\n",
    "import urllib.request\n",
    "# 导入json库用于处理JSON数据\n",
    "import json\n",
    "\n",
    "\n",
    "def query_model(prompt, model=\"llama3.1:70b\", url=\"http://localhost:11434/api/chat\"):\n",
    "    # 创建数据载荷字典\n",
    "    data = {\n",
    "        # 指定要使用的模型\n",
    "        \"model\": model,\n",
    "        # 消息列表,包含用户输入\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        # 设置模型选项\n",
    "        \"options\": {\n",
    "            \"seed\": 123,  # 随机种子\n",
    "            \"temperature\": 0,  # 温度参数\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # 将字典转换为JSON字符串并编码为字节\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "\n",
    "    # 创建请求对象,设置POST方法和必要的头部\n",
    "    request = urllib.request.Request(url, data=payload, method=\"POST\")\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    # 发送请求并获取响应\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        # 读取并解码响应\n",
    "        while True:\n",
    "            # 逐行读取响应\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            # 如果没有更多数据则退出循环\n",
    "            if not line:\n",
    "                break\n",
    "            # 解析JSON响应\n",
    "            response_json = json.loads(line)\n",
    "            # 累加响应内容\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "\n",
    "    # 返回完整的响应内容\n",
    "    return response_data\n",
    "\n",
    "\n",
    "# 测试查询模型\n",
    "result = query_model(\"What do Llamas eat?\")\n",
    "# 打印结果\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162a4739-6f03-4092-a5c2-f57a0b6a4c4d",
   "metadata": {},
   "source": [
    "## Load JSON Entries\n",
    "## 加载JSON条目"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca011a8b-20c5-4101-979e-9b5fccf62f8a",
   "metadata": {},
   "source": [
    "- 现在,让我们开始数据生成部分\n",
    "- Now, let's get to the data generation part\n",
    "- 在这个实践示例中,我们使用之前在第7章用于指令微调模型的`instruction-data.json`文件:\n",
    "- Here, for a hands-on example, we use the `instruction-data.json` file that we originally used to instruction-finetune the model in chapter 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b2d393a-aa92-4190-9d44-44326a6f699b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "# 导入Path类用于处理文件路径\n",
    "from pathlib import Path\n",
    "\n",
    "# 设置JSON文件路径,指向上一级目录的instruction-data.json文件\n",
    "json_file = Path(\"..\", \"01_main-chapter-code\", \"instruction-data.json\")\n",
    "\n",
    "# 打开并读取JSON文件\n",
    "with open(json_file, \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "# 打印数据条目数量\n",
    "print(\"Number of entries:\", len(json_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c9751b-59b7-43fe-acc7-14e8daf2fa66",
   "metadata": {},
   "source": [
    "- The structure of this file is as follows, where we have the given response in the test dataset (`'output'`) that we trained the model to generate via instruction finetuning based on the `'input'` and `'instruction'`\n",
    "- 该文件的结构如下,其中包含了测试数据集中的给定响应(`'output'`),这是我们基于`'input'`和`'instruction'`通过指令微调训练模型生成的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7222fdc0-5684-4f2b-b741-3e341851359e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Evaluate the following phrase by transforming it into the spelling given.',\n",
       " 'input': 'freind --> friend',\n",
       " 'output': 'The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 打印第一个数据条目以查看数据结构\n",
    "json_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf0331b-6024-4bba-89a9-a088b14a1046",
   "metadata": {},
   "source": [
    "- Below is a small utility function that formats the instruction and input:\n",
    "- 下面是一个用于格式化指令和输入的小工具函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43263cd3-e5fb-4ab5-871e-3ad6e7d21a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    \"\"\"\n",
    "    格式化输入文本\n",
    "    \n",
    "    参数:\n",
    "        entry (dict): 包含instruction和input字段的字典\n",
    "        \n",
    "    返回:\n",
    "        str: 格式化后的文本,包含指令和输入(如果有)\n",
    "    \"\"\"\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. Write a response that \"\n",
    "        f\"appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    instruction_text + input_text\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a55283-7d51-4136-ba60-f799d49f4098",
   "metadata": {},
   "source": [
    "- Now, let's try the ollama API to generate a `'chosen'` and `'rejected'` response for preference tuning a model\n",
    "- 现在,让我们尝试使用ollama API来生成用于偏好调优模型的`'chosen'`和`'rejected'`响应\n",
    "- Here, to for illustration purposes, we create answers that are more or less polite\n",
    "- 在这里,为了说明目的,我们创建或多或少带有礼貌性的答案\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "735cc089-d127-480a-b39d-0782581f0c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset response:\n",
      ">> The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".\n",
      "\n",
      "impolite response:\n",
      ">> The spelling of the given phrase \"freind\" is flat out wrong, get it together, the correct spelling is \"friend\".\n",
      "\n",
      "Dataset response:\n",
      ">> He goes to the park every day.\n",
      "\n",
      "polite response:\n",
      ">> He goes to the park daily, if I'm not mistaken.\n",
      "\n",
      "Dataset response:\n",
      ">> 45 kilometers is 45000 meters.\n",
      "\n",
      "polite response:\n",
      ">> 45 kilometers is equivalent to 45000 meters.\n",
      "\n",
      "Dataset response:\n",
      ">> Although it was raining, they went for a walk.\n",
      "\n",
      "polite response:\n",
      ">> Although it was raining outside, they still decided to go for a walk.\n",
      "\n",
      "Dataset response:\n",
      ">> 1, 4, 9, 16, 25, 36, 49, 64, 81, 100.\n",
      "\n",
      "impolite response:\n",
      ">> Here are your precious square numbers: 1, 4, 9, 16, 25, 36, 49, 64, 81, 100.\n"
     ]
    }
   ],
   "source": [
    "# 导入random模块用于随机选择\n",
    "import random\n",
    "\n",
    "\n",
    "# 遍历前5个数据条目\n",
    "for entry in json_data[:5]:\n",
    "    \n",
    "    # 随机选择是生成礼貌或不礼貌的回复\n",
    "    politeness = random.choice([\"polite\", \"impolite\"])    \n",
    "    # 构建提示语,要求模型根据原始输入和输出生成更礼貌或不礼貌的回复\n",
    "    prompt = (\n",
    "        f\"Given the input `{format_input(entry)}` \"\n",
    "        f\"and correct output `{entry['output']}`, \"\n",
    "        f\"slightly rewrite the output to be more {politeness}.\"\n",
    "        \"Keep the modification minimal.\"\n",
    "        \"Only return return the generated response and nothing else.\"\n",
    "    )\n",
    "    # 打印原始数据集中的回复\n",
    "    print(\"\\nDataset response:\")\n",
    "    print(\">>\", entry['output'])\n",
    "    # 打印生成的新回复(更礼貌或不礼貌)\n",
    "    print(f\"\\n{politeness} response:\")\n",
    "    print(\">>\", query_model(prompt))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142dfaa7-429f-4eb0-b74d-ff327f79547a",
   "metadata": {},
   "source": [
    "- If we find that the generated responses above look reasonable, we can go to the next step and apply the prompt to the whole dataset\n",
    "- 如果我们发现上面生成的响应看起来合理，我们就可以进入下一步，将提示应用到整个数据集\n",
    "\n",
    "- Here, we add a `'chosen'` key for the preferred response and a `'rejected'` response for the dispreferred response\n",
    "- 在这里，我们为首选响应添加一个`'chosen'`键，为非首选响应添加一个`'rejected'`键"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3349dbbc-963f-4af3-9790-12dbfdca63c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入随机数生成模块\n",
    "import random\n",
    "# 导入进度条显示模块\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 定义生成模型响应的函数\n",
    "def generate_model_responses(json_data):\n",
    "\n",
    "    # 遍历数据集中的每个条目,显示进度条\n",
    "    for i, entry in enumerate(tqdm(json_data, desc=\"Writing entries\")):\n",
    "        # 随机选择是生成礼貌或不礼貌的回答\n",
    "        politeness = random.choice([\"polite\", \"impolite\"])    \n",
    "        # 构建提示语,要求模型根据输入和正确输出生成更礼貌或不礼貌的回答\n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`, \"\n",
    "            f\"slightly rewrite the output to be more {politeness}.\"\n",
    "            \"Keep the modification minimal.\"\n",
    "            \"Only return return the generated response and nothing else.\"\n",
    "        )\n",
    "        # 调用模型生成回答\n",
    "        response = query_model(prompt)\n",
    "        \n",
    "        # 如果是礼貌回答,将其设为chosen,原回答设为rejected\n",
    "        if politeness == \"polite\":\n",
    "            json_data[i][\"chosen\"] = response\n",
    "            json_data[i][\"rejected\"] = entry[\"output\"]\n",
    "        # 如果是不礼貌回答,将其设为rejected,原回答设为chosen\n",
    "        else:\n",
    "            json_data[i][\"rejected\"] = response\n",
    "            json_data[i][\"chosen\"] = entry[\"output\"]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b071ce84-1866-427f-a272-b46700f364b2",
   "metadata": {},
   "source": [
    "- Let's now apply this evaluation to the whole dataset and compute the average score of each model (this takes about 1 minute per model on an M3 MacBook Air laptop)\n",
    "- 让我们现在将这个评估应用到整个数据集，并计算每个模型的平均分数(在M3 MacBook Air笔记本电脑上每个模型大约需要1分钟)\n",
    "- Note that ollama is not fully deterministic across operating systems (as of this writing) so the numbers you are getting might slightly differ from the ones shown below\n",
    "- 请注意，ollama在不同操作系统上并非完全确定性的(在撰写本文时)，因此您获得的数字可能与下面显示的数字略有不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f700d4b-19e5-4404-afa7-b0f093024232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing entries: 100%|██████████| 1100/1100 [17:20<00:00,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# 对整个数据集生成模型响应\n",
    "# 这个过程可能需要一些时间，具体取决于数据集大小\n",
    "generate_model_responses(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "838d9747-0f7d-46fe-aab5-9ee6b765d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将带有偏好数据的JSON保存到文件中\n",
    "with open(\"instruction-data-with-preference.json\", \"w\") as file:\n",
    "    json.dump(json_data, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
